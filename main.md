---
title: Investigating Adaptive Step Size in Gradient Flow TODO?? I can do better than that.
author: "Candidate number: 1045966"
bibliography: references.biblatex
reference-section-title: References
header-includes: |
    \institute{University of Oxford}

    \usepackage[margin=3cm]{geometry}
    \usepackage{caption}
    \usepackage{xcolor}

    \usepackage{fancyhdr}
    \pagestyle{fancy}
    \lhead{}
    \rhead{}
    \chead{}
    \renewcommand\headrulewidth{0.1pt}
    \renewcommand\footrulewidth{0pt}
    \fancyhead[LO,LE]{Geometric Deep Learning}
    \fancyhead[RO,RE]{Candidate: 1045966}
documentclass: llncs
fontsize: 11pt
numbersections: false
toc: false
---

# Introduction

Graph neural networks (GNNs) have been used successfully on a wide range of applications, including social networks[@wangMCNEEndtoEndFramework2019], image segmentation[@aflaloDeepCutUnsupervisedSegmentation2022], and protein interactions[@jhaPredictionProteinProtein2022]. Geometric deep learning studies deep neural networks, including GNNs, by looking at the geometry of the underlying spaces[@bronsteinGeometricDeepLearning2017]. One result in this field studied GNNs by viewing them as discretized descriptions of particle interactions, and derived the GRAFF layer from that analysis[@digiovanniGraphNeuralNetworks2022a]. Apart from wide-ranging theoretical guarantees, GRAFF can potentially be implemented very efficiently, and generalizes better to different types of data. While a GRAFF-based network competitively on state-of-the art models on both homophilic and heterophilic tasks, the relatively low parameter count and low reliance on non-linearities does not fit the intuition of a GNN "processing" graph data. This raises the question of whether it can perform well on complex tasks with larger problem radii.

This study investigates this question by evaluating a GRAFF-based network against a GNN benchmark on a synthetic datasets. The datasets are based on logical expressions of varying length, and are taken as a proxy for increasing task complexity. In addition, a new MultiGRAFF architecture based on the composition of different GRAFF layers is proposed, and evaluated on the same dataset.

TODO I find that...

# Methodology

To establish how GRAFF and the novel MultiGRAFF perform for tasks of different complexities, they were trained and evaluated on a synthetic dataset. In addition, the standard SASGE GNN was trained as a benchmark[@hamiltonInductiveRepresentationLearning2018].

## Synthetic Dataset

To test model performance across different task complexities, a range of 6 synthetic datasets was generated. Each dataset consists of 50 graphs $G=(V,E,c)$, with nodes $V$, edges $E$ and node colors $c: V \rightarrow \left\{ 1, 2, \ldots, 6 \right\}$. The tasks were all binary node classifications tasks, where the class of a node for the $k$-th dataset is given by a logical expression $\phi_k : V \rightarrow \mathbb B$, for $1 \leq k \leq 6$.

The first expression is

$$ \phi_1 (v) = \left[ c(v) = 1 \right], $$ {#eq:met-phi1}

indicating that $\phi_1 (v)$ is true _iff_ $v$ has the first color. All following expressions are defined inductively by

$$ \phi_{k+1}(u) = \left[ \exists v . (u, v) \in E \land c(u) = k+1 \land \phi_k(v) . \right]$$ {#eq:met-phik}

For an example task, see [@fig:task_example].

![Illustration of $\phi_1$ (left) and $\phi_2$ (right). The order of colors is 1=red, 2=green, 3=blue, 4=purple. Big nodes evaluate to true, small to false. In plain English, the left task is simply _all red nodes_. The right task translates to _all blue nodes, neighbouring [a green node neighbouring a red node]_. \label{fig:task_example}](./img/task_example.pdf)

The graphs are generated by preferential attachment with out-degree 2 and $\alpha=3$[@petersonDistanceTwoRandom2015]. The graphs are then converted into undirected graphs by calculating the symmetric closure of $E$.

Observe that @eq:met-phik is of the form

$$ \exists v . (u, v) \in E \land \xi , $$ {#eq:met-graded}

where $v$ is free in $\xi$. This is known as graded normal form. Due to work by @barceloLogicalExpressivenessGraph2020 we therefore know that for any $k$ there exists a GCN of the architecture described below which is able to represent $\phi_k$ exactly.

However, we also note that $\phi_k$ has a problem radius equal to $k$. A GCN with $l$ layers will therefore not be able to represent $\phi_k$ for $k > \phi$, due to under-reaching. Furthermore, with increasing $k$, training difficulty will increase due to over-squashing[@alonBottleneckGraphNeural2021].

Each individual dataset was split into training, validation, and test sets using a 60%/30%/10% split.

## Experimental Setup

As shown in @tab:class-sizes, the data become more class imbalanced with increasing $k$. The models were therefore trained using a dice loss, which was designed to provide a strong gradient signal in imbalanced datasets in medical image analysis[@crumGeneralizedOverlapMeasures2006].

To tune the hyperparameters of the respective models, an exhaustive grid search was performed, with the possible value ranges indicated for each model individually. Note that the range of used parameters was limited by computational constraints. Hyperparameters were optimized separately for each dataset. All models were trained using an Adam optimizer over 100 epochs, with $\beta_1 = 0.9$ and $\beta_2 = 0.999$. The learning rate was a hyperparameter. (TODO, was it really just 100 epochs in the end?).

The input data are given by the adjacency matrix $\pmb A \in \mathbb{R}^{|V| \times |V|}$ and a feature vector $\pmb X \in \mathbb{R}^{|V| \times 6}$ which is a one-hot encoding of the color $c(v)$ for all $v \in V$.

### GRAFF

The GRAFF model consists of an encoder, a decoder, and an operation

$$ \text{GRAFF}: \pmb H (t + \tau) = \pmb H (t) + \tau \left( - \pmb H (t) \text{diag} (\pmb \omega) + \overline {\pmb A} \pmb H (t) \overline {\pmb W} - \beta \pmb H (0) \right), $$ {#eq:exp-graff}

where $\pmb \omega \in \mathbb {R}^{d_h}$, $\beta \in \mathbb R$, and $\pmb W \in \pmb{R}^{d_{hidden} \times d_{hidden}}$ are learned parameters, and the step length $\tau$ is a hyperparameter, as described in [@digiovanniGraphNeuralNetworks2022a, Eq. 17]. The matrix $\overline {\pmb A}$ is the degree-normalized adjacency matrix $\overline {\pmb A} = \pmb{D}^{-1/2} \pmb A \pmb{D}^{-1/2}$, with degree matrix $\pmb D$. The matrix $\pmb W$ is symmetric.

The encoder $\text{Enc} : \mathbb{R}^{d_{in}} \rightarrow \mathbb{R}^{d_{hidden}}$ and decoder $\text{Dec} : \mathbb{R}^{d_{hidden}} \rightarrow \mathbb{R}^{d_{out}}$ are both single perceptrons combined with a dropout layer. In addition, the decoder applies a `LeakyReLU` with negative slope 0.01, and a softmax to obtain class probabilities. The integers $d_{in}$, $d_{out}$, and $d_{hidden}$, are the input, output, and hidden dimensions, respectively. The dropout rate $d$ and $d_{out}$ are further hyperparameters.

The output $\pmb y$ of this model is calculated by setting

$$ \pmb H(0) = \text{Enc} ( \pmb X ), \quad \pmb y = \text{Dec} \left( \pmb H(T) \right), $$ {#eq:exp-wrapping-graff}

where the total time $T$ is a hyperparameter.

TODO: hyperparameteers tried, hyperparameters picked.

### MultiGRAFF

This is a natural extension of the GRAFF model, which layers $N$ GRAFF operations, and interleaves them with simple perceptrons and nonlinearities. $\text{Enc}$ and $\text{Dec}$ are defined as above, except that the decoder is simply given by the softmax function. In addition we use simple perceptrons $\text{Lin}_ i$ for $1 \leq i \leq N$. Then this model is given by

$$\text{MGF}: \text{Dec} \circ \text{Lin}_ N \circ \text{GRAFF}_ N \circ \text{Lin}_ {N-1} \circ \text{GRAFF}_ {N-1} \circ \cdots \circ \text{Lin}_ 1 \circ \text{GRAFF}_ 1 \circ \text{Enc},  $$ {#eq:exp-multigraff}

where, abusing notation, $\text{GRAFF}(\pmb{X})$ means setting $\pmb H (0) = \pmb X$, and applying @eq:exp-graff $T/\tau$ times, yielding $\pmb H (T)$ as the result (it is assumed that $\tau$ divides $T$). All the $\text{GRAFF}_ i$ operations use distinct parameters $\beta_ i$, $\pmb {\omega}_ i$, and $\pmb{W}_ i$, while $\tau$ and $T$ are shared hyperparameters.

* TODO: hyperparameteers tried, hyperparameters picked.
* TODO: this can be thought of as many channel mixing tasks split into subtasks or something

### SAGE GNN

The SAGE GNN is used[@hamiltonInductiveRepresentationLearning2018], as in the standard Torch Geometric implementation[@TorchGeometricNn]. The layers are given by

$$\mathbf{x}^{\prime}_i = \mathbf{W}_1 \mathbf{x}_i + \mathbf{W}_2 \cdot \mathrm{mean}_{j \in \mathcal{N(i)}} \mathbf{x}_j, $$

and they are interleaved with `ReLU` activations. The hyperparameters for this model are the size of the hidden layers, and the number of layers.

TODO: hyperparameteers tried, hyperparameters picked.

# Results

# Discussion and Outlook

* I did this thing, and showed the following.
* test results are reported for the

* Graphs show (maybe?) that we would need to train for longer to do the thing fully, but computational constraints. Similar for more thorough hyperparameter optimization.
* Also we now need to evaluate on real complex heterophilic tasks now, with much interdependencies

---

# Here's whhat plots/tables I need

* table about class imbalance
* plot of complexity vs test performance, for all three networks
