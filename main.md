---
title: "MultiGRAFF: Learning Complex Logical Tasks"
author: "Candidate number: 1045966"
bibliography: references.biblatex
reference-section-title: References
header-includes: |
    \institute{University of Oxford}

    \usepackage[margin=3cm]{geometry}
    \usepackage{caption}
    \usepackage{xcolor}
    \usepackage{graphicx}

    \usepackage{fancyhdr}
    \pagestyle{fancy}
    \lhead{}
    \rhead{}
    \chead{}
    \renewcommand\headrulewidth{0.1pt}
    \renewcommand\footrulewidth{0pt}
    \fancyhead[LO,LE]{Geometric Deep Learning}
    \fancyhead[RO,RE]{Candidate: 1045966}
documentclass: llncs
twocolumn: false
fontsize: 11pt
numbersections: false
toc: false
---

# Introduction

Graph neural networks (GNNs) have been used successfully on a wide range of applications, including learning social network representations[@wangMCNEEndtoEndFramework2019], image segmentation[@aflaloDeepCutUnsupervisedSegmentation2022], and prediction protein interactions[@jhaPredictionProteinProtein2022]. Geometric deep learning analyses deep neural networks, including GNNs, by looking at the geometry of the underlying spaces[@bronsteinGeometricDeepLearning2017]. One paper in this field studied GNNs by viewing them as discretized descriptions of particle interactions, and derived the GRAFF layer[@digiovanniGraphNeuralNetworks2022a]. Apart from wide-ranging theoretical guarantees, variants of GRAFF can potentially be implemented very efficiently, and match or beat state-of-the art models on both homophilic and heterophilic tasks. However, the relatively low parameter count and diffusion-based design does not fit the intuition of a GNN discretely "processing" graph data. This suggests that it would have difficulties with complex tasks with larger problem radii.

This study investigates this hypothesis by evaluating a GRAFF-based network against a GNN benchmark on a (to my knowledge) novel class of synthetic datasets which are based on logical expressions of varying length. This length is taken as a proxy for task complexity. In addition, a new MultiGRAFF architecture based on the composition of different GRAFF layers is proposed, and evaluated on the same dataset.

I find that on this particular class of long range task, MultiGRAFF and GNN perform about similarly, and both outperform a GRAFF-based network.

# Methodology

To establish how GRAFF and the novel MultiGRAFF perform for tasks of different complexities, they were trained and evaluated on a synthetic dataset. In addition, the standard SASGE GNN was trained to provide a benchmark[@hamiltonInductiveRepresentationLearning2018].

## Synthetic Dataset

To test model performance across different task complexities, a range of 6 synthetic datasets was generated. Each dataset consists of 50 graphs $G=(V,E,c)$, with nodes $V$, edges $E$ and node colors $c: V \rightarrow \left\{ 1, 2, \ldots, 6 \right\}$. The tasks are all binary node classifications tasks, where the class of a node for the $k$-th dataset is given by a logical expression $\phi_k : V \rightarrow \mathbb B$, for $1 \leq k \leq 6$.

The first expression is

$$ \phi_1 (v) = \left[ c(v) = 1 \right], $$ {#eq:met-phi1}

indicating that $\phi_1 (v)$ is true _iff_ $v$ has the first color. All following expressions are defined inductively by

$$ \phi_{k+1}(u) = \left[ \exists v . (u, v) \in E \land c(u) = k+1 \land \phi_k(v) . \right]$$ {#eq:met-phik}

For an example task, see [@fig:the_figure]  (A, B). The task is designed so that the proportion of nodes with a `true` label drops off with increasing $k$ (see tab. \ref{tab:class-sizes}).

\begin{figure*}
\includegraphics[width=\textwidth]{./img/the_figure.pdf} %*
\caption{(A, B) Illustration of $\phi_1$ (A) and $\phi_3$ (B). The order of colors is 1=red, 2=green, 3=blue, 4=purple. Big nodes evaluate to true, small to false. In plain English, the left task is simply \textit{all red nodes}. The right task translates to \textit{all blue nodes, neighbouring [a green node neighbouring a red node]}. (C) Performance of different models on tasks of differing complexity. The y axis is the dice score, x axis is the complexity $k$. The lines indicate mean performance of 5 models, the error bars are centred on the mean and show one standard deviation in both directions. The benchmark is the dice score of a classifier which always guesses the larger class. \label{fig:the_figure}}
\end{figure*}

The graphs are generated by preferential attachment with out-degree 2 and $\alpha=3$[@petersonDistanceTwoRandom2015]. The graphs are then converted into undirected graphs by calculating the symmetric closure of $E$.

Observe that @eq:met-phik is of the form

$$ \exists v . (u, v) \in E \land \xi , $$ {#eq:met-graded}

where $v$ is free in $\xi$. This is known as graded normal form. Due to work by @barceloLogicalExpressivenessGraph2020 we therefore know that for any $k$ there exists a GCN of the architecture described below which is able to represent $\phi_k$ exactly.

However, we also note that $\phi_k$ has a problem radius equal to $k$. A GCN with $l$ layers will therefore not be able to represent $\phi_k$ for $k > \phi$, due to under-reaching. Furthermore, with increasing $k$, training difficulty will also rise due to over-squashing[@alonBottleneckGraphNeural2021] and class imbalance.

\begin{table}
\begin{tabular*}{\linewidth}{@{\extracolsep{\fill}}c | r r r r r r}
$k$   & 1 & 2 & 3 & 4 & 5 & 6 \\
\hline
yes-labels [\%] & 15.8 & 8.4 & 2.8 & 3.4 & 1.2 & 1.4 \\
\end{tabular*}
\caption{Fraction of nodes $v$ for which $\phi_k (v) = \texttt{true}$ in each of the $k$ datasets. \label{tab:class-sizes}}
\end{table}

Each individual dataset was split into training, validation, and test sets using a 60%/30%/10% split.

## Experimental Setup

As shown in tab. \ref{tab:class-sizes}, the data become more class imbalanced with increasing $k$. The models were therefore trained using a dice loss, which was designed to provide a strong gradient signal in imbalanced datasets in medical image analysis[@crumGeneralizedOverlapMeasures2006].

To tune the hyperparameters of the respective models, an exhaustive grid search was performed, with the possible value ranges indicated for each model individually. Hyperparameters were optimized separately for each dataset. All models were trained using an Adam optimizer over 100 epochs, with $\beta_1 = 0.9$ and $\beta_2 = 0.999$.

The input data are given by the adjacency matrix $\pmb A \in \mathbb{R}^{|V| \times |V|}$ and a feature vector $\pmb X \in \mathbb{R}^{|V| \times 6}$ which consists of one-hot encodings of the color $c(v)$ for all $v \in V$.

### GRAFF

The GRAFF model consists of an encoder, a decoder, and an operation

$$ \text{GRAFF}: \pmb H (t + \tau) = \pmb H (t) + \tau \left( - \pmb H (t) \text{diag} (\pmb \omega) + \overline {\pmb A} \pmb H (t) \overline {\pmb W} - \beta \pmb H (0) \right), $$ {#eq:exp-graff}

where $\pmb \omega \in \mathbb {R}^{d_h}$, $\beta \in \mathbb R$, and $\pmb W \in \pmb{R}^{d_{hidden} \times d_{hidden}}$ are learned parameters, and the step length $\tau$ is a hyperparameter, as described in [@digiovanniGraphNeuralNetworks2022a, Eq. 17]. The matrix $\overline {\pmb A}$ is the degree-normalized adjacency matrix $\overline {\pmb A} = \pmb{D}^{-1/2} \pmb A \pmb{D}^{-1/2}$, with degree matrix $\pmb D$. The matrix $\pmb W$ is symmetric.

The encoder $\text{Enc} : \mathbb{R}^{d_{in}} \rightarrow \mathbb{R}^{d_{hidden}}$ and decoder $\text{Dec} : \mathbb{R}^{d_{hidden}} \rightarrow \mathbb{R}^{d_{out}}$ are both single perceptrons combined with a dropout layer. In addition, the decoder applies a `LeakyReLU` with negative slope 0.01, and a softmax to obtain class probabilities. The integers $d_{in}$, $d_{out}$, and $d_{hidden}$, are the input, output, and hidden dimensions, respectively. The dropout rate $d$ and $d_{out}$ are further hyperparameters.

The output $\pmb y$ of this model is calculated by setting

$$ \pmb H(0) = \text{Enc} ( \pmb X ), \quad \pmb y = \text{Dec} \left( \pmb H(T) \right), $$ {#eq:exp-wrapping-graff}

where the total time $T$ is a hyperparameter.

<!--The hyperparameters were picked out of the following ranges: $T \in \left\{ 2,3 \right\}$, $\tau \in \left\{ 0.25, 0.5 \right\}$, $\textt{dropout} \in \left\{0, 0.25, 0.5, 0.75\right\}$. -->

The following hyperparameters produced the best results on the validation sets on average: $\texttt{dropout}=0.25$, learning rate 0.002, $d_{hidden}=8$ for all $k$; $T=2$ for $k=1$, $T=3$ for the remaining datasets; $\tau = 0.25$ for $k \in \left\{1,5,6\right\}$, $\tau=0.5$ for the remaining datasets.

### MultiGRAFF

This is a natural extension of the GRAFF model, which layers $N$ GRAFF operations, and interleaves them with simple perceptrons and nonlinearities. $\text{Enc}$ and $\text{Dec}$ are defined as above, except that the decoder is simply given by the softmax function. In addition we use simple perceptrons $\text{SLP}_ i$ for $1 \leq i \leq N$. The model is given by

$$\text{MGF}: \text{Dec} \circ \text{SLP}_ N \circ \text{GRAFF}_ N \circ \text{SLP}_ {N-1} \circ \text{GRAFF}_ {N-1} \circ \cdots \circ \text{SLP}_ 1 \circ \text{GRAFF}_ 1 \circ \text{Enc},  $$ {#eq:exp-multigraff}

where, abusing notation, $\text{GRAFF}(\pmb{X})$ means setting $\pmb H (0) = \pmb X$, and applying @eq:exp-graff $T/\tau$ times, yielding $\pmb H (T)$ as the result (it is assumed that $\tau$ divides $T$). All the $\text{GRAFF}_ i$ operations use distinct parameters $\beta_ i$, $\pmb {\omega}_ i$, and $\pmb{W}_ i$, while $\tau$ and $T$ are shared hyperparameters. Optionally, the perceptron weights can be shared, in which case $\text{SLP}_ i = \text{SLP}_ j$ for all $i, j$.

This extension of GRAFF fits well into the deep learning blueprint: Since both the $\text{GRAFF}$ operation and the $\text{SLP}$ are permutation equivariant, the MultiGRAFF also has this property.

The following hyperparameters achieved best average performance on the validation sets: learning rate 0.01 and $d_{hidden}=8$ for all datasets; $T=2$ for $k \in \left\{ 1,2,5,6 \right\}$; $T=3$ elsewhere; $\tau = 1$ for $k \in \left\{3,6\right\}$, $\tau=0.5$ elsewhere; $\texttt{num\_layers} = 4$ for $k \in \left\{3,5\right\}$; the equivariant transformation was shared for $k \in \left\{ 4,6\right\}$, and not shared elsewhere.

### SAGE GNN

The SAGE GNN is used[@hamiltonInductiveRepresentationLearning2018], as in the standard Torch Geometric implementation[@TorchGeometricNn]. The layers are given by

$$\mathbf{h}^{\prime}_u = \mathbf{W}_1 \mathbf{h}_u + \mathbf{W}_2 \cdot \mathrm{mean}_{v \in \mathcal{N}(u)} \mathbf{h}_v, $$ {#eq:exp-sage}

where $\pmb{h}_ u$ and $\pmb{h}'_ v$ are the hidden features of node $u$ for the current and next layer, respectively. The layers are interleaved with `ReLU` activations. The hyperparameters for this model are the size of the hidden layers, and the number of layers.

The following hyperparameters achieved the best average performance on the validation sets: learning rate 0.01, $d_{hidden}=4$ across all datasets; $\texttt{num\_layers}=3$ for $T \in \left\{1,2\right\}$, $\texttt{num\_layers}=4$ for $T \in \left\{3,6\right\}$, $\texttt{num\_layers} = 5$ elsewhere.

# Results

The experiment results are shown in @fig:the_figure (C). The models show an overall drop-off in performance as task complexity increases, which shows that the complexity measure used in this study does in fact correspond to difficulty. All models perform considerably better than guessing the distribution mode, indicating that they have learned some relevant features. There were considerable differences in model performance for the same task, and choice of hyperparameters, with dice score differences of 0.1 or more.

MultiGRAFF and SAGE show no statistically significant difference in performance, but both models outperform GRAFF. This confirms both hypotheses of this paper: that GRAFF would struggle with tasks of large logical complexity, and that this issue could be mitigated by layering multiple different GRAFF networks on top of each other. However, MultiGRAFF is more computationally expensive in both training and usage, taking more than three times longer than SAGE on all tasks.


* relate this to existing litterature??
    * maybe that it works because it's permutation equivariant?
    * maybe something about how the task is like (low- vs high-frequency)
    * maybe something abou class-imbalance
    * or over-squashing
    * talk about how SAGE is a SOTA benchmark (this is actually a good idea)

# Discussion and Outlook

The study investigated whether a GRAFF-based network could perform well on a specific class of complex tasks, and whether performance could be increased by MultiGRAFF, a layering of several GRAFF networks. To establish this, a novel synthetic benchmark was created. Experiments showed that GRAFF is indeed outperformed on the benchmark by standard SAGE model, and that MultiGRAFF is able to match SAGE. However, MultiGRAFF falls behind SAGE in performance by about a factor of three, and is therefore not a competitive model architecture in its current form. One direction for making it so could be to deploy it on longer-range tasks, where it could potentially make due with much fewer layers than a SAGE-like architecture.

While this study provides insights into model performance on complex logical tasks, I suggest three things to gain more confidence in the claims of this work. First, more thorough hyperparameter optimization should be done, and the models should be run for more epochs, seeing as the experiments were constrained by available computing power. Second, the synthetic benchmark combined both class imbalance and increased logical complexity, and it would be interesting to study the effect of both  of those factors on model performance in isolation. Third, examination of loss curves showed that the variance in MultiGRAFF model performance is partially due to exploding or vanishing gradients on a small fraction of models. This issue could be addressed, for instance, through more principled parameter initialization schemes.

While some practical issues remain, MultiGRAFF provides a model architecture that can match state-of-the-art GNNs on complex tasks, that also consists of components which lend itself to principled, formal analysis.
