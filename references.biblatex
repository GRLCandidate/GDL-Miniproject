@online{abboudShortestPathNetworks2022,
  title = {Shortest {{Path Networks}} for {{Graph Property Prediction}}},
  author = {Abboud, Ralph and Dimitrov, Radoslav and Ceylan, İsmail İlkan},
  date = {2022-11-29},
  number = {arXiv:2206.01003},
  eprint = {arXiv:2206.01003},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.2206.01003},
  url = {http://arxiv.org/abs/2206.01003},
  urldate = {2022-12-07},
  abstract = {Most graph neural network models rely on a particular message passing paradigm, where the idea is to iteratively propagate node representations of a graph to each node in the direct neighborhood. While very prominent, this paradigm leads to information propagation bottlenecks, as information is repeatedly compressed at intermediary node representations, which causes loss of information, making it practically impossible to gather meaningful signals from distant nodes. To address this, we propose shortest path message passing neural networks, where the node representations of a graph are propagated to each node in the shortest path neighborhoods. In this setting, nodes can directly communicate between each other even if they are not neighbors, breaking the information bottleneck and hence leading to more adequately learned representations. Our framework generalizes message passing neural networks, resulting in a class of more expressive models, including some recent state-of-the-art models. We verify the capacity of a basic model of this framework on dedicated synthetic experiments, and on real-world graph classification and regression benchmarks, and obtain state-of-the art results.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/olaf/Zotero/storage/RK9FNG7J/Abboud et al. - 2022 - Shortest Path Networks for Graph Property Predicti.pdf;/home/olaf/Zotero/storage/D68RXJW9/2206.html}
}

@online{alonBottleneckGraphNeural2021,
  title = {On the {{Bottleneck}} of {{Graph Neural Networks}} and Its {{Practical Implications}}},
  author = {Alon, Uri and Yahav, Eran},
  date = {2021-03-09},
  number = {arXiv:2006.05205},
  eprint = {arXiv:2006.05205},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.2006.05205},
  url = {http://arxiv.org/abs/2006.05205},
  urldate = {2022-12-07},
  abstract = {Since the proposal of the graph neural network (GNN) by Gori et al. (2005) and Scarselli et al. (2008), one of the major problems in training GNNs was their struggle to propagate information between distant nodes in the graph. We propose a new explanation for this problem: GNNs are susceptible to a bottleneck when aggregating messages across a long path. This bottleneck causes the over-squashing of exponentially growing information into fixed-size vectors. As a result, GNNs fail to propagate messages originating from distant nodes and perform poorly when the prediction task depends on long-range interaction. In this paper, we highlight the inherent problem of over-squashing in GNNs: we demonstrate that the bottleneck hinders popular GNNs from fitting long-range signals in the training data; we further show that GNNs that absorb incoming edges equally, such as GCN and GIN, are more susceptible to over-squashing than GAT and GGNN; finally, we show that prior work, which extensively tuned GNN models of long-range problems, suffers from over-squashing, and that breaking the bottleneck improves their state-of-the-art results without any tuning or additional weights. Our code is available at https://github.com/tech-srl/bottleneck/ .},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/olaf/Zotero/storage/C56UA6VN/Alon et Yahav - 2021 - On the Bottleneck of Graph Neural Networks and its.pdf;/home/olaf/Zotero/storage/BYXK9VUU/2006.html}
}

@online{arnaiz-rodriguezDiffWireInductiveGraph2022,
  title = {{{DiffWire}}: {{Inductive Graph Rewiring}} via the {{Lov}}\textbackslash 'asz {{Bound}}},
  shorttitle = {{{DiffWire}}},
  author = {Arnaiz-Rodriguez, Adrian and Begga, Ahmed and Escolano, Francisco and Oliver, Nuria},
  date = {2022-06-15},
  number = {arXiv:2206.07369},
  eprint = {arXiv:2206.07369},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.2206.07369},
  url = {http://arxiv.org/abs/2206.07369},
  urldate = {2022-12-07},
  abstract = {Graph Neural Networks (GNNs) have been shown to achieve competitive results to tackle graph-related tasks, such as node and graph classification, link prediction and node and graph clustering in a variety of domains. Most GNNs use a message passing framework and hence are called MPNNs. Despite their promising results, MPNNs have been reported to suffer from over-smoothing, over-squashing and under-reaching. Graph rewiring and graph pooling have been proposed in the literature as solutions to address these limitations. However, most state-of-the-art graph rewiring methods fail to preserve the global topology of the graph, are neither differentiable nor inductive, and require the tuning of hyper-parameters. In this paper, we propose DiffWire, a novel framework for graph rewiring in MPNNs that is principled, fully differentiable and parameter-free by leveraging the Lov\textbackslash 'asz bound. The proposed approach provides a unified theory for graph rewiring by proposing two new, complementary layers in MPNNs: CT-Layer, a layer that learns the commute times and uses them as a relevance function for edge re-weighting; and GAP-Layer, a layer to optimize the spectral gap, depending on the nature of the network and the task at hand. We empirically validate the value of each of these layers separately with benchmark datasets for graph classification. We also perform preliminary studies on the use of CT-Layer for homophilic and heterophilic node classification tasks. DiffWire brings together the learnability of commute times to related definitions of curvature, opening the door to creating more expressive MPNNs.},
  pubstate = {preprint},
  version = {1},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/olaf/Zotero/storage/UM92SZTS/Arnaiz-Rodriguez et al. - 2022 - DiffWire Inductive Graph Rewiring via the Lov'as.pdf;/home/olaf/Zotero/storage/M68L2LHF/2206.html}
}

@online{bahdanauNeuralMachineTranslation2016,
  title = {Neural {{Machine Translation}} by {{Jointly Learning}} to {{Align}} and {{Translate}}},
  author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  date = {2016-05-19},
  number = {arXiv:1409.0473},
  eprint = {arXiv:1409.0473},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.1409.0473},
  url = {http://arxiv.org/abs/1409.0473},
  urldate = {2022-11-21},
  abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/home/olaf/Zotero/storage/MUHD29X2/Bahdanau et al. - 2016 - Neural Machine Translation by Jointly Learning to .pdf;/home/olaf/Zotero/storage/EYAIIHD7/1409.html}
}

@inproceedings{barceloLogicalExpressivenessGraph2020,
  title = {The {{Logical Expressiveness}} of {{Graph Neural Networks}}},
  author = {Barceló, Pablo and Kostylev, Egor V. and Monet, Mikael and Pérez, Jorge and Reutter, Juan and Silva, Juan Pablo},
  date = {2020-03-11},
  url = {https://openreview.net/forum?id=r1lZ7AEKvB},
  urldate = {2022-12-06},
  abstract = {The ability of graph neural networks (GNNs) for distinguishing nodes in graphs has been recently characterized in terms of the Weisfeiler-Lehman (WL) test for checking graph isomorphism. This characterization, however, does not settle the issue of which Boolean node classifiers (i.e., functions classifying nodes in graphs as true or false) can be expressed by GNNs. We tackle this problem by focusing on Boolean classifiers expressible as formulas in the logic FOC2, a well-studied fragment of first order logic. FOC2 is tightly related to the WL test, and hence to GNNs. We start by studying a popular class of GNNs, which we call AC-GNNs, in which the features of each node in the graph are updated, in successive layers, only in terms of the features of its neighbors. We show that this class of GNNs is too weak to capture all FOC2 classifiers, and provide a syntactic characterization of the largest subclass of FOC2 classifiers that can be captured by AC-GNNs. This subclass coincides with a logic heavily used by the knowledge representation community. We then look at what needs to be added to AC-GNNs for capturing all FOC2 classifiers. We show that it suffices to add readout functions, which allow to update the features of a node not only in terms of its neighbors, but also in terms of a global attribute vector. We call GNNs of this kind ACR-GNNs. We experimentally validate our findings showing that, on synthetic data conforming to FOC2 formulas, AC-GNNs struggle to fit the training data while ACR-GNNs can generalize even to graphs of sizes not seen during training.},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  langid = {english},
  file = {/home/olaf/Zotero/storage/VFC7HAQZ/Barceló et al. - 2020 - The Logical Expressiveness of Graph Neural Network.pdf;/home/olaf/Zotero/storage/E7ZWNDHQ/forum.html}
}

@online{barceloWeisfeilerLemanGo2022,
  title = {Weisfeiler and {{Leman Go Relational}}},
  author = {Barcelo, Pablo and Galkin, Mikhail and Morris, Christopher and Orth, Miguel Romero},
  date = {2022-11-30},
  number = {arXiv:2211.17113},
  eprint = {arXiv:2211.17113},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.2211.17113},
  url = {http://arxiv.org/abs/2211.17113},
  urldate = {2023-03-20},
  abstract = {Knowledge graphs, modeling multi-relational data, improve numerous applications such as question answering or graph logical reasoning. Many graph neural networks for such data emerged recently, often outperforming shallow architectures. However, the design of such multi-relational graph neural networks is ad-hoc, driven mainly by intuition and empirical insights. Up to now, their expressivity, their relation to each other, and their (practical) learning performance is poorly understood. Here, we initiate the study of deriving a more principled understanding of multi-relational graph neural networks. Namely, we investigate the limitations in the expressive power of the well-known Relational GCN and Compositional GCN architectures and shed some light on their practical learning performance. By aligning both architectures with a suitable version of the Weisfeiler-Leman test, we establish under which conditions both models have the same expressive power in distinguishing non-isomorphic (multi-relational) graphs or vertices with different structural roles. Further, by leveraging recent progress in designing expressive graph neural networks, we introduce the \$k\$-RN architecture that provably overcomes the expressiveness limitations of the above two architectures. Empirically, we confirm our theoretical findings in a vertex classification setting over small and large multi-relational graphs.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/home/olaf/Zotero/storage/A3Q6JPWQ/Barcelo et al. - 2022 - Weisfeiler and Leman Go Relational.pdf;/home/olaf/Zotero/storage/ESHKUJEJ/2211.html}
}

@article{barronApproximationEstimationBounds1994,
  title = {Approximation and Estimation Bounds for Artificial Neural Networks},
  author = {Barron, Andrew R.},
  date = {1994-01-01},
  journaltitle = {Machine Learning},
  shortjournal = {Mach Learn},
  volume = {14},
  number = {1},
  pages = {115--133},
  issn = {1573-0565},
  doi = {10.1007/BF00993164},
  url = {https://doi.org/10.1007/BF00993164},
  urldate = {2022-12-05},
  abstract = {For a common class of artificial neural networks, the mean integrated squared error between the estimated network and a target functionf is shown to be bounded by\$\$O\textbackslash left( \{\textbackslash frac\{\{\textbackslash mathop c\textbackslash nolimits\_f\^2 \}\}\{n\}\} \textbackslash right) + O\textbackslash left( \{\textbackslash frac\{\{nd\}\}\{N\}\textbackslash log  N\} \textbackslash right)\$\$wheren is the number of nodes,d is the input dimension of the function,N is the number of training observations, andCfis the first absolute moment of the Fourier magnitude distribution off. The two contributions to this total risk are the approximation error and the estimation error. Approximation error refers to the distance between the target function and the closest neural network function of a given architecture and estimation error refers to the distance between this ideal network function and an estimated network function. Withn ∼ Cf(N/(d logN))1/2 nodes, the order of the bound on the mean integrated squared error is optimized to beO(Cf((d/N) logN)1/2). The bound demonstrates surprisingly favorable properties of network estimation compared to traditional series and nonparametric curve estimation techniques in the case thatd is moderately large. Similar bounds are obtained when the number of nodesn is not preselected as a function ofCf(which is generally not knowna priori), but rather the number of nodes is optimized from the observed data by the use of a complexity regularization or minimum description length criterion. The analysis involves Fourier techniques for the approximation error, metric entropy considerations for the estimation error, and a calculation of the index of resolvability of minimum complexity estimation of the family of networks.},
  langid = {english},
  keywords = {approximation theory,complexity regularization,estimation theory,Neural nets,statistical risk},
  file = {/home/olaf/Zotero/storage/X6WLASD7/Barron - 1994 - Approximation and estimation bounds for artificial.pdf}
}

@online{battagliaRelationalInductiveBiases2018,
  title = {Relational Inductive Biases, Deep Learning, and Graph Networks},
  author = {Battaglia, Peter W. and Hamrick, Jessica B. and Bapst, Victor and Sanchez-Gonzalez, Alvaro and Zambaldi, Vinicius and Malinowski, Mateusz and Tacchetti, Andrea and Raposo, David and Santoro, Adam and Faulkner, Ryan and Gulcehre, Caglar and Song, Francis and Ballard, Andrew and Gilmer, Justin and Dahl, George and Vaswani, Ashish and Allen, Kelsey and Nash, Charles and Langston, Victoria and Dyer, Chris and Heess, Nicolas and Wierstra, Daan and Kohli, Pushmeet and Botvinick, Matt and Vinyals, Oriol and Li, Yujia and Pascanu, Razvan},
  date = {2018-10-17},
  number = {arXiv:1806.01261},
  eprint = {arXiv:1806.01261},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.1806.01261},
  url = {http://arxiv.org/abs/1806.01261},
  urldate = {2022-12-08},
  abstract = {Artificial intelligence (AI) has undergone a renaissance recently, making major progress in key domains such as vision, language, control, and decision-making. This has been due, in part, to cheap data and cheap compute resources, which have fit the natural strengths of deep learning. However, many defining characteristics of human intelligence, which developed under much different pressures, remain out of reach for current approaches. In particular, generalizing beyond one's experiences--a hallmark of human intelligence from infancy--remains a formidable challenge for modern AI. The following is part position paper, part review, and part unification. We argue that combinatorial generalization must be a top priority for AI to achieve human-like abilities, and that structured representations and computations are key to realizing this objective. Just as biology uses nature and nurture cooperatively, we reject the false choice between "hand-engineering" and "end-to-end" learning, and instead advocate for an approach which benefits from their complementary strengths. We explore how using relational inductive biases within deep learning architectures can facilitate learning about entities, relations, and rules for composing them. We present a new building block for the AI toolkit with a strong relational inductive bias--the graph network--which generalizes and extends various approaches for neural networks that operate on graphs, and provides a straightforward interface for manipulating structured knowledge and producing structured behaviors. We discuss how graph networks can support relational reasoning and combinatorial generalization, laying the foundation for more sophisticated, interpretable, and flexible patterns of reasoning. As a companion to this paper, we have released an open-source software library for building graph networks, with demonstrations of how to use them in practice.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/olaf/Zotero/storage/Y8GBIIQB/Battaglia et al. - 2018 - Relational inductive biases, deep learning, and gr.pdf;/home/olaf/Zotero/storage/FPMSCZJE/1806.html}
}

@online{bevilacquaEquivariantSubgraphAggregation2022,
  title = {Equivariant {{Subgraph Aggregation Networks}}},
  author = {Bevilacqua, Beatrice and Frasca, Fabrizio and Lim, Derek and Srinivasan, Balasubramaniam and Cai, Chen and Balamurugan, Gopinath and Bronstein, Michael M. and Maron, Haggai},
  date = {2022-03-16},
  number = {arXiv:2110.02910},
  eprint = {arXiv:2110.02910},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.2110.02910},
  url = {http://arxiv.org/abs/2110.02910},
  urldate = {2023-03-20},
  abstract = {Message-passing neural networks (MPNNs) are the leading architecture for deep learning on graph-structured data, in large part due to their simplicity and scalability. Unfortunately, it was shown that these architectures are limited in their expressive power. This paper proposes a novel framework called Equivariant Subgraph Aggregation Networks (ESAN) to address this issue. Our main observation is that while two graphs may not be distinguishable by an MPNN, they often contain distinguishable subgraphs. Thus, we propose to represent each graph as a set of subgraphs derived by some predefined policy, and to process it using a suitable equivariant architecture. We develop novel variants of the 1-dimensional Weisfeiler-Leman (1-WL) test for graph isomorphism, and prove lower bounds on the expressiveness of ESAN in terms of these new WL variants. We further prove that our approach increases the expressive power of both MPNNs and more expressive architectures. Moreover, we provide theoretical results that describe how design choices such as the subgraph selection policy and equivariant neural architecture affect our architecture's expressive power. To deal with the increased computational cost, we propose a subgraph sampling scheme, which can be viewed as a stochastic version of our framework. A comprehensive set of experiments on real and synthetic datasets demonstrates that our framework improves the expressive power and overall performance of popular GNN architectures.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/olaf/Zotero/storage/TV444THT/Bevilacqua et al. - 2022 - Equivariant Subgraph Aggregation Networks.pdf;/home/olaf/Zotero/storage/6QTG32X9/2110.html}
}

@article{bleiVariationalInferenceReview2017,
  title = {Variational {{Inference}}: {{A Review}} for {{Statisticians}}},
  shorttitle = {Variational {{Inference}}},
  author = {Blei, David M. and Kucukelbir, Alp and McAuliffe, Jon D.},
  date = {2017-04-03},
  journaltitle = {Journal of the American Statistical Association},
  shortjournal = {Journal of the American Statistical Association},
  volume = {112},
  number = {518},
  eprint = {1601.00670},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  pages = {859--877},
  issn = {0162-1459, 1537-274X},
  doi = {10.1080/01621459.2017.1285773},
  url = {http://arxiv.org/abs/1601.00670},
  urldate = {2022-11-22},
  abstract = {One of the core problems of modern statistics is to approximate difficult-to-compute probability densities. This problem is especially important in Bayesian statistics, which frames all inference about unknown quantities as a calculation involving the posterior density. In this paper, we review variational inference (VI), a method from machine learning that approximates probability densities through optimization. VI has been used in many applications and tends to be faster than classical methods, such as Markov chain Monte Carlo sampling. The idea behind VI is to first posit a family of densities and then to find the member of that family which is close to the target. Closeness is measured by Kullback-Leibler divergence. We review the ideas behind mean-field variational inference, discuss the special case of VI applied to exponential family models, present a full example with a Bayesian mixture of Gaussians, and derive a variant that uses stochastic optimization to scale up to massive data. We discuss modern research in VI and highlight important open problems. VI is powerful, but it is not yet well understood. Our hope in writing this paper is to catalyze statistical research on this class of algorithms.},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Computation,Statistics - Machine Learning},
  file = {/home/olaf/Zotero/storage/XX7MT4QE/Blei et al. - 2017 - Variational Inference A Review for Statisticians.pdf}
}

@online{bodnarWeisfeilerLehmanGo2022,
  title = {Weisfeiler and {{Lehman Go Cellular}}: {{CW Networks}}},
  shorttitle = {Weisfeiler and {{Lehman Go Cellular}}},
  author = {Bodnar, Cristian and Frasca, Fabrizio and Otter, Nina and Wang, Yu Guang and Liò, Pietro and Montúfar, Guido and Bronstein, Michael},
  date = {2022-01-31},
  number = {arXiv:2106.12575},
  eprint = {arXiv:2106.12575},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.2106.12575},
  url = {http://arxiv.org/abs/2106.12575},
  urldate = {2023-03-20},
  abstract = {Graph Neural Networks (GNNs) are limited in their expressive power, struggle with long-range interactions and lack a principled way to model higher-order structures. These problems can be attributed to the strong coupling between the computational graph and the input graph structure. The recently proposed Message Passing Simplicial Networks naturally decouple these elements by performing message passing on the clique complex of the graph. Nevertheless, these models can be severely constrained by the rigid combinatorial structure of Simplicial Complexes (SCs). In this work, we extend recent theoretical results on SCs to regular Cell Complexes, topological objects that flexibly subsume SCs and graphs. We show that this generalisation provides a powerful set of graph "lifting" transformations, each leading to a unique hierarchical message passing procedure. The resulting methods, which we collectively call CW Networks (CWNs), are strictly more powerful than the WL test and not less powerful than the 3-WL test. In particular, we demonstrate the effectiveness of one such scheme, based on rings, when applied to molecular graph problems. The proposed architecture benefits from provably larger expressivity than commonly used GNNs, principled modelling of higher-order signals and from compressing the distances between nodes. We demonstrate that our model achieves state-of-the-art results on a variety of molecular datasets.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/olaf/Zotero/storage/XC4RDMIX/Bodnar et al. - 2022 - Weisfeiler and Lehman Go Cellular CW Networks.pdf;/home/olaf/Zotero/storage/4GWLK27H/2106.html}
}

@incollection{bousquetIntroductionStatisticalLearning2004,
  title = {Introduction to {{Statistical Learning Theory}}},
  booktitle = {Advanced {{Lectures}} on {{Machine Learning}}},
  author = {Bousquet, Olivier and Boucheron, Stéphane and Lugosi, Gábor},
  editor = {Bousquet, Olivier and family=Luxburg, given=Ulrike, prefix=von, useprefix=true and Rätsch, Gunnar},
  date = {2004},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {3176},
  pages = {169--207},
  publisher = {{Springer Berlin Heidelberg}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-28650-9_8},
  url = {http://link.springer.com/10.1007/978-3-540-28650-9_8},
  urldate = {2023-01-18},
  abstract = {The goal of statistical learning theory is to study, in a statistical framework, the properties of learning algorithms. In particular, most results take the form of so-called error bounds. This tutorial introduces the techniques that are used to obtain such results.},
  isbn = {978-3-540-23122-6 978-3-540-28650-9},
  langid = {english},
  file = {/home/olaf/Zotero/storage/YQHU3RAZ/Bousquet et al. - 2004 - Introduction to Statistical Learning Theory.pdf}
}

@article{bronsteinGeometricDeepLearning2017,
  title = {Geometric Deep Learning: Going beyond {{Euclidean}} Data},
  shorttitle = {Geometric Deep Learning},
  author = {Bronstein, Michael M. and Bruna, Joan and LeCun, Yann and Szlam, Arthur and Vandergheynst, Pierre},
  date = {2017-07},
  journaltitle = {IEEE Signal Processing Magazine},
  shortjournal = {IEEE Signal Process. Mag.},
  volume = {34},
  number = {4},
  eprint = {1611.08097},
  eprinttype = {arxiv},
  eprintclass = {cs},
  pages = {18--42},
  issn = {1053-5888, 1558-0792},
  doi = {10.1109/MSP.2017.2693418},
  url = {http://arxiv.org/abs/1611.08097},
  urldate = {2022-12-07},
  abstract = {Many scientific fields study data with an underlying structure that is a non-Euclidean space. Some examples include social networks in computational social sciences, sensor networks in communications, functional networks in brain imaging, regulatory networks in genetics, and meshed surfaces in computer graphics. In many applications, such geometric data are large and complex (in the case of social networks, on the scale of billions), and are natural targets for machine learning techniques. In particular, we would like to use deep neural networks, which have recently proven to be powerful tools for a broad range of problems from computer vision, natural language processing, and audio analysis. However, these tools have been most successful on data with an underlying Euclidean or grid-like structure, and in cases where the invariances of these structures are built into networks used to model them. Geometric deep learning is an umbrella term for emerging techniques attempting to generalize (structured) deep neural models to non-Euclidean domains such as graphs and manifolds. The purpose of this paper is to overview different examples of geometric deep learning problems and present available solutions, key difficulties, applications, and future research directions in this nascent field.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/olaf/Zotero/storage/RESE47JZ/Bronstein et al. - 2017 - Geometric deep learning going beyond Euclidean da.pdf;/home/olaf/Zotero/storage/49FJ7ANW/1611.html}
}

@online{bronsteinGeometricDeepLearning2021,
  title = {Geometric {{Deep Learning}}: {{Grids}}, {{Groups}}, {{Graphs}}, {{Geodesics}}, and {{Gauges}}},
  shorttitle = {Geometric {{Deep Learning}}},
  author = {Bronstein, Michael M. and Bruna, Joan and Cohen, Taco and Veličković, Petar},
  date = {2021-05-02},
  number = {arXiv:2104.13478},
  eprint = {arXiv:2104.13478},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.2104.13478},
  url = {http://arxiv.org/abs/2104.13478},
  urldate = {2023-01-18},
  abstract = {The last decade has witnessed an experimental revolution in data science and machine learning, epitomised by deep learning methods. Indeed, many high-dimensional learning tasks previously thought to be beyond reach -- such as computer vision, playing Go, or protein folding -- are in fact feasible with appropriate computational scale. Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent type methods, typically implemented as backpropagation. While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world. This text is concerned with exposing these regularities through unified geometric principles that can be applied throughout a wide spectrum of applications. Such a 'geometric unification' endeavour, in the spirit of Felix Klein's Erlangen Program, serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other hand, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computational Geometry,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/olaf/Zotero/storage/6V9FXB7I/Bronstein et al. - 2021 - Geometric Deep Learning Grids, Groups, Graphs, Ge.pdf;/home/olaf/Zotero/storage/4PP6BKL4/2104.html}
}

@online{brownLanguageModelsAre2020,
  title = {Language {{Models}} Are {{Few-Shot Learners}}},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  date = {2020-07-22},
  number = {arXiv:2005.14165},
  eprint = {arXiv:2005.14165},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.2005.14165},
  url = {http://arxiv.org/abs/2005.14165},
  urldate = {2022-12-08},
  abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {/home/olaf/Zotero/storage/9UIMNV29/Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf;/home/olaf/Zotero/storage/FIE5IYX2/2005.html}
}

@online{caiNoteOverSmoothingGraph2020,
  title = {A {{Note}} on {{Over-Smoothing}} for {{Graph Neural Networks}}},
  author = {Cai, Chen and Wang, Yusu},
  date = {2020-06-23},
  number = {arXiv:2006.13318},
  eprint = {arXiv:2006.13318},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.2006.13318},
  url = {http://arxiv.org/abs/2006.13318},
  urldate = {2022-12-07},
  abstract = {Graph Neural Networks (GNNs) have achieved a lot of success on graph-structured data. However, it is observed that the performance of graph neural networks does not improve as the number of layers increases. This effect, known as over-smoothing, has been analyzed mostly in linear cases. In this paper, we build upon previous results \textbackslash cite\{oono2019graph\} to further analyze the over-smoothing effect in the general graph neural network architecture. We show when the weight matrix satisfies the conditions determined by the spectrum of augmented normalized Laplacian, the Dirichlet energy of embeddings will converge to zero, resulting in the loss of discriminative power. Using Dirichlet energy to measure "expressiveness" of embedding is conceptually clean; it leads to simpler proofs than \textbackslash cite\{oono2019graph\} and can handle more non-linearities.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/olaf/Zotero/storage/TDHPXVN6/Cai et Wang - 2020 - A Note on Over-Smoothing for Graph Neural Networks.pdf;/home/olaf/Zotero/storage/CDCNPTNJ/2006.html}
}

@misc{ceylanismaililkanLecture12Expressive2022,
  title = {Lecture 12: {{Expressive Power}} of {{Message Passing Neural Networks}}},
  author = {Ceylan, İsmail İlkan},
  year = {Michaelmas Term 2022}
}

@article{chawlaSMOTESyntheticMinority2002,
  title = {{{SMOTE}}: {{Synthetic Minority Over-sampling Technique}}},
  shorttitle = {{{SMOTE}}},
  author = {Chawla, N. V. and Bowyer, K. W. and Hall, L. O. and Kegelmeyer, W. P.},
  date = {2002-06-01},
  journaltitle = {Journal of Artificial Intelligence Research},
  shortjournal = {jair},
  volume = {16},
  eprint = {1106.1813},
  eprinttype = {arxiv},
  eprintclass = {cs},
  pages = {321--357},
  issn = {1076-9757},
  doi = {10.1613/jair.953},
  url = {http://arxiv.org/abs/1106.1813},
  urldate = {2023-03-16},
  abstract = {An approach to the construction of classifiers from imbalanced datasets is described. A dataset is imbalanced if the classification categories are not approximately equally represented. Often real-world data sets are predominately composed of "normal" examples with only a small percentage of "abnormal" or "interesting" examples. It is also the case that the cost of misclassifying an abnormal (interesting) example as a normal example is often much higher than the cost of the reverse error. Under-sampling of the majority (normal) class has been proposed as a good means of increasing the sensitivity of a classifier to the minority class. This paper shows that a combination of our method of over-sampling the minority (abnormal) class and under-sampling the majority (normal) class can achieve better classifier performance (in ROC space) than only under-sampling the majority class. This paper also shows that a combination of our method of over-sampling the minority class and under-sampling the majority class can achieve better classifier performance (in ROC space) than varying the loss ratios in Ripper or class priors in Naive Bayes. Our method of over-sampling the minority class involves creating synthetic minority class examples. Experiments are performed using C4.5, Ripper and a Naive Bayes classifier. The method is evaluated using the area under the Receiver Operating Characteristic curve (AUC) and the ROC convex hull strategy.},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/home/olaf/Zotero/storage/T5E77YGW/Chawla et al. - 2002 - SMOTE Synthetic Minority Over-sampling Technique.pdf;/home/olaf/Zotero/storage/QEZMIZVZ/1106.html}
}

@article{chenBagTricksTraining2022,
  title = {Bag of {{Tricks}} for {{Training Deeper Graph Neural Networks}}: {{A Comprehensive Benchmark Study}}},
  shorttitle = {Bag of {{Tricks}} for {{Training Deeper Graph Neural Networks}}},
  author = {Chen, Tianlong and Zhou, Kaixiong and Duan, Keyu and Zheng, Wenqing and Wang, Peihao and Hu, Xia and Wang, Zhangyang},
  date = {2022},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  pages = {1--1},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2022.3174515},
  abstract = {Training deep graph neural networks (GNNs) is notoriously hard. Besides the standard plights in training deep architectures such as vanishing gradients and overfitting, it also uniquely suffers from over-smoothing, information squashing, and so on, which limits their potential power for encoding the high-order neighbor structure in large-scale graphs. Although numerous efforts are proposed to address these limitations, such as various forms of skip connections, graph normalization, and random dropping, it is difficult to disentangle the advantages brought by a deep GNN architecture from those tricks" necessary to train such an architecture. Moreover, the lack of a standardized benchmark with fair and consistent experimental settings poses an almost insurmountable obstacle to gauging the effectiveness of new mechanisms. In view of those, we present the first fair and reproducible benchmark dedicated to assessing the tricks" of training deep GNNs. We categorize existing approaches, investigate their hyperparameter sensitivity, and unify the basic configuration. Comprehensive evaluations are then conducted on tens of representative graph datasets including the recent large-scale Open Graph Benchmark, with diverse deep GNN backbones. We demonstrate that an organic combo of initial connection, identity mapping, group, and batch normalization attains the new state-of-the-art results for deep GNNs on large datasets.},
  eventtitle = {{{IEEE Transactions}} on {{Pattern Analysis}} and {{Machine Intelligence}}},
  keywords = {Benchmark,Benchmark testing,Computer architecture,Deep Graph Neural Networks,Graph neural networks,Over-smoothing,Peer-to-peer computing,Standards,Task analysis,Training,Training Technique},
  file = {/home/olaf/Zotero/storage/6L3FLXT7/Chen et al. - 2022 - Bag of Tricks for Training Deeper Graph Neural Net.pdf;/home/olaf/Zotero/storage/QZYMC3R4/9773017.html}
}

@online{chenMeasuringRelievingOversmoothing2019,
  title = {Measuring and {{Relieving}} the {{Over-smoothing Problem}} for {{Graph Neural Networks}} from the {{Topological View}}},
  author = {Chen, Deli and Lin, Yankai and Li, Wei and Li, Peng and Zhou, Jie and Sun, Xu},
  date = {2019-11-18},
  number = {arXiv:1909.03211},
  eprint = {arXiv:1909.03211},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.1909.03211},
  url = {http://arxiv.org/abs/1909.03211},
  urldate = {2022-12-07},
  abstract = {Graph Neural Networks (GNNs) have achieved promising performance on a wide range of graph-based tasks. Despite their success, one severe limitation of GNNs is the over-smoothing issue (indistinguishable representations of nodes in different classes). In this work, we present a systematic and quantitative study on the over-smoothing issue of GNNs. First, we introduce two quantitative metrics, MAD and MADGap, to measure the smoothness and over-smoothness of the graph nodes representations, respectively. Then, we verify that smoothing is the nature of GNNs and the critical factor leading to over-smoothness is the low information-to-noise ratio of the message received by the nodes, which is partially determined by the graph topology. Finally, we propose two methods to alleviate the over-smoothing issue from the topological view: (1) MADReg which adds a MADGap-based regularizer to the training objective;(2) AdaGraph which optimizes the graph topology based on the model predictions. Extensive experiments on 7 widely-used graph datasets with 10 typical GNN models show that the two proposed methods are effective for relieving the over-smoothing issue, thus improving the performance of various GNN models.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Computer Science - Social and Information Networks,Statistics - Machine Learning},
  file = {/home/olaf/Zotero/storage/ZTGUEBDS/Chen et al. - 2019 - Measuring and Relieving the Over-smoothing Problem.pdf;/home/olaf/Zotero/storage/S7IHXFA2/1909.html}
}

@article{chenUniversalGraphDeep2022,
  title = {A Universal Graph Deep Learning Interatomic Potential for the Periodic Table},
  author = {Chen, Chi and Ong, Shyue Ping},
  date = {2022-11},
  journaltitle = {Nature Computational Science},
  shortjournal = {Nat Comput Sci},
  volume = {2},
  number = {11},
  pages = {718--728},
  publisher = {{Nature Publishing Group}},
  issn = {2662-8457},
  doi = {10.1038/s43588-022-00349-3},
  url = {https://www.nature.com/articles/s43588-022-00349-3},
  urldate = {2023-01-03},
  abstract = {Interatomic potentials (IAPs), which describe the potential energy surface of atoms, are a fundamental input for atomistic simulations. However, existing IAPs are either fitted to narrow chemistries or too inaccurate for general applications. Here we report a universal IAP for materials based on graph neural networks with three-body interactions (M3GNet). The M3GNet IAP was trained on the massive database of structural relaxations performed by the Materials Project over the past ten years and has broad applications in structural relaxation, dynamic simulations and property prediction of materials across diverse chemical spaces. About 1.8 million materials from a screening of 31 million hypothetical crystal structures were identified to be potentially stable against existing Materials Project crystals based on M3GNet energies. Of the top 2,000 materials with the lowest energies above the convex hull, 1,578 were verified to be stable using density functional theory calculations. These results demonstrate a machine learning-accelerated pathway to the discovery of synthesizable materials with exceptional properties.},
  issue = {11},
  langid = {english},
  keywords = {Atomistic models,Computational methods,Materials science},
  file = {/home/olaf/Zotero/storage/QLEC9W8Y/Chen et Ong - 2022 - A universal graph deep learning interatomic potent.pdf}
}

@online{cicek3DUNetLearning2016,
  title = {{{3D U-Net}}: {{Learning Dense Volumetric Segmentation}} from {{Sparse Annotation}}},
  shorttitle = {{{3D U-Net}}},
  author = {Çiçek, Özgün and Abdulkadir, Ahmed and Lienkamp, Soeren S. and Brox, Thomas and Ronneberger, Olaf},
  date = {2016-06-21},
  number = {arXiv:1606.06650},
  eprint = {arXiv:1606.06650},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.1606.06650},
  url = {http://arxiv.org/abs/1606.06650},
  urldate = {2023-03-14},
  abstract = {This paper introduces a network for volumetric segmentation that learns from sparsely annotated volumetric images. We outline two attractive use cases of this method: (1) In a semi-automated setup, the user annotates some slices in the volume to be segmented. The network learns from these sparse annotations and provides a dense 3D segmentation. (2) In a fully-automated setup, we assume that a representative, sparsely annotated training set exists. Trained on this data set, the network densely segments new volumetric images. The proposed network extends the previous u-net architecture from Ronneberger et al. by replacing all 2D operations with their 3D counterparts. The implementation performs on-the-fly elastic deformations for efficient data augmentation during training. It is trained end-to-end from scratch, i.e., no pre-trained network is required. We test the performance of the proposed method on a complex, highly variable 3D structure, the Xenopus kidney, and achieve good results for both use cases.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/olaf/Zotero/storage/252KRPQY/Çiçek et al. - 2016 - 3D U-Net Learning Dense Volumetric Segmentation f.pdf;/home/olaf/Zotero/storage/79S5TX9J/1606.html}
}

@online{cleriguesAcuteSubacuteStroke2018,
  title = {Acute and Sub-Acute Stroke Lesion Segmentation from Multimodal {{MRI}}},
  author = {Clèrigues, Albert and Valverde, Sergi and Bernal, Jose and Freixenet, Jordi and Oliver, Arnau and Lladó, Xavier},
  date = {2018-10-31},
  number = {arXiv:1810.13304},
  eprint = {arXiv:1810.13304},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.1810.13304},
  url = {http://arxiv.org/abs/1810.13304},
  urldate = {2023-03-16},
  abstract = {Acute stroke lesion segmentation tasks are of great clinical interest as they can help doctors make better informed treatment decisions. Magnetic resonance imaging (MRI) is time demanding but can provide images that are considered gold standard for diagnosis. Automated stroke lesion segmentation can provide with an estimate of the location and volume of the lesioned tissue, which can help in the clinical practice to better assess and evaluate the risks of each treatment. We propose a deep learning methodology for acute and sub-acute stroke lesion segmentation using multimodal MR imaging. The proposed method is evaluated using two public datasets from the 2015 Ischemic Stroke Lesion Segmentation challenge (ISLES 2015). These involve the tasks of sub-acute stroke lesion segmentation (SISS) and acute stroke penumbra estimation (SPES) from diffusion, perfusion and anatomical MRI modalities. The performance is compared against state-of-the-art methods with a blind online testing set evaluation on each of the challenges. At the time of submitting this manuscript, our approach is the first method in the online rankings for the SISS (DSC=0.59\$\textbackslash pm\$0.31) and SPES sub-tasks (DSC=0.84\$\textbackslash pm\$0.10). When compared with the rest of submitted strategies, we achieve top rank performance with a lower Hausdorff distance. Better segmentation results are obtained by leveraging the anatomy and pathophysiology of acute stroke lesions and using a combined approach to minimize the effects of class imbalance. The same training procedure is used for both tasks, showing the proposed methodology can generalize well enough to deal with different unrelated tasks and imaging modalities without training hyper-parameter tuning. A public version of the proposed method has been released to the scientific community at https://github.com/NIC-VICOROB/stroke-mri-segmentation.},
  pubstate = {preprint},
  version = {1},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/olaf/Zotero/storage/YCQX9TSF/Clèrigues et al. - 2018 - Acute and sub-acute stroke lesion segmentation fro.pdf;/home/olaf/Zotero/storage/KGAT6FKD/1810.html}
}

@article{coifmanDiffusionMaps2006,
  title = {Diffusion Maps},
  author = {Coifman, Ronald R. and Lafon, Stéphane},
  date = {2006-07},
  journaltitle = {Applied and Computational Harmonic Analysis},
  shortjournal = {Applied and Computational Harmonic Analysis},
  volume = {21},
  number = {1},
  pages = {5--30},
  issn = {10635203},
  doi = {10.1016/j.acha.2006.04.006},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1063520306000546},
  urldate = {2022-12-07},
  abstract = {In this paper, we provide a framework based upon diffusion processes for finding meaningful geometric descriptions of data sets. We show that eigenfunctions of Markov matrices can be used to construct coordinates called diffusion maps that generate efficient representations of complex geometric structures. The associated family of diffusion distances, obtained by iterating the Markov matrix, defines multiscale geometries that prove to be useful in the context of data parametrization and dimensionality reduction. The proposed framework relates the spectral properties of Markov processes to their geometric counterparts and it unifies ideas arising in a variety of contexts such as machine learning, spectral graph theory and eigenmap methods.},
  langid = {english},
  file = {/home/olaf/Zotero/storage/YPECRN6D/Coifman et Lafon - 2006 - Diffusion maps.pdf}
}

@article{commandeurDeepLearningQuantification2018,
  title = {Deep {{Learning}} for {{Quantification}} of {{Epicardial}} and {{Thoracic Adipose Tissue From Non-Contrast CT}}},
  author = {Commandeur, Frederic and Goeller, Markus and Betancur, Julian and Cadet, Sebastien and Doris, Mhairi and Chen, Xi and Berman, Daniel S. and Slomka, Piotr J. and Tamarappoo, Balaji K. and Dey, Damini},
  date = {2018-08},
  journaltitle = {IEEE Transactions on Medical Imaging},
  volume = {37},
  number = {8},
  pages = {1835--1846},
  issn = {1558-254X},
  doi = {10.1109/TMI.2018.2804799},
  abstract = {Epicardial adipose tissue (EAT) is a visceral fat deposit related to coronary artery disease. Fully automated quantification of EAT volume in clinical routine could be a timesaving and reliable tool for cardiovascular risk assessment. We propose a new fully automated deep learning framework for EAT and thoracic adipose tissue (TAT) quantification from non-contrast coronary artery calcium computed tomography (CT) scans. The first multi-task convolutional neural network (ConvNet) is used to determine heart limits and perform segmentation of heart and adipose tissues. The second ConvNet, combined with a statistical shape model, allows for pericardium detection. EAT and TAT segmentations are then obtained from outputs of both ConvNets. We evaluate the performance of the method on CT data sets from 250 asymptomatic individuals. Strong agreement between automatic and expert manual quantification is obtained for both EAT and TAT with median Dice score coefficients of 0.823 (inter-quartile range (IQR): 0.779-0.860) and 0.905 (IQR: 0.862-0.928), respectively; with excellent correlations of 0.924 and 0.945 for EAT and TAT volumes. Computations are performed in {$<$};26 s on a standard personal computer for one CT scan. Therefore, the proposed method represents a tool for rapid fully automated quantification of adipose tissue and may improve cardiovascular risk stratification in patients referred for routine CT calcium scans.},
  eventtitle = {{{IEEE Transactions}} on {{Medical Imaging}}},
  keywords = {Arteries,Biomedical imaging,Calcium,Computed tomography,Convolutional neural networks,deep learning,epicardial adipose tissue,Feature extraction,Heart,Machine learning,non-contrast computed tomography (CT)},
  file = {/home/olaf/Zotero/storage/S4Z34UZN/Commandeur et al. - 2018 - Deep Learning for Quantification of Epicardial and.pdf;/home/olaf/Zotero/storage/GB22MP39/8288602.html}
}

@article{crumGeneralizedOverlapMeasures2006,
  title = {Generalized Overlap Measures for Evaluation and Validation in Medical Image Analysis},
  author = {Crum, William R. and Camara, Oscar and Hill, Derek L. G.},
  date = {2006-11},
  journaltitle = {IEEE transactions on medical imaging},
  shortjournal = {IEEE Trans Med Imaging},
  volume = {25},
  number = {11},
  eprint = {17117774},
  eprinttype = {pmid},
  pages = {1451--1461},
  issn = {0278-0062},
  doi = {10.1109/TMI.2006.880587},
  abstract = {Measures of overlap of labelled regions of images, such as the Dice and Tanimoto coefficients, have been extensively used to evaluate image registration and segmentation algorithms. Modern studies can include multiple labels defined on multiple images yet most evaluation schemes report one overlap per labelled region, simply averaged over multiple images. In this paper, common overlap measures are generalized to measure the total overlap of ensembles of labels defined on multiple test images and account for fractional labels using fuzzy set theory. This framework allows a single "figure-of-merit" to be reported which summarises the results of a complex experiment by image pair, by label or overall. A complementary measure of error, the overlap distance, is defined which captures the spatial extent of the nonoverlapping part and is related to the Hausdorff distance computed on grey level images. The generalized overlap measures are validated on synthetic images for which the overlap can be computed analytically and used as similarity measures in nonrigid registration of three-dimensional magnetic resonance imaging (MRI) brain images. Finally, a pragmatic segmentation ground truth is constructed by registering a magnetic resonance atlas brain to 20 individual scans, and used with the overlap measures to evaluate publicly available brain segmentation algorithms.},
  langid = {english},
  keywords = {Algorithms,Brain,Humans,Image Enhancement,Image Interpretation; Computer-Assisted,Information Storage and Retrieval,Magnetic Resonance Imaging,Pattern Recognition; Automated,Quality Assurance; Health Care,Reproducibility of Results,Sensitivity and Specificity,Subtraction Technique}
}

@article{debnathStructureactivityRelationshipMutagenic1991,
  title = {Structure-Activity Relationship of Mutagenic Aromatic and Heteroaromatic Nitro Compounds. {{Correlation}} with Molecular Orbital Energies and Hydrophobicity},
  author = {Debnath, Asim Kumar and Lopez de Compadre, Rosa L. and Debnath, Gargi and Shusterman, Alan J. and Hansch, Corwin},
  date = {1991-02},
  journaltitle = {Journal of Medicinal Chemistry},
  shortjournal = {J. Med. Chem.},
  volume = {34},
  number = {2},
  pages = {786--797},
  issn = {0022-2623, 1520-4804},
  doi = {10.1021/jm00106a046},
  url = {https://pubs.acs.org/doi/abs/10.1021/jm00106a046},
  urldate = {2023-01-01},
  langid = {english},
  file = {/home/olaf/Zotero/storage/YKSRKV2C/Debnath et al. - 1991 - Structure-activity relationship of mutagenic aroma.pdf}
}

@article{debnathStructureactivityRelationshipMutagenic1991a,
  title = {Structure-Activity Relationship of Mutagenic Aromatic and Heteroaromatic Nitro Compounds. {{Correlation}} with Molecular Orbital Energies and Hydrophobicity},
  author = {Debnath, A. K. and Lopez de Compadre, R. L. and Debnath, G. and Shusterman, A. J. and Hansch, C.},
  date = {1991-02},
  journaltitle = {Journal of Medicinal Chemistry},
  shortjournal = {J Med Chem},
  volume = {34},
  number = {2},
  eprint = {1995902},
  eprinttype = {pmid},
  pages = {786--797},
  issn = {0022-2623},
  doi = {10.1021/jm00106a046},
  abstract = {A review of the literature yielded data on over 200 aromatic and heteroaromatic nitro compounds tested for mutagenicity in the Ames test using S. typhimurium TA98. From the data, a quantitative structure-activity relationship (QSAR) has been derived for 188 congeners. The main determinants of mutagenicity are the hydrophobicity (modeled by octanol/water partition coefficients) and the energies of the lowest unoccupied molecular orbitals calculated using the AM1 method. It is also shown that chemicals possessing three or more fused rings possess much greater mutagenic potency than compounds with one or two fused rings. Since the QSAR is based on a very wide range in structural variation, aromatic rings from benzene to coronene are included as well as many different types of heterocycles, it is a significant step toward a predictive toxicology of value in the design of less mutagenic bioactive compounds.},
  langid = {english},
  keywords = {Animals,Chemical Phenomena,Chemistry,Mutagenicity Tests,Mutagens,Nitro Compounds,Structure-Activity Relationship}
}

@article{despotovicMRISegmentationHuman2015,
  title = {{{MRI Segmentation}} of the {{Human Brain}}: {{Challenges}}, {{Methods}}, and {{Applications}}},
  shorttitle = {{{MRI Segmentation}} of the {{Human Brain}}},
  author = {Despotović, Ivana and Goossens, Bart and Philips, Wilfried},
  date = {2015},
  journaltitle = {Computational and Mathematical Methods in Medicine},
  shortjournal = {Computational and Mathematical Methods in Medicine},
  volume = {2015},
  pages = {1--23},
  issn = {1748-670X, 1748-6718},
  doi = {10.1155/2015/450341},
  url = {http://www.hindawi.com/journals/cmmm/2015/450341/},
  urldate = {2023-03-16},
  abstract = {Image segmentation is one of the most important tasks in medical image analysis and is often the first and the most critical step in many clinical applications. In brain MRI analysis, image segmentation is commonly used for measuring and visualizing the brain’s anatomical structures, for analyzing brain changes, for delineating pathological regions, and for surgical planning and image-guided interventions. In the last few decades, various segmentation techniques of different accuracy and degree of complexity have been developed and reported in the literature. In this paper we review the most popular methods commonly used for brain MRI segmentation. We highlight differences between them and discuss their capabilities, advantages, and limitations. To address the complexity and challenges of the brain MRI segmentation problem, we first introduce the basic concepts of image segmentation. Then, we explain different MRI preprocessing steps including image registration, bias field correction, and removal of nonbrain tissue. Finally, after reviewing different brain MRI segmentation methods, we discuss the validation problem in brain MRI segmentation.},
  langid = {english},
  file = {/home/olaf/Zotero/storage/VM98CDIA/Despotović et al. - 2015 - MRI Segmentation of the Human Brain Challenges, M.pdf}
}

@online{digiovanniGraphNeuralNetworks2022,
  title = {Graph {{Neural Networks}} as {{Gradient Flows}}: Understanding Graph Convolutions via Energy},
  shorttitle = {Graph {{Neural Networks}} as {{Gradient Flows}}},
  author = {Di Giovanni, Francesco and Rowbottom, James and Chamberlain, Benjamin P. and Markovich, Thomas and Bronstein, Michael M.},
  date = {2022-10-08},
  number = {arXiv:2206.10991},
  eprint = {arXiv:2206.10991},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.2206.10991},
  url = {http://arxiv.org/abs/2206.10991},
  urldate = {2023-02-27},
  abstract = {Gradient flows are differential equations that minimize an energy functional and constitute the main descriptors of physical systems. We apply this formalism to Graph Neural Networks (GNNs) to develop new frameworks for learning on graphs as well as provide a better theoretical understanding of existing ones. We derive GNNs as a gradient flow equation of a parametric energy that provides a physics-inspired interpretation of GNNs as learning particle dynamics in the feature space. In particular, we show that in graph convolutional models (GCN), the positive/negative eigenvalues of the channel mixing matrix correspond to attractive/repulsive forces between adjacent features. We rigorously prove how the channel-mixing can learn to steer the dynamics towards low or high frequencies, which allows to deal with heterophilic graphs. We show that the same class of energies is decreasing along a larger family of GNNs; albeit not gradient flows, they retain their inductive bias. We experimentally evaluate an instance of the gradient flow framework that is principled, more efficient than GCN, and achieves competitive performance on graph datasets of varying homophily often outperforming recent baselines specifically designed to target heterophily.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/olaf/Zotero/storage/UDRTKMGQ/Di Giovanni et al. - 2022 - Graph Neural Networks as Gradient Flows understan.pdf;/home/olaf/Zotero/storage/ZSTWBBH3/2206.html}
}

@online{dwivediLongRangeGraph2022,
  title = {Long {{Range Graph Benchmark}}},
  author = {Dwivedi, Vijay Prakash and Rampášek, Ladislav and Galkin, Mikhail and Parviz, Ali and Wolf, Guy and Luu, Anh Tuan and Beaini, Dominique},
  date = {2022-11-11},
  number = {arXiv:2206.08164},
  eprint = {arXiv:2206.08164},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.2206.08164},
  url = {http://arxiv.org/abs/2206.08164},
  urldate = {2022-12-08},
  abstract = {Graph Neural Networks (GNNs) that are based on the message passing (MP) paradigm generally exchange information between 1-hop neighbors to build node representations at each layer. In principle, such networks are not able to capture long-range interactions (LRI) that may be desired or necessary for learning a given task on graphs. Recently, there has been an increasing interest in development of Transformer-based methods for graphs that can consider full node connectivity beyond the original sparse structure, thus enabling the modeling of LRI. However, MP-GNNs that simply rely on 1-hop message passing often fare better in several existing graph benchmarks when combined with positional feature representations, among other innovations, hence limiting the perceived utility and ranking of Transformer-like architectures. Here, we present the Long Range Graph Benchmark (LRGB) with 5 graph learning datasets: PascalVOC-SP, COCO-SP, PCQM-Contact, Peptides-func and Peptides-struct that arguably require LRI reasoning to achieve strong performance in a given task. We benchmark both baseline GNNs and Graph Transformer networks to verify that the models which capture long-range dependencies perform significantly better on these tasks. Therefore, these datasets are suitable for benchmarking and exploration of MP-GNNs and Graph Transformer architectures that are intended to capture LRI.},
  pubstate = {preprint},
  version = {2},
  keywords = {Computer Science - Machine Learning},
  file = {/home/olaf/Zotero/storage/D2N7R8ZY/Dwivedi et al. - 2022 - Long Range Graph Benchmark.pdf;/home/olaf/Zotero/storage/GYK3H93Y/2206.html}
}

@article{faberAsynchronousMessagePassing2023,
  title = {Asynchronous {{Message Passing}}: {{A}} New {{Framework}} for {{Learning}} in {{Graphs}}},
  shorttitle = {Asynchronous {{Message Passing}}},
  author = {Faber, Lukas and Wattenhofer, Roger},
  date = {2023-02-01},
  url = {https://openreview.net/forum?id=2_I3JQ70U2},
  urldate = {2023-03-20},
  abstract = {This paper studies asynchronous message passing (AMP), a new framework for applying neural networks to graphs. Existing graph neural networks (GNNs) use the message passing framework which is based on the synchronous distributed computing model. In traditional GNNs, nodes aggregate their neighbors in each round, which causes problems such as oversmoothing and expressiveness limitations. On the other hand, our AMP framework is based on the \textbackslash textit\{asynchronous\} model, where nodes react to messages of their neighbors individually. We prove (i) AMP is at least as powerful as the message passing framework, (ii) AMP is more powerful than the \$1-\$WL test for graph isomorphism, an important benchmark for message passing GNNs, and (iii) conceptually, AMP can even separate any pair of graphs and compute graph isomorphism. We experimentally validate the findings on AMP's expressiveness, and show that AMP might be better suited to propagate messages over large distances in graphs. We also demonstrate that AMP performs well on several graph classification benchmarks.},
  langid = {english},
  file = {/home/olaf/Zotero/storage/KQN3BTYY/Faber et Wattenhofer - 2023 - Asynchronous Message Passing A new Framework for .pdf}
}

@online{faberAsynchronousNeuralNetworks2022,
  title = {Asynchronous {{Neural Networks}} for {{Learning}} in {{Graphs}}},
  author = {Faber, Lukas and Wattenhofer, Roger},
  date = {2022-05-24},
  number = {arXiv:2205.12245},
  eprint = {arXiv:2205.12245},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.2205.12245},
  url = {http://arxiv.org/abs/2205.12245},
  urldate = {2023-03-20},
  abstract = {This paper studies asynchronous message passing (AMP), a new paradigm for applying neural network based learning to graphs. Existing graph neural networks use the synchronous distributed computing model and aggregate their neighbors in each round, which causes problems such as oversmoothing and limits their expressiveness. On the other hand, AMP is based on the asynchronous model, where nodes react to messages of their neighbors individually. We prove that (i) AMP can simulate synchronous GNNs and that (ii) AMP can theoretically distinguish any pair of graphs. We experimentally validate AMP's expressiveness. Further, we show that AMP might be better suited to propagate messages over large distances in graphs and performs well on several graph classification benchmarks.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning},
  file = {/home/olaf/Zotero/storage/62D4JDPM/Faber et Wattenhofer - 2022 - Asynchronous Neural Networks for Learning in Graph.pdf;/home/olaf/Zotero/storage/N2KBNPFV/2205.html}
}

@article{fabreRetrievingAdversarialCliques2022,
  title = {Retrieving {{Adversarial Cliques}} in {{Cognitive Communities}}: {{A New Conceptual Framework}} for {{Scientific Knowledge Graphs}}},
  shorttitle = {Retrieving {{Adversarial Cliques}} in {{Cognitive Communities}}},
  author = {Fabre, Renaud and Azeroual, Otmane and Bellot, Patrice and Schöpfel, Joachim and Egret, Daniel},
  date = {2022-09},
  journaltitle = {Future Internet},
  volume = {14},
  number = {9},
  pages = {262},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {1999-5903},
  doi = {10.3390/fi14090262},
  url = {https://www.mdpi.com/1999-5903/14/9/262},
  urldate = {2023-03-20},
  abstract = {The variety and diversity of published content are currently expanding in all fields of scholarly communication. Yet, scientific knowledge graphs (SKG) provide only poor images of the varied directions of alternative scientific choices, and in particular scientific controversies, which are not currently identified and interpreted. We propose to use the rich variety of knowledge present in search histories to represent cliques modeling the main interpretable practices of information retrieval issued from the same “cognitive community”, identified by their use of keywords and by the search experience of the users sharing the same research question. Modeling typical cliques belonging to the same cognitive community is achieved through a new conceptual framework, based on user profiles, namely a bipartite geometric scientific knowledge graph, SKG GRAPHYP. Further studies of interpretation will test differences of documentary profiles and their meaning in various possible contexts which studies on “disagreements in scientific literature” have outlined. This final adjusted version of GRAPHYP optimizes the modeling of “Manifold Subnetworks of Cliques in Cognitive Communities” (MSCCC), captured from previous user experience in the same search domain. Cliques are built from graph grids of three parameters outlining the manifold of search experiences: mass of users; intensity of uses of items; and attention, identified as a ratio of “feature augmentation” by literature on information retrieval, its mean value allows calculation of an observed “steady” value of the user/item ratio or, conversely, a documentary behavior “deviating” from this mean value. An illustration of our approach is supplied in a positive first test, which stimulates further work on modeling subnetworks of users in search experience, that could help identify the varied alternative documentary sources of information retrieval, and in particular the scientific controversies and scholarly disputes.},
  issue = {9},
  langid = {english},
  keywords = {cliques,community detection,entity alignment,graph completion,graph subnetwork,meta learning,model interpretability,multiplex,search history},
  file = {/home/olaf/Zotero/storage/FQD34QGL/Fabre et al. - 2022 - Retrieving Adversarial Cliques in Cognitive Commun.pdf}
}

@online{frascaUnderstandingExtendingSubgraph2022,
  title = {Understanding and {{Extending Subgraph GNNs}} by {{Rethinking Their Symmetries}}},
  author = {Frasca, Fabrizio and Bevilacqua, Beatrice and Bronstein, Michael M. and Maron, Haggai},
  date = {2022-10-13},
  number = {arXiv:2206.11140},
  eprint = {arXiv:2206.11140},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.2206.11140},
  url = {http://arxiv.org/abs/2206.11140},
  urldate = {2023-03-20},
  abstract = {Subgraph GNNs are a recent class of expressive Graph Neural Networks (GNNs) which model graphs as collections of subgraphs. So far, the design space of possible Subgraph GNN architectures as well as their basic theoretical properties are still largely unexplored. In this paper, we study the most prominent form of subgraph methods, which employs node-based subgraph selection policies such as ego-networks or node marking and deletion. We address two central questions: (1) What is the upper-bound of the expressive power of these methods? and (2) What is the family of equivariant message passing layers on these sets of subgraphs?. Our first step in answering these questions is a novel symmetry analysis which shows that modelling the symmetries of node-based subgraph collections requires a significantly smaller symmetry group than the one adopted in previous works. This analysis is then used to establish a link between Subgraph GNNs and Invariant Graph Networks (IGNs). We answer the questions above by first bounding the expressive power of subgraph methods by 3-WL, and then proposing a general family of message-passing layers for subgraph methods that generalises all previous node-based Subgraph GNNs. Finally, we design a novel Subgraph GNN dubbed SUN, which theoretically unifies previous architectures while providing better empirical performance on multiple benchmarks.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning},
  file = {/home/olaf/Zotero/storage/83RMZFLF/Frasca et al. - 2022 - Understanding and Extending Subgraph GNNs by Rethi.pdf;/home/olaf/Zotero/storage/8DQWUXJZ/2206.html}
}

@online{godwinSimpleGNNRegularisation2022,
  title = {Simple {{GNN Regularisation}} for {{3D Molecular Property Prediction}} \& {{Beyond}}},
  author = {Godwin, Jonathan and Schaarschmidt, Michael and Gaunt, Alexander and Sanchez-Gonzalez, Alvaro and Rubanova, Yulia and Veličković, Petar and Kirkpatrick, James and Battaglia, Peter},
  date = {2022-03-15},
  number = {arXiv:2106.07971},
  eprint = {arXiv:2106.07971},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.2106.07971},
  url = {http://arxiv.org/abs/2106.07971},
  urldate = {2022-12-07},
  abstract = {In this paper we show that simple noise regularisation can be an effective way to address GNN oversmoothing. First we argue that regularisers addressing oversmoothing should both penalise node latent similarity and encourage meaningful node representations. From this observation we derive "Noisy Nodes", a simple technique in which we corrupt the input graph with noise, and add a noise correcting node-level loss. The diverse node level loss encourages latent node diversity, and the denoising objective encourages graph manifold learning. Our regulariser applies well-studied methods in simple, straightforward ways which allow even generic architectures to overcome oversmoothing and achieve state of the art results on quantum chemistry tasks, and improve results significantly on Open Graph Benchmark (OGB) datasets. Our results suggest Noisy Nodes can serve as a complementary building block in the GNN toolkit.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning},
  file = {/home/olaf/Zotero/storage/XYLWCXFM/Godwin et al. - 2022 - Simple GNN Regularisation for 3D Molecular Propert.pdf;/home/olaf/Zotero/storage/PXBXSR82/2106.html}
}

@online{gokdenCoulGATExperimentInterpretability2019a,
  title = {{{CoulGAT}}: {{An Experiment}} on {{Interpretability}} of {{Graph Attention Networks}}},
  shorttitle = {{{CoulGAT}}},
  author = {Gokden, Burc},
  date = {2019-12-18},
  number = {arXiv:1912.08409},
  eprint = {arXiv:1912.08409},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.1912.08409},
  url = {http://arxiv.org/abs/1912.08409},
  urldate = {2022-12-31},
  abstract = {We present an attention mechanism inspired from definition of screened Coulomb potential. This attention mechanism was used to interpret the Graph Attention (GAT) model layers and training dataset by using a flexible and scalable framework (CoulGAT) developed for this purpose. Using CoulGAT, a forest of plain and resnet models were trained and characterized using this attention mechanism against CHAMPS dataset. The learnable variables of the attention mechanism are used to extract node-node and node-feature interactions to define an empirical standard model for the graph structure and hidden layer. This representation of graph and hidden layers can be used as a tool to compare different models, optimize hidden layers and extract a compact definition of graph structure of the dataset.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/olaf/Zotero/storage/9SBRMI92/Gokden - 2019 - CoulGAT An Experiment on Interpretability of Grap.pdf;/home/olaf/Zotero/storage/E4LYAS8R/1912.html}
}

@article{guerraExplainabilitySubgraphsenhancedGraph2023,
  title = {Explainability in Subgraphs-Enhanced {{Graph Neural Networks}}},
  author = {Guerra, Michele and Spinelli, Indro and Scardapane, Simone and Bianchi, Filippo Maria},
  date = {2023-01-23},
  journaltitle = {Proceedings of the Northern Lights Deep Learning Workshop},
  shortjournal = {nldl},
  volume = {4},
  eprint = {2209.07926},
  eprinttype = {arxiv},
  eprintclass = {cs},
  issn = {2703-6928},
  doi = {10.7557/18.6796},
  url = {http://arxiv.org/abs/2209.07926},
  urldate = {2023-03-20},
  abstract = {Recently, subgraphs-enhanced Graph Neural Networks (SGNNs) have been introduced to enhance the expressive power of Graph Neural Networks (GNNs), which was proved to be not higher than the 1-dimensional Weisfeiler-Leman isomorphism test. The new paradigm suggests using subgraphs extracted from the input graph to improve the model's expressiveness, but the additional complexity exacerbates an already challenging problem in GNNs: explaining their predictions. In this work, we adapt PGExplainer, one of the most recent explainers for GNNs, to SGNNs. The proposed explainer accounts for the contribution of all the different subgraphs and can produce a meaningful explanation that humans can interpret. The experiments that we performed both on real and synthetic datasets show that our framework is successful in explaining the decision process of an SGNN on graph classification tasks.},
  keywords = {Computer Science - Machine Learning},
  file = {/home/olaf/Zotero/storage/XASDKQDW/Guerra et al. - 2023 - Explainability in subgraphs-enhanced Graph Neural .pdf;/home/olaf/Zotero/storage/BT3U8Z6S/2209.html}
}

@online{guoLinklessLinkPrediction2022,
  title = {Linkless {{Link Prediction}} via {{Relational Distillation}}},
  author = {Guo, Zhichun and Shiao, William and Zhang, Shichang and Liu, Yozen and Chawla, Nitesh and Shah, Neil and Zhao, Tong},
  date = {2022-10-19},
  number = {arXiv:2210.05801},
  eprint = {arXiv:2210.05801},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.2210.05801},
  url = {http://arxiv.org/abs/2210.05801},
  urldate = {2023-03-20},
  abstract = {Graph Neural Networks (GNNs) have been widely used on graph data and have shown exceptional performance in the task of link prediction. Despite their effectiveness, GNNs often suffer from high latency due to non-trivial neighborhood data dependency in practical deployments. To address this issue, researchers have proposed methods based on knowledge distillation (KD) to transfer the knowledge from teacher GNNs to student MLPs, which are known to be efficient even with industrial scale data, and have shown promising results on node classification. Nonetheless, using KD to accelerate link prediction is still unexplored. In this work, we start with exploring two direct analogs of traditional KD for link prediction, i.e., predicted logit-based matching and node representation-based matching. Upon observing direct KD analogs do not perform well for link prediction, we propose a relational KD framework, Linkless Link Prediction (LLP). Unlike simple KD methods that match independent link logits or node representations, LLP distills relational knowledge that is centered around each (anchor) node to the student MLP. Specifically, we propose two matching strategies that complement each other: rank-based matching and distribution-based matching. Extensive experiments demonstrate that LLP boosts the link prediction performance of MLPs with significant margins, and even outperforms the teacher GNNs on 6 out of 9 benchmarks. LLP also achieves a 776.37x speedup in link prediction inference compared to GNNs on the large scale OGB-Citation2 dataset.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning},
  file = {/home/olaf/Zotero/storage/XE8BBSXB/Guo et al. - 2022 - Linkless Link Prediction via Relational Distillati.pdf;/home/olaf/Zotero/storage/4MZ9CCG5/2210.html}
}

@online{hamiltonInductiveRepresentationLearning2018,
  title = {Inductive {{Representation Learning}} on {{Large Graphs}}},
  author = {Hamilton, William L. and Ying, Rex and Leskovec, Jure},
  date = {2018-09-10},
  number = {arXiv:1706.02216},
  eprint = {arXiv:1706.02216},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.1706.02216},
  url = {http://arxiv.org/abs/1706.02216},
  urldate = {2022-12-19},
  abstract = {Low-dimensional embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks, from content recommendation to identifying protein functions. However, most existing approaches require that all nodes in the graph are present during training of the embeddings; these previous approaches are inherently transductive and do not naturally generalize to unseen nodes. Here we present GraphSAGE, a general, inductive framework that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings for previously unseen data. Instead of training individual embeddings for each node, we learn a function that generates embeddings by sampling and aggregating features from a node's local neighborhood. Our algorithm outperforms strong baselines on three inductive node-classification benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and Reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Computer Science - Social and Information Networks,Statistics - Machine Learning},
  file = {/home/olaf/Zotero/storage/PD7NJXQV/Hamilton et al. - 2018 - Inductive Representation Learning on Large Graphs.pdf;/home/olaf/Zotero/storage/RJ5GQCY3/1706.html}
}

@incollection{hamiltonRicciFlowSurfaces1988,
  title = {The {{Ricci}} Flow on Surfaces},
  booktitle = {Contemporary {{Mathematics}}},
  author = {Hamilton, Richard S.},
  editor = {Isenberg, James A.},
  date = {1988},
  volume = {71},
  pages = {237--262},
  publisher = {{American Mathematical Society}},
  location = {{Providence, Rhode Island}},
  doi = {10.1090/conm/071/954419},
  url = {http://www.ams.org/conm/071/},
  urldate = {2022-12-09},
  isbn = {978-0-8218-5079-4 978-0-8218-7660-2},
  langid = {english}
}

@article{havaeiBrainTumorSegmentation2017,
  title = {Brain Tumor Segmentation with {{Deep Neural Networks}}},
  author = {Havaei, Mohammad and Davy, Axel and Warde-Farley, David and Biard, Antoine and Courville, Aaron and Bengio, Yoshua and Pal, Chris and Jodoin, Pierre-Marc and Larochelle, Hugo},
  date = {2017-01},
  journaltitle = {Medical Image Analysis},
  shortjournal = {Med Image Anal},
  volume = {35},
  eprint = {27310171},
  eprinttype = {pmid},
  pages = {18--31},
  issn = {1361-8423},
  doi = {10.1016/j.media.2016.05.004},
  abstract = {In this paper, we present a fully automatic brain tumor segmentation method based on Deep Neural Networks (DNNs). The proposed networks are tailored to glioblastomas (both low and high grade) pictured in MR images. By their very nature, these tumors can appear anywhere in the brain and have almost any kind of shape, size, and contrast. These reasons motivate our exploration of a machine learning solution that exploits a flexible, high capacity DNN while being extremely efficient. Here, we give a description of different model choices that we've found to be necessary for obtaining competitive performance. We explore in particular different architectures based on Convolutional Neural Networks (CNN), i.e. DNNs specifically adapted to image data. We present a novel CNN architecture which differs from those traditionally used in computer vision. Our CNN exploits both local features as well as more global contextual features simultaneously. Also, different from most traditional uses of CNNs, our networks use a final layer that is a convolutional implementation of a fully connected layer which allows a 40 fold speed up. We also describe a 2-phase training procedure that allows us to tackle difficulties related to the imbalance of tumor labels. Finally, we explore a cascade architecture in which the output of a basic CNN is treated as an additional source of information for a subsequent CNN. Results reported on the 2013 BRATS test data-set reveal that our architecture improves over the currently published state-of-the-art while being over 30 times faster.},
  langid = {english},
  keywords = {Brain Neoplasms,Brain tumor segmentation,Cascaded convolutional neural networks,Convolutional neural networks,Deep neural networks,Glioblastoma,Humans,Image Processing; Computer-Assisted,Machine Learning,Magnetic Resonance Imaging,Neural Networks; Computer},
  file = {/home/olaf/Zotero/storage/38SVE37U/Havaei et al. - 2017 - Brain tumor segmentation with Deep Neural Networks.pdf}
}

@article{heGraphMLPMixer2023,
  title = {Graph {{MLP-Mixer}}},
  author = {He, Xiaoxin and Hooi, Bryan and Laurent, Thomas and Perold, Adam and LeCun, Yann and Bresson, Xavier},
  date = {2023-02-01},
  url = {https://openreview.net/forum?id=N4k3klHNzQj},
  urldate = {2023-03-20},
  abstract = {Graph Neural Networks (GNNs) have shown great potential in the field of graph representation learning. Standard GNNs define a local message-passing mechanism which propagates information over the whole graph domain by stacking multiple layers. This paradigm suffers from two major limitations, over-squashing and poor long-range dependencies, that can be solved using global attention but significantly increases the computational cost to quadratic complexity. In this work, we consider an alternative approach to overcome these structural limitations while keeping a low complexity cost. Motivated by the recent MLP-Mixer architecture introduced in computer vision, we propose to generalize this network to graphs. This GNN model, namely Graph MLP-Mixer, can make long-range connections without over-squashing or high complexity due to the mixer layer applied to the graph patches extracted from the original graph. As a result, this architecture exhibits promising results when comparing standard GNNs vs. Graph MLP-Mixers on benchmark graph datasets.},
  langid = {english},
  file = {/home/olaf/Zotero/storage/X2DQ6ENY/He et al. - 2023 - Graph MLP-Mixer.pdf}
}

@article{hesamianDeepLearningTechniques2019,
  title = {Deep {{Learning Techniques}} for {{Medical Image Segmentation}}: {{Achievements}} and {{Challenges}}},
  shorttitle = {Deep {{Learning Techniques}} for {{Medical Image Segmentation}}},
  author = {Hesamian, Mohammad Hesam and Jia, Wenjing and He, Xiangjian and Kennedy, Paul},
  date = {2019-08},
  journaltitle = {Journal of Digital Imaging},
  shortjournal = {J Digit Imaging},
  volume = {32},
  number = {4},
  eprint = {31144149},
  eprinttype = {pmid},
  pages = {582--596},
  issn = {1618-727X},
  doi = {10.1007/s10278-019-00227-x},
  abstract = {Deep learning-based image segmentation is by now firmly established as a robust tool in image segmentation. It has been widely used to separate homogeneous areas as the first and critical component of diagnosis and treatment pipeline. In this article, we present a critical appraisal of popular methods that have employed deep-learning techniques for medical image segmentation. Moreover, we summarize the most common challenges incurred and suggest possible solutions.},
  langid = {english},
  pmcid = {PMC6646484},
  keywords = {CNN,Deep learning,Deep Learning,Diagnostic Imaging,Humans,Image Processing; Computer-Assisted,Medical image segmentation,Organ segmentation},
  file = {/home/olaf/Zotero/storage/BGPLV4YU/Hesamian et al. - 2019 - Deep Learning Techniques for Medical Image Segment.pdf}
}

@article{hochreiterLongShorttermMemory1997,
  title = {Long {{Short-term Memory}}},
  author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
  date = {1997-12-01},
  journaltitle = {Neural computation},
  shortjournal = {Neural computation},
  volume = {9},
  pages = {1735--80},
  doi = {10.1162/neco.1997.9.8.1735},
  abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient-based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O(1). Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
  file = {/home/olaf/Zotero/storage/PZKEMBBI/Hochreiter et Schmidhuber - 1997 - Long Short-term Memory.pdf}
}

@article{hornikMultilayerFeedforwardNetworks1989,
  title = {Multilayer Feedforward Networks Are Universal Approximators},
  author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
  date = {1989-01},
  journaltitle = {Neural Networks},
  shortjournal = {Neural Networks},
  volume = {2},
  number = {5},
  pages = {359--366},
  issn = {08936080},
  doi = {10.1016/0893-6080(89)90020-8},
  url = {https://linkinghub.elsevier.com/retrieve/pii/0893608089900208},
  urldate = {2022-12-05},
  abstract = {This paper rigorously establishes thut standard rnultiluyer feedforward networks with as f\&v us one hidden layer using arbitrary squashing functions ure capable of upproximating uny Bore1 measurable function from one finite dimensional space to another to any desired degree of uccuracy, provided sujficirntly muny hidden units are available. In this sense, multilayer feedforward networks are u class of universul rlpproximators.},
  langid = {english},
  file = {/home/olaf/Zotero/storage/KQBMFGG2/Hornik et al. - 1989 - Multilayer feedforward networks are universal appr.pdf}
}

@online{huangBoostingCycleCounting2022,
  title = {Boosting the {{Cycle Counting Power}} of {{Graph Neural Networks}} with {{I}}\$\^2\$-{{GNNs}}},
  author = {Huang, Yinan and Peng, Xingang and Ma, Jianzhu and Zhang, Muhan},
  date = {2022-10-22},
  number = {arXiv:2210.13978},
  eprint = {arXiv:2210.13978},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.2210.13978},
  url = {http://arxiv.org/abs/2210.13978},
  urldate = {2023-03-20},
  abstract = {Message Passing Neural Networks (MPNNs) are a widely used class of Graph Neural Networks (GNNs). The limited representational power of MPNNs inspires the study of provably powerful GNN architectures. However, knowing one model is more powerful than another gives little insight about what functions they can or cannot express. It is still unclear whether these models are able to approximate specific functions such as counting certain graph substructures, which is essential for applications in biology, chemistry and social network analysis. Motivated by this, we propose to study the counting power of Subgraph MPNNs, a recent and popular class of powerful GNN models that extract rooted subgraphs for each node, assign the root node a unique identifier and encode the root node's representation within its rooted subgraph. Specifically, we prove that Subgraph MPNNs fail to count more-than-4-cycles at node level, implying that node representations cannot correctly encode the surrounding substructures like ring systems with more than four atoms. To overcome this limitation, we propose I\$\^2\$-GNNs to extend Subgraph MPNNs by assigning different identifiers for the root node and its neighbors in each subgraph. I\$\^2\$-GNNs' discriminative power is shown to be strictly stronger than Subgraph MPNNs and partially stronger than the 3-WL test. More importantly, I\$\^2\$-GNNs are proven capable of counting all 3, 4, 5 and 6-cycles, covering common substructures like benzene rings in organic chemistry, while still keeping linear complexity. To the best of our knowledge, it is the first linear-time GNN model that can count 6-cycles with theoretical guarantees. We validate its counting power in cycle counting tasks and demonstrate its competitive performance in molecular prediction benchmarks.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning},
  file = {/home/olaf/Zotero/storage/RK2TDESY/Huang et al. - 2022 - Boosting the Cycle Counting Power of Graph Neural .pdf;/home/olaf/Zotero/storage/9X6PE5J5/2210.html}
}

@inproceedings{huangGoingDeeperPermutationSensitive2022,
  title = {Going {{Deeper}} into {{Permutation-Sensitive Graph Neural Networks}}},
  booktitle = {Proceedings of the 39th {{International Conference}} on {{Machine Learning}}},
  author = {Huang, Zhongyu and Wang, Yingheng and Li, Chaozhuo and He, Huiguang},
  date = {2022-06-28},
  pages = {9377--9409},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v162/huang22l.html},
  urldate = {2023-03-20},
  abstract = {The invariance to permutations of the adjacency matrix, i.e., graph isomorphism, is an overarching requirement for Graph Neural Networks (GNNs). Conventionally, this prerequisite can be satisfied by the invariant operations over node permutations when aggregating messages. However, such an invariant manner may ignore the relationships among neighboring nodes, thereby hindering the expressivity of GNNs. In this work, we devise an efficient permutation-sensitive aggregation mechanism via permutation groups, capturing pairwise correlations between neighboring nodes. We prove that our approach is strictly more powerful than the 2-dimensional Weisfeiler-Lehman (2-WL) graph isomorphism test and not less powerful than the 3-WL test. Moreover, we prove that our approach achieves the linear sampling complexity. Comprehensive experiments on multiple synthetic and real-world datasets demonstrate the superiority of our model.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/home/olaf/Zotero/storage/WI6DULZ8/Huang et al. - 2022 - Going Deeper into Permutation-Sensitive Graph Neur.pdf}
}

@online{huangTacklingOverSmoothingGeneral2022,
  title = {Tackling {{Over-Smoothing}} for {{General Graph Convolutional Networks}}},
  author = {Huang, Wenbing and Rong, Yu and Xu, Tingyang and Sun, Fuchun and Huang, Junzhou},
  date = {2022-07-08},
  number = {arXiv:2008.09864},
  eprint = {arXiv:2008.09864},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.2008.09864},
  url = {http://arxiv.org/abs/2008.09864},
  urldate = {2022-12-07},
  abstract = {Increasing the depth of GCN, which is expected to permit more expressivity, is shown to incur performance detriment especially on node classification. The main cause of this lies in over-smoothing. The over-smoothing issue drives the output of GCN towards a space that contains limited distinguished information among nodes, leading to poor expressivity. Several works on refining the architecture of deep GCN have been proposed, but it is still unknown in theory whether or not these refinements are able to relieve over-smoothing. In this paper, we first theoretically analyze how general GCNs act with the increase in depth, including generic GCN, GCN with bias, ResGCN, and APPNP. We find that all these models are characterized by a universal process: all nodes converging to a cuboid. Upon this theorem, we propose DropEdge to alleviate over-smoothing by randomly removing a certain number of edges at each training epoch. Theoretically, DropEdge either reduces the convergence speed of over-smoothing or relieves the information loss caused by dimension collapse. Experimental evaluations on simulated dataset have visualized the difference in over-smoothing between different GCNs. Moreover, extensive experiments on several real benchmarks support that DropEdge consistently improves the performance on a variety of both shallow and deep GCNs.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/olaf/Zotero/storage/6J4SD4JK/Huang et al. - 2022 - Tackling Over-Smoothing for General Graph Convolut.pdf;/home/olaf/Zotero/storage/S8CFHRWF/2008.html}
}

@online{ioffeBatchNormalizationAccelerating2015,
  title = {Batch {{Normalization}}: {{Accelerating Deep Network Training}} by {{Reducing Internal Covariate Shift}}},
  shorttitle = {Batch {{Normalization}}},
  author = {Ioffe, Sergey and Szegedy, Christian},
  date = {2015-03-02},
  number = {arXiv:1502.03167},
  eprint = {arXiv:1502.03167},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.1502.03167},
  url = {http://arxiv.org/abs/1502.03167},
  urldate = {2023-03-17},
  abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9\% top-5 validation error (and 4.8\% test error), exceeding the accuracy of human raters.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning},
  file = {/home/olaf/Zotero/storage/LX9YHDCC/Ioffe et Szegedy - 2015 - Batch Normalization Accelerating Deep Network Tra.pdf;/home/olaf/Zotero/storage/SVSMFP57/1502.html}
}

@online{IXIDatasetBrain,
  title = {{{IXI Dataset}} – {{Brain Development}}},
  url = {https://brain-development.org/ixi-dataset/},
  urldate = {2023-03-15},
  langid = {american},
  file = {/home/olaf/Zotero/storage/RBX3XM98/ixi-dataset.html}
}

@inproceedings{jangMedicalImageMatching2014a,
  title = {Medical Image Matching Using Variable Randomized Undersampling Probability Pattern in Data Acquisition},
  booktitle = {2014 {{International Conference}} on {{Electronics}}, {{Information}} and {{Communications}} ({{ICEIC}})},
  author = {Jang, Jinseong and Eo, Tae-joon and Kim, Minoh and Choi, Narae and Han, Dongyup and Kim, Donghyun and Hwang, Dosik},
  date = {2014-01},
  pages = {1--2},
  doi = {10.1109/ELINFOCOM.2014.6914453},
  abstract = {This paper proposes a randomized variable probability pattern in under-sampling acquisition for medical image matching which is a method that can perform the quantitative analysis of tissue parameters. For high-speed estimation of tissue parameters, random under-sampling with less than the Nyquist rate in k-space is required. This study presents an accurate parameter mapping method for under-sampled data by using various randomized probability pattern. In comparison to the fixed probability pattern, the proposed method shows improved estimation results with reduced artifacts such as ghosting effects due to the undersampling scheme.},
  eventtitle = {2014 {{International Conference}} on {{Electronics}}, {{Information}} and {{Communications}} ({{ICEIC}})},
  keywords = {Biomedical imaging,Encoding,Magnetic resonance,Mathematical model,MRF,parameter,Pattern matching,Trajectory,undersampling pattern},
  file = {/home/olaf/Zotero/storage/C54ZUHD2/Jang et al. - 2014 - Medical image matching using variable randomized u.pdf;/home/olaf/Zotero/storage/3VLEN5DD/6914453.html}
}

@article{kamnitsasEfficientMultiscale3D2017,
  title = {Efficient Multi-Scale {{3D CNN}} with Fully Connected {{CRF}} for Accurate Brain Lesion Segmentation},
  author = {Kamnitsas, Konstantinos and Ledig, Christian and Newcombe, Virginia F. J. and Simpson, Joanna P. and Kane, Andrew D. and Menon, David K. and Rueckert, Daniel and Glocker, Ben},
  date = {2017-02},
  journaltitle = {Medical Image Analysis},
  shortjournal = {Med Image Anal},
  volume = {36},
  eprint = {27865153},
  eprinttype = {pmid},
  pages = {61--78},
  issn = {1361-8423},
  doi = {10.1016/j.media.2016.10.004},
  abstract = {We propose a dual pathway, 11-layers deep, three-dimensional Convolutional Neural Network for the challenging task of brain lesion segmentation. The devised architecture is the result of an in-depth analysis of the limitations of current networks proposed for similar applications. To overcome the computational burden of processing 3D medical scans, we have devised an efficient and effective dense training scheme which joins the processing of adjacent image patches into one pass through the network while automatically adapting to the inherent class imbalance present in the data. Further, we analyze the development of deeper, thus more discriminative 3D CNNs. In order to incorporate both local and larger contextual information, we employ a dual pathway architecture that processes the input images at multiple scales simultaneously. For post-processing of the network's soft segmentation, we use a 3D fully connected Conditional Random Field which effectively removes false positives. Our pipeline is extensively evaluated on three challenging tasks of lesion segmentation in multi-channel MRI patient data with traumatic brain injuries, brain tumours, and ischemic stroke. We improve on the state-of-the-art for all three applications, with top ranking performance on the public benchmarks BRATS 2015 and ISLES 2015. Our method is computationally efficient, which allows its adoption in a variety of research and clinical settings. The source code of our implementation is made publicly available.},
  langid = {english},
  keywords = {3D convolutional neural network,Brain,Brain Injuries; Traumatic,Brain Ischemia,Brain lesions,Brain Neoplasms,Deep learning,Fully connected CRF,Humans,Neural Networks; Computer,Reproducibility of Results,Segmentation,Sensitivity and Specificity},
  file = {/home/olaf/Zotero/storage/7XY3EBPX/Kamnitsas et al. - 2017 - Efficient multi-scale 3D CNN with fully connected .pdf}
}

@article{kensertGraphConvolutionalNetworks2021a,
  title = {Graph {{Convolutional Networks}} for {{Improved Prediction}} and {{Interpretability}} of {{Chromatographic Retention Data}}},
  author = {Kensert, Alexander and Bouwmeester, Robbin and Efthymiadis, Kyriakos and Van Broeck, Peter and Desmet, Gert and Cabooter, Deirdre},
  date = {2021-11-30},
  journaltitle = {Analytical Chemistry},
  shortjournal = {Anal. Chem.},
  volume = {93},
  number = {47},
  pages = {15633--15641},
  issn = {0003-2700, 1520-6882},
  doi = {10.1021/acs.analchem.1c02988},
  url = {https://pubs.acs.org/doi/10.1021/acs.analchem.1c02988},
  urldate = {2023-01-01},
  abstract = {Machine learning is a popular technique to predict the retention times of molecules based on descriptors. Descriptors and associated labels (e.g., retention times) of a set of molecules can be used to train a machine learning algorithm. However, descriptors are fixed molecular features which are not necessarily optimized for the given machine learning problem (e.g., to predict retention times). Recent advances in molecular machine learning make use of so-called graph convolutional networks (GCNs) to learn molecular representations from atoms and their bonds to adjacent atoms to optimize the molecular representation for the given problem. In this study, two GCNs were implemented to predict the retention times of molecules for three different chromatographic data sets and compared to seven benchmarks (including two state-of-the art machine learning models). Additionally, saliency maps were computed from trained GCNs to better interpret the importance of certain molecular sub-structures in the data sets. Based on the overall observations of this study, the GCNs performed better than all benchmarks, either significantly outperforming them (5−25\% lower mean absolute error) or performing similar to them ({$<$}5\% difference). Saliency maps revealed a significant difference in molecular sub-structures that are important for predictions of different chromatographic data sets (reversed-phase liquid chromatography vs hydrophilic interaction liquid chromatography).},
  langid = {english},
  file = {/home/olaf/Zotero/storage/EQXPW6MC/Kensert et al. - 2021 - Graph Convolutional Networks for Improved Predicti.pdf}
}

@inproceedings{kimEquivariantHypergraphNeural2022,
  title = {Equivariant {{Hypergraph Neural Networks}}},
  booktitle = {Computer {{Vision}} – {{ECCV}} 2022},
  author = {Kim, Jinwoo and Oh, Saeyoon and Cho, Sungjun and Hong, Seunghoon},
  editor = {Avidan, Shai and Brostow, Gabriel and Cissé, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
  date = {2022},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {86--103},
  publisher = {{Springer Nature Switzerland}},
  location = {{Cham}},
  doi = {10.1007/978-3-031-19803-8_6},
  abstract = {Many problems in computer vision and machine learning can be cast as learning on hypergraphs that represent higher-order relations. Recent approaches for hypergraph learning extend graph neural networks based on message passing, which is simple yet fundamentally limited in modeling long-range dependencies and expressive power. On the other hand, tensor-based equivariant neural networks enjoy maximal expressiveness, but their application has been limited in hypergraphs due to heavy computation and strict assumptions on fixed-order hyperedges. We resolve these problems and present Equivariant Hypergraph Neural Network (EHNN), the first attempt to realize maximally expressive equivariant layers for general hypergraph learning. We also present two practical realizations of our framework based on hypernetworks (EHNN-MLP) and self-attention (EHNN-Transformer), which are easy to implement and theoretically more expressive than most message passing approaches. We demonstrate their capability in a range of hypergraph learning problems, including synthetic k-edge identification, semi-supervised classification, and visual keypoint matching, and report improved performances over strong message passing baselines. Our implementation is available at https://github.com/jw9730/ehnn.},
  isbn = {978-3-031-19803-8},
  langid = {english},
  keywords = {Graph neural network,Hypergraph neural network,Keypoint matching,Permutation equivariance,Semi-supervised classification},
  file = {/home/olaf/Zotero/storage/D3ACE37U/Kim et al. - 2022 - Equivariant Hypergraph Neural Networks.pdf}
}

@online{kimStructuredAttentionNetworks2017,
  title = {Structured {{Attention Networks}}},
  author = {Kim, Yoon and Denton, Carl and Hoang, Luong and Rush, Alexander M.},
  date = {2017-02-16},
  number = {arXiv:1702.00887},
  eprint = {arXiv:1702.00887},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.1702.00887},
  url = {http://arxiv.org/abs/1702.00887},
  urldate = {2022-11-21},
  abstract = {Attention networks have proven to be an effective approach for embedding categorical inference within a deep neural network. However, for many tasks we may want to model richer structural dependencies without abandoning end-to-end training. In this work, we experiment with incorporating richer structural distributions, encoded using graphical models, within deep networks. We show that these structured attention networks are simple extensions of the basic attention procedure, and that they allow for extending attention beyond the standard soft-selection approach, such as attending to partial segmentations or to subtrees. We experiment with two different classes of structured attention networks: a linear-chain conditional random field and a graph-based parsing model, and describe how these models can be practically implemented as neural network layers. Experiments show that this approach is effective for incorporating structural biases, and structured attention networks outperform baseline attention models on a variety of synthetic and real tasks: tree transduction, neural machine translation, question answering, and natural language inference. We further find that models trained in this way learn interesting unsupervised hidden representations that generalize simple attention.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/home/olaf/Zotero/storage/4CFEC2QV/Kim et al. - 2017 - Structured Attention Networks.pdf;/home/olaf/Zotero/storage/R2K84B4B/1702.html}
}

@online{kingmaAdamMethodStochastic2017,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  shorttitle = {Adam},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  date = {2017-01-29},
  number = {arXiv:1412.6980},
  eprint = {arXiv:1412.6980},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.1412.6980},
  url = {http://arxiv.org/abs/1412.6980},
  urldate = {2023-03-16},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning},
  file = {/home/olaf/Zotero/storage/GSXGKB5J/Kingma et Ba - 2017 - Adam A Method for Stochastic Optimization.pdf;/home/olaf/Zotero/storage/RWEM5CG5/1412.html}
}

@article{kingmaIntroductionVariationalAutoencoders2019,
  title = {An {{Introduction}} to {{Variational Autoencoders}}},
  author = {Kingma, Diederik P. and Welling, Max},
  date = {2019},
  journaltitle = {Foundations and Trends® in Machine Learning},
  shortjournal = {FNT in Machine Learning},
  volume = {12},
  number = {4},
  eprint = {1906.02691},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  pages = {307--392},
  issn = {1935-8237, 1935-8245},
  doi = {10.1561/2200000056},
  url = {http://arxiv.org/abs/1906.02691},
  urldate = {2022-11-22},
  abstract = {Variational autoencoders provide a principled framework for learning deep latent-variable models and corresponding inference models. In this work, we provide an introduction to variational autoencoders and some important extensions.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/olaf/Zotero/storage/F5C64G6G/Kingma et Welling - 2019 - An Introduction to Variational Autoencoders.pdf;/home/olaf/Zotero/storage/D2XZSFJX/1906.html}
}

@online{kingmaSemiSupervisedLearningDeep2014,
  title = {Semi-{{Supervised Learning}} with {{Deep Generative Models}}},
  author = {Kingma, Diederik P. and Rezende, Danilo J. and Mohamed, Shakir and Welling, Max},
  date = {2014-10-31},
  number = {arXiv:1406.5298},
  eprint = {arXiv:1406.5298},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.1406.5298},
  url = {http://arxiv.org/abs/1406.5298},
  urldate = {2022-11-22},
  abstract = {The ever-increasing size of modern data sets combined with the difficulty of obtaining label information has made semi-supervised learning one of the problems of significant practical importance in modern data analysis. We revisit the approach to semi-supervised learning with generative models and develop new models that allow for effective generalisation from small labelled data sets to large unlabelled ones. Generative approaches have thus far been either inflexible, inefficient or non-scalable. We show that deep generative models and approximate Bayesian inference exploiting recent advances in variational methods can be used to provide significant improvements, making generative approaches highly competitive for semi-supervised learning.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/olaf/Zotero/storage/CA2C8IV5/Kingma et al. - 2014 - Semi-Supervised Learning with Deep Generative Mode.pdf;/home/olaf/Zotero/storage/K52DA5XZ/1406.html}
}

@online{kipfSemiSupervisedClassificationGraph2017,
  title = {Semi-{{Supervised Classification}} with {{Graph Convolutional Networks}}},
  author = {Kipf, Thomas N. and Welling, Max},
  date = {2017-02-22},
  number = {arXiv:1609.02907},
  eprint = {arXiv:1609.02907},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.1609.02907},
  url = {http://arxiv.org/abs/1609.02907},
  urldate = {2022-12-19},
  abstract = {We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/olaf/Zotero/storage/IAXCR4V9/Kipf et Welling - 2017 - Semi-Supervised Classification with Graph Convolut.pdf;/home/olaf/Zotero/storage/KFVGICH3/1609.html}
}

@online{kipfSemiSupervisedClassificationGraph2017a,
  title = {Semi-{{Supervised Classification}} with {{Graph Convolutional Networks}}},
  author = {Kipf, Thomas N. and Welling, Max},
  date = {2017-02-22},
  number = {arXiv:1609.02907},
  eprint = {arXiv:1609.02907},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.1609.02907},
  url = {http://arxiv.org/abs/1609.02907},
  urldate = {2023-01-01},
  abstract = {We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/olaf/Zotero/storage/AFWZIPDS/Kipf et Welling - 2017 - Semi-Supervised Classification with Graph Convolut.pdf;/home/olaf/Zotero/storage/IKNF5B5H/1609.html}
}

@online{liDeeperInsightsGraph2018,
  title = {Deeper {{Insights}} into {{Graph Convolutional Networks}} for {{Semi-Supervised Learning}}},
  author = {Li, Qimai and Han, Zhichao and Wu, Xiao-Ming},
  date = {2018-01-22},
  number = {arXiv:1801.07606},
  eprint = {arXiv:1801.07606},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.1801.07606},
  url = {http://arxiv.org/abs/1801.07606},
  urldate = {2022-12-07},
  abstract = {Many interesting problems in machine learning are being revisited with new deep learning tools. For graph-based semisupervised learning, a recent important development is graph convolutional networks (GCNs), which nicely integrate local vertex features and graph topology in the convolutional layers. Although the GCN model compares favorably with other state-of-the-art methods, its mechanisms are not clear and it still requires a considerable amount of labeled data for validation and model selection. In this paper, we develop deeper insights into the GCN model and address its fundamental limits. First, we show that the graph convolution of the GCN model is actually a special form of Laplacian smoothing, which is the key reason why GCNs work, but it also brings potential concerns of over-smoothing with many convolutional layers. Second, to overcome the limits of the GCN model with shallow architectures, we propose both co-training and self-training approaches to train GCNs. Our approaches significantly improve GCNs in learning with very few labels, and exempt them from requiring additional labels for validation. Extensive experiments on benchmarks have verified our theory and proposals.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/olaf/Zotero/storage/7DF78ZYF/Li et al. - 2018 - Deeper Insights into Graph Convolutional Networks .pdf;/home/olaf/Zotero/storage/99TGR3KS/1801.html}
}

@online{limSignBasisInvariant2022,
  title = {Sign and {{Basis Invariant Networks}} for {{Spectral Graph Representation Learning}}},
  author = {Lim, Derek and Robinson, Joshua and Zhao, Lingxiao and Smidt, Tess and Sra, Suvrit and Maron, Haggai and Jegelka, Stefanie},
  date = {2022-09-30},
  number = {arXiv:2202.13013},
  eprint = {arXiv:2202.13013},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.2202.13013},
  url = {http://arxiv.org/abs/2202.13013},
  urldate = {2023-03-20},
  abstract = {We introduce SignNet and BasisNet -- new neural architectures that are invariant to two key symmetries displayed by eigenvectors: (i) sign flips, since if \$v\$ is an eigenvector then so is \$-v\$; and (ii) more general basis symmetries, which occur in higher dimensional eigenspaces with infinitely many choices of basis eigenvectors. We prove that under certain conditions our networks are universal, i.e., they can approximate any continuous function of eigenvectors with the desired invariances. When used with Laplacian eigenvectors, our networks are provably more expressive than existing spectral methods on graphs; for instance, they subsume all spectral graph convolutions, certain spectral graph invariants, and previously proposed graph positional encodings as special cases. Experiments show that our networks significantly outperform existing baselines on molecular graph regression, learning expressive graph representations, and learning neural fields on triangle meshes. Our code is available at https://github.com/cptq/SignNet-BasisNet .},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/olaf/Zotero/storage/S7DJL329/Lim et al. - 2022 - Sign and Basis Invariant Networks for Spectral Gra.pdf;/home/olaf/Zotero/storage/6NPCJG7H/2202.html}
}

@inproceedings{liuDeeperGraphNeural2020,
  title = {Towards {{Deeper Graph Neural Networks}}},
  booktitle = {Proceedings of the 26th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Liu, Meng and Gao, Hongyang and Ji, Shuiwang},
  date = {2020-08-20},
  series = {{{KDD}} '20},
  pages = {338--348},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3394486.3403076},
  url = {https://doi.org/10.1145/3394486.3403076},
  urldate = {2022-12-07},
  abstract = {Graph neural networks have shown significant success in the field of graph representation learning. Graph convolutions perform neighborhood aggregation and represent one of the most important graph operations. Nevertheless, one layer of these neighborhood aggregation methods only consider immediate neighbors, and the performance decreases when going deeper to enable larger receptive fields. Several recent studies attribute this performance deterioration to the over-smoothing issue, which states that repeated propagation makes node representations of different classes indistinguishable. In this work, we study this observation systematically and develop new insights towards deeper graph neural networks. First, we provide a systematical analysis on this issue and argue that the key factor compromising the performance significantly is the entanglement of representation transformation and propagation in current graph convolution operations. After decoupling these two operations, deeper graph neural networks can be used to learn graph node representations from larger receptive fields. We further provide a theoretical analysis of the above observation when building very deep models, which can serve as a rigorous and gentle description of the over-smoothing issue. Based on our theoretical and empirical analysis, we propose Deep Adaptive Graph Neural Network (DAGNN) to adaptively incorporate information from large receptive fields. A set of experiments on citation, co-authorship, and co-purchase datasets have confirmed our analysis and insights and demonstrated the superiority of our proposed methods.},
  isbn = {978-1-4503-7998-4},
  keywords = {deep learning,graph neural networks,graph representation learning},
  file = {/home/olaf/Zotero/storage/K5MN8JTL/Liu et al. - 2020 - Towards Deeper Graph Neural Networks.pdf}
}

@online{liuEEGNNEdgeEnhanced2022a,
  title = {{{EEGNN}}: {{Edge Enhanced Graph Neural Networks}}},
  shorttitle = {{{EEGNN}}},
  author = {Liu, Yirui and Qiao, Xinghao and Wang, Liying and Lam, Jessica},
  date = {2022-08-12},
  number = {arXiv:2208.06322},
  eprint = {arXiv:2208.06322},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.2208.06322},
  url = {http://arxiv.org/abs/2208.06322},
  urldate = {2022-12-08},
  abstract = {Training deep graph neural networks (GNNs) poses a challenging task, as the performance of GNNs may suffer from the number of hidden message-passing layers. The literature has focused on the proposals of over-smoothing and under-reaching to explain the performance deterioration of deep GNNs. In this paper, we propose a new explanation for such deteriorated performance phenomenon, mis-simplification, that is, mistakenly simplifying graphs by preventing self-loops and forcing edges to be unweighted. We show that such simplifying can reduce the potential of message-passing layers to capture the structural information of graphs. In view of this, we propose a new framework, edge enhanced graph neural network(EEGNN). EEGNN uses the structural information extracted from the proposed Dirichlet mixture Poisson graph model, a Bayesian nonparametric model for graphs, to improve the performance of various deep message-passing GNNs. Experiments over different datasets show that our method achieves considerable performance increase compared to baselines.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/olaf/Zotero/storage/VV3Z4DHQ/Liu et al. - 2022 - EEGNN Edge Enhanced Graph Neural Networks.pdf;/home/olaf/Zotero/storage/PC2Q2QUN/2208.html}
}

@online{liuGraphPoolingGraph2022,
  title = {Graph {{Pooling}} for {{Graph Neural Networks}}: {{Progress}}, {{Challenges}}, and {{Opportunities}}},
  shorttitle = {Graph {{Pooling}} for {{Graph Neural Networks}}},
  author = {Liu, Chuang and Zhan, Yibing and Li, Chang and Du, Bo and Wu, Jia and Hu, Wenbin and Liu, Tongliang and Tao, Dacheng},
  date = {2022-04-15},
  number = {arXiv:2204.07321},
  eprint = {arXiv:2204.07321},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.2204.07321},
  url = {http://arxiv.org/abs/2204.07321},
  urldate = {2023-02-02},
  abstract = {Graph neural networks have emerged as a leading architecture for many graph-level tasks such as graph classification and graph generation with a notable improvement. Among these tasks, graph pooling is an essential component of graph neural network architectures for obtaining a holistic graph-level representation of the entire graph. Although a great variety of methods have been proposed in this promising and fast-developing research field, to the best of our knowledge, little effort has been made to systematically summarize these methods. To set the stage for the development of future works, in this paper, we attempt to fill this gap by providing a broad review of recent methods on graph pooling. Specifically, 1) we first propose a taxonomy of existing graph pooling methods and provide a mathematical summary for each category; 2) next, we provide an overview of the libraries related to graph pooling, including the commonly used datasets, model architectures for downstream tasks, and open-source implementations; 3) then, we further outline in brief the applications that incorporate the idea of graph pooling in a number of domains; 4) and finally, we discuss some critical challenges faced by the current studies and share our insights on potential directions for improving graph pooling in the future.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning},
  file = {/home/olaf/Zotero/storage/E4MRUAQU/Liu et al. - 2022 - Graph Pooling for Graph Neural Networks Progress,.pdf;/home/olaf/Zotero/storage/UK5S28AU/2204.html}
}

@article{liuNetworkRepresentationLearning2021,
  title = {Network Representation Learning: {{A}} Macro and Micro View},
  shorttitle = {Network Representation Learning},
  author = {Liu, Xueyi and Tang, Jie},
  date = {2021-01-01},
  journaltitle = {AI Open},
  shortjournal = {AI Open},
  volume = {2},
  pages = {43--64},
  issn = {2666-6510},
  doi = {10.1016/j.aiopen.2021.02.001},
  url = {https://www.sciencedirect.com/science/article/pii/S2666651021000024},
  urldate = {2022-12-07},
  abstract = {Graph is a universe data structure that is widely used to organize data in real-world. Various real-word networks like the transportation network, social and academic network can be represented by graphs. Recent years have witnessed the quick development on representing vertices in the network into a low-dimensional vector space, referred to as network representation learning. Representation learning can facilitate the design of new algorithms on the graph data. In this survey, we conduct a comprehensive review of current literature on network representation learning. Existing algorithms can be categorized into three groups: shallow embedding models, heterogeneous network embedding models, graph neural network based models. We review state-of-the-art algorithms for each category and discuss the essential differences between these algorithms. One advantage of the survey is that we systematically study the underlying theoretical foundations underlying the different categories of algorithms, which offers deep insights for better understanding the development of the network representation learning field.},
  langid = {english},
  keywords = {Graph neural networks,Graph spectral theory,Network representation learning},
  file = {/home/olaf/Zotero/storage/8VB8HFNW/Liu et Tang - 2021 - Network representation learning A macro and micro.pdf;/home/olaf/Zotero/storage/MX2S7AMQ/S2666651021000024.html}
}

@online{loshchilovDecoupledWeightDecay2019,
  title = {Decoupled {{Weight Decay Regularization}}},
  author = {Loshchilov, Ilya and Hutter, Frank},
  date = {2019-01-04},
  number = {arXiv:1711.05101},
  eprint = {arXiv:1711.05101},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.1711.05101},
  url = {http://arxiv.org/abs/1711.05101},
  urldate = {2023-01-01},
  abstract = {L\$\_2\$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is \textbackslash emph\{not\} the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L\$\_2\$ regularization (often calling it "weight decay" in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by \textbackslash emph\{decoupling\} the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in TensorFlow and PyTorch; the complete source code for our experiments is available at https://github.com/loshchil/AdamW-and-SGDW},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Mathematics - Optimization and Control},
  file = {/home/olaf/Zotero/storage/C32QKCIZ/Loshchilov et Hutter - 2019 - Decoupled Weight Decay Regularization.pdf;/home/olaf/Zotero/storage/L7E53QV8/1711.html}
}

@inproceedings{louisSamplingEnclosingSubgraphs2022,
  title = {Sampling {{Enclosing Subgraphs}} for {{Link Prediction}}},
  booktitle = {Proceedings of the 31st {{ACM International Conference}} on {{Information}} \& {{Knowledge Management}}},
  author = {Louis, Paul and Jacob, Shweta Ann and Salehi-Abari, Amirali},
  date = {2022-10-17},
  series = {{{CIKM}} '22},
  pages = {4269--4273},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3511808.3557688},
  url = {https://dl.acm.org/doi/10.1145/3511808.3557688},
  urldate = {2023-03-20},
  abstract = {Link prediction is a fundamental problem for graph-structured data (e.g., social networks, drug side-effect networks, etc.). Graph neural networks have offered robust solutions for this problem, specifically by learning the representation of the subgraph enclosing the target link (i.e., pair of nodes). However, these solutions do not scale well to large graphs as extraction and operation on enclosing subgraphs are computationally expensive. This paper presents a scalable link prediction solution, that we call ScaLed, which utilizes sparse enclosing subgraphs to make predictions. To extract sparse enclosing subgraphs, ScaLed takes multiple random walks from a target pair of nodes, then operates on the sampled enclosing subgraph induced by all visited nodes. By leveraging the smaller sampled enclosing subgraph, ScaLed can scale to larger graphs with much less overhead while maintaining high accuracy. Through comprehensive experiments, we have shown that ScaLed can produce comparable accuracy to those reported by the existing subgraph representation learning frameworks while being less computationally demanding.},
  isbn = {978-1-4503-9236-5},
  keywords = {graph neural networks,link prediction,subgraph sampling},
  file = {/home/olaf/Zotero/storage/XG5QURIU/Louis et al. - 2022 - Sampling Enclosing Subgraphs for Link Prediction.pdf}
}

@online{loukasWhatGraphNeural2020,
  title = {What Graph Neural Networks Cannot Learn: Depth vs Width},
  shorttitle = {What Graph Neural Networks Cannot Learn},
  author = {Loukas, Andreas},
  date = {2020-01-28},
  number = {arXiv:1907.03199},
  eprint = {arXiv:1907.03199},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.1907.03199},
  url = {http://arxiv.org/abs/1907.03199},
  urldate = {2022-12-05},
  abstract = {This paper studies the expressive power of graph neural networks falling within the message-passing framework (GNNmp). Two results are presented. First, GNNmp are shown to be Turing universal under sufficient conditions on their depth, width, node attributes, and layer expressiveness. Second, it is discovered that GNNmp can lose a significant portion of their power when their depth and width is restricted. The proposed impossibility statements stem from a new technique that enables the repurposing of seminal results from distributed computing and leads to lower bounds for an array of decision, optimization, and estimation problems involving graphs. Strikingly, several of these problems are deemed impossible unless the product of a GNNmp's depth and width exceeds a polynomial of the graph size; this dependence remains significant even for tasks that appear simple or when considering approximation.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/olaf/Zotero/storage/PL3PED7W/Loukas - 2020 - What graph neural networks cannot learn depth vs .pdf;/home/olaf/Zotero/storage/FNLQY4MZ/1907.html}
}

@article{luExpressivePowerNeural,
  title = {The {{Expressive Power}} of {{Neural Networks}}: {{A View}} from the {{Width}}},
  author = {Lu, Zhou and Pu, Hongming and Wang, Feicheng and Hu, Zhiqiang and Wang, Liwei},
  pages = {9},
  abstract = {The expressive power of neural networks is important for understanding deep learning. Most existing works consider this problem from the view of the depth of a network. In this paper, we study how width affects the expressiveness of neural networks. Classical results state that depth-bounded (e.g. depth-2) networks with suitable activation functions are universal approximators. We show a universal approximation theorem for width-bounded ReLU networks: width-(n + 4) ReLU networks, where n is the input dimension, are universal approximators. Moreover, except for a measure zero set, all functions cannot be approximated by width-n ReLU networks, which exhibits a phase transition. Several recent works demonstrate the benefits of depth by proving the depth-efficiency of neural networks. That is, there are classes of deep networks which cannot be realized by any shallow network whose size is no more than an exponential bound. Here we pose the dual question on the width-efficiency of ReLU networks: Are there wide networks that cannot be realized by narrow networks whose size is not substantially larger? We show that there exist classes of wide networks which cannot be realized by any narrow network whose depth is no more than a polynomial bound. On the other hand, we demonstrate by extensive experiments that narrow networks whose size exceed the polynomial bound by a constant factor can approximate wide and shallow network with high accuracy. Our results provide more comprehensive evidence that depth may be more effective than width for the expressiveness of ReLU networks.},
  langid = {english},
  file = {/home/olaf/Zotero/storage/43TKZUEY/Lu et al. - The Expressive Power of Neural Networks A View fr.pdf}
}

@inproceedings{lukovnikovImprovingBreadthWiseBackpropagation2021,
  title = {Improving {{Breadth-Wise Backpropagation}} in {{Graph Neural Networks Helps Learning Long-Range Dependencies}}.},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author = {Lukovnikov, Denis and Fischer, Asja},
  date = {2021-07-01},
  pages = {7180--7191},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v139/lukovnikov21a.html},
  urldate = {2022-12-07},
  abstract = {In this work, we focus on the ability of graph neural networks (GNNs) to learn long-range patterns in graphs with edge features. Learning patterns that involve longer paths in the graph, requires using deeper GNNs. However, GNNs suffer from a drop in performance with increasing network depth. To improve the performance of deeper GNNs, previous works have investigated normalization techniques and various types of skip connections. While they are designed to improve depth-wise backpropagation between the representations of the same node in successive layers, they do not improve breadth-wise backpropagation between representations of neighbouring nodes. To analyse the consequences, we design synthetic datasets serving as a testbed for the ability of GNNs to learn long-range patterns. Our analysis shows that several commonly used GNN variants with only depth-wise skip connections indeed have problems learning long-range patterns. They are clearly outperformed by an attention-based GNN architecture that we propose for improving both depth- and breadth-wise backpropagation. We also verify that the presented architecture is competitive on real-world data.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/home/olaf/Zotero/storage/6C6UJX6Z/Lukovnikov et Fischer - 2021 - Improving Breadth-Wise Backpropagation in Graph Ne.pdf;/home/olaf/Zotero/storage/D3AY63KV/Lukovnikov et Fischer - 2021 - Improving Breadth-Wise Backpropagation in Graph Ne.pdf}
}

@online{mahdaviBetterOutofDistributionGeneralization2022,
  title = {Towards {{Better Out-of-Distribution Generalization}} of {{Neural Algorithmic Reasoning Tasks}}},
  author = {Mahdavi, Sadegh and Swersky, Kevin and Kipf, Thomas and Hashemi, Milad and Thrampoulidis, Christos and Liao, Renjie},
  date = {2022-11-01},
  number = {arXiv:2211.00692},
  eprint = {arXiv:2211.00692},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.2211.00692},
  url = {http://arxiv.org/abs/2211.00692},
  urldate = {2023-03-20},
  abstract = {In this paper, we study the OOD generalization of neural algorithmic reasoning tasks, where the goal is to learn an algorithm (e.g., sorting, breadth-first search, and depth-first search) from input-output pairs using deep neural networks. First, we argue that OOD generalization in this setting is significantly different than common OOD settings. For example, some phenomena in OOD generalization of image classifications such as \textbackslash emph\{accuracy on the line\} are not observed here, and techniques such as data augmentation methods do not help as assumptions underlying many augmentation techniques are often violated. Second, we analyze the main challenges (e.g., input distribution shift, non-representative data generation, and uninformative validation metrics) of the current leading benchmark, i.e., CLRS \textbackslash citep\{deepmind2021clrs\}, which contains 30 algorithmic reasoning tasks. We propose several solutions, including a simple-yet-effective fix to the input distribution shift and improved data generation. Finally, we propose an attention-based 2WL-graph neural network (GNN) processor which complements message-passing GNNs so their combination outperforms the state-of-the-art model by a 3\% margin averaged over all algorithms. Our code is available at: \textbackslash url\{https://github.com/smahdavi4/clrs\}.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning},
  file = {/home/olaf/Zotero/storage/WL3UN6FP/Mahdavi et al. - 2022 - Towards Better Out-of-Distribution Generalization .pdf;/home/olaf/Zotero/storage/K5J64XJ4/2211.html}
}

@online{martinkusAgentbasedGraphNeural2023,
  title = {Agent-Based {{Graph Neural Networks}}},
  author = {Martinkus, Karolis and Papp, Pál András and Schesch, Benedikt and Wattenhofer, Roger},
  date = {2023-02-27},
  number = {arXiv:2206.11010},
  eprint = {arXiv:2206.11010},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.2206.11010},
  url = {http://arxiv.org/abs/2206.11010},
  urldate = {2023-03-20},
  abstract = {We present a novel graph neural network we call AgentNet, which is designed specifically for graph-level tasks. AgentNet is inspired by sublinear algorithms, featuring a computational complexity that is independent of the graph size. The architecture of AgentNet differs fundamentally from the architectures of traditional graph neural networks. In AgentNet, some trained \textbackslash textit\{neural agents\} intelligently walk the graph, and then collectively decide on the output. We provide an extensive theoretical analysis of AgentNet: We show that the agents can learn to systematically explore their neighborhood and that AgentNet can distinguish some structures that are even indistinguishable by 2-WL. Moreover, AgentNet is able to separate any two graphs which are sufficiently different in terms of subgraphs. We confirm these theoretical results with synthetic experiments on hard-to-distinguish graphs and real-world graph classification tasks. In both cases, we compare favorably not only to standard GNNs but also to computationally more expensive GNN extensions.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/olaf/Zotero/storage/DKCK8Y7C/Martinkus et al. - 2023 - Agent-based Graph Neural Networks.pdf;/home/olaf/Zotero/storage/6B2APFL5/2206.html}
}

@inproceedings{mercataliSymmetryinducedDisentanglementGraphs2022,
  title = {Symmetry-Induced {{Disentanglement}} on {{Graphs}}},
  author = {Mercatali, Giangiacomo and Freitas, Andre and Garg, Vikas},
  date = {2022-10-31},
  url = {https://openreview.net/forum?id=4tM0P_4N8D9},
  urldate = {2023-03-20},
  abstract = {Learning disentangled representations is important for unraveling the underlying complex interactions between latent generative factors. Disentanglement has been formalized using a symmetry-centric notion for unstructured spaces, however, graphs have eluded a similarly rigorous treatment. We fill this gap with a new notion of conditional symmetry for disentanglement, and leverage tools from Lie algebras to encode graph properties into subgroups using suitable adaptations of generative models such as Variational Autoencoders. Unlike existing works on disentanglement, the proposed models segregate the latent space into uncoupled and entangled parts. Experiments on synthetic and real datasets suggest that these models can learn effective disengaged representations, and improve performance on downstream tasks such as few-shot classification and molecular generation.},
  eventtitle = {Advances in {{Neural Information Processing Systems}}},
  langid = {english},
  file = {/home/olaf/Zotero/storage/4MXTZULA/Mercatali et al. - 2022 - Symmetry-induced Disentanglement on Graphs.pdf}
}

@article{metafundamentalairesearchdiplomacyteamfairHumanlevelPlayGame2022,
  title = {Human-Level Play in the Game of {{Diplomacy}} by Combining Language Models with Strategic Reasoning},
  author = {{META FUNDAMENTAL AI RESEARCH DIPLOMACY TEAM (FAIR)} and Bakhtin, Anton and Brown, Noam and Dinan, Emily and Farina, Gabriele and Flaherty, Colin and Fried, Daniel and Goff, Andrew and Gray, Jonathan and Hu, Hengyuan and Jacob, Athul Paul and Komeili, Mojtaba and Konath, Karthik and Kwon, Minae and Lerer, Adam and Lewis, Mike and Miller, Alexander H. and Mitts, Sasha and Renduchintala, Adithya and Roller, Stephen and Rowe, Dirk and Shi, Weiyan and Spisak, Joe and Wei, Alexander and Wu, David and Zhang, Hugh and Zijlstra, Markus},
  date = {2022-11-22},
  journaltitle = {Science},
  volume = {0},
  number = {0},
  pages = {eade9097},
  publisher = {{American Association for the Advancement of Science}},
  doi = {10.1126/science.ade9097},
  url = {https://www.science.org/doi/10.1126/science.ade9097},
  urldate = {2022-11-28},
  abstract = {Despite much progress in training AI systems to imitate human language, building agents that use language to communicate intentionally with humans in interactive environments remains a major challenge. We introduce Cicero, the first AI agent to achieve human-level performance in Diplomacy, a strategy game involving both cooperation and competition that emphasizes natural language negotiation and tactical coordination between seven players. Cicero integrates a language model with planning and reinforcement learning algorithms by inferring players' beliefs and intentions from its conversations and generating dialogue in pursuit of its plans. Across 40 games of an anonymous online Diplomacy league, Cicero achieved more than double the average score of the human players and ranked in the top 10\% of participants who played more than one game.},
  file = {/home/olaf/Zotero/storage/62KXZZ27/META FUNDAMENTAL AI RESEARCH DIPLOMACY TEAM (FAIR) et al. - 2022 - Human-level play in the game of Diplomacy by combi.pdf}
}

@inproceedings{milletariVNetFullyConvolutional2016,
  title = {V-{{Net}}: {{Fully Convolutional Neural Networks}} for {{Volumetric Medical Image Segmentation}}},
  shorttitle = {V-{{Net}}},
  booktitle = {2016 {{Fourth International Conference}} on {{3D Vision}} ({{3DV}})},
  author = {Milletari, Fausto and Navab, Nassir and Ahmadi, Seyed-Ahmad},
  date = {2016-10},
  pages = {565--571},
  doi = {10.1109/3DV.2016.79},
  abstract = {Convolutional Neural Networks (CNNs) have been recently employed to solve problems from both the computer vision and medical image analysis fields. Despite their popularity, most approaches are only able to process 2D images while most medical data used in clinical practice consists of 3D volumes. In this work we propose an approach to 3D image segmentation based on a volumetric, fully convolutional, neural network. Our CNN is trained end-to-end on MRI volumes depicting prostate, and learns to predict segmentation for the whole volume at once. We introduce a novel objective function, that we optimise during training, based on Dice coefficient. In this way we can deal with situations where there is a strong imbalance between the number of foreground and background voxels. To cope with the limited number of annotated volumes available for training, we augment the data applying random non-linear transformations and histogram matching. We show in our experimental evaluation that our approach achieves good performances on challenging test data while requiring only a fraction of the processing time needed by other previous methods.},
  eventtitle = {2016 {{Fourth International Conference}} on {{3D Vision}} ({{3DV}})},
  keywords = {Biomedical imaging,convolutional neural networks,Deep learning,Feature extraction,Image segmentation,machine learning,Magnetic resonance imaging,Neural networks,prostate,segmentation,Three-dimensional displays,Two dimensional displays},
  file = {/home/olaf/Zotero/storage/Q63FAGMM/Milletari et al. - 2016 - V-Net Fully Convolutional Neural Networks for Vol.pdf;/home/olaf/Zotero/storage/Z8CRCWU4/7785132.html}
}

@incollection{Mmap,
  title = {Mmap(3)},
  booktitle = {{{POSIX Programmer}}'s {{Manual}}},
  series = {Linux Manual Page},
  url = {https://man7.org/linux/man-pages/man3/mmap.3p.html},
  urldate = {2023-03-15},
  file = {/home/olaf/Zotero/storage/THPMMCHV/mmap.3p.html}
}

@inproceedings{Morris+2020,
  title = {{{TUDataset}}: {{A}} Collection of Benchmark Datasets for Learning with Graphs},
  booktitle = {{{ICML}} 2020 Workshop on Graph Representation Learning and beyond ({{GRL}}+ 2020)},
  author = {Morris, Christopher and Kriege, Nils M. and Bause, Franka and Kersting, Kristian and Mutzel, Petra and Neumann, Marion},
  date = {2020},
  eprint = {2007.08663},
  eprinttype = {arxiv},
  url = {www.graphlearning.io}
}

@online{morrisTUDatasetCollectionBenchmark2020,
  title = {{{TUDataset}}: {{A}} Collection of Benchmark Datasets for Learning with Graphs},
  shorttitle = {{{TUDataset}}},
  author = {Morris, Christopher and Kriege, Nils M. and Bause, Franka and Kersting, Kristian and Mutzel, Petra and Neumann, Marion},
  date = {2020-07-16},
  number = {arXiv:2007.08663},
  eprint = {arXiv:2007.08663},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.2007.08663},
  url = {http://arxiv.org/abs/2007.08663},
  urldate = {2023-03-20},
  abstract = {Recently, there has been an increasing interest in (supervised) learning with graph data, especially using graph neural networks. However, the development of meaningful benchmark datasets and standardized evaluation procedures is lagging, consequently hindering advancements in this area. To address this, we introduce the TUDataset for graph classification and regression. The collection consists of over 120 datasets of varying sizes from a wide range of applications. We provide Python-based data loaders, kernel and graph neural network baseline implementations, and evaluation tools. Here, we give an overview of the datasets, standardized evaluation procedures, and provide baseline experiments. All datasets are available at www.graphlearning.io. The experiments are fully reproducible from the code available at www.github.com/chrsmrrs/tudataset.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/home/olaf/Zotero/storage/62LKDF2L/Morris et al. - 2020 - TUDataset A collection of benchmark datasets for .pdf;/home/olaf/Zotero/storage/WKTGPZ5Q/2007.html}
}

@online{morrisWeisfeilerLemanGo2021,
  title = {Weisfeiler and {{Leman Go Neural}}: {{Higher-order Graph Neural Networks}}},
  shorttitle = {Weisfeiler and {{Leman Go Neural}}},
  author = {Morris, Christopher and Ritzert, Martin and Fey, Matthias and Hamilton, William L. and Lenssen, Jan Eric and Rattan, Gaurav and Grohe, Martin},
  date = {2021-11-30},
  number = {arXiv:1810.02244},
  eprint = {arXiv:1810.02244},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.1810.02244},
  url = {http://arxiv.org/abs/1810.02244},
  urldate = {2022-12-06},
  abstract = {In recent years, graph neural networks (GNNs) have emerged as a powerful neural architecture to learn vector representations of nodes and graphs in a supervised, end-to-end fashion. Up to now, GNNs have only been evaluated empirically -- showing promising results. The following work investigates GNNs from a theoretical point of view and relates them to the \$1\$-dimensional Weisfeiler-Leman graph isomorphism heuristic (\$1\$-WL). We show that GNNs have the same expressiveness as the \$1\$-WL in terms of distinguishing non-isomorphic (sub-)graphs. Hence, both algorithms also have the same shortcomings. Based on this, we propose a generalization of GNNs, so-called \$k\$-dimensional GNNs (\$k\$-GNNs), which can take higher-order graph structures at multiple scales into account. These higher-order structures play an essential role in the characterization of social networks and molecule graphs. Our experimental evaluation confirms our theoretical findings as well as confirms that higher-order information is useful in the task of graph classification and regression.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/home/olaf/Zotero/storage/232YUM9E/Morris et al. - 2021 - Weisfeiler and Leman Go Neural Higher-order Graph.pdf;/home/olaf/Zotero/storage/7N78D2IK/1810.html}
}

@online{morrisWeisfeilerLemanGo2023,
  title = {Weisfeiler and {{Leman}} Go {{Machine Learning}}: {{The Story}} so Far},
  shorttitle = {Weisfeiler and {{Leman}} Go {{Machine Learning}}},
  author = {Morris, Christopher and Lipman, Yaron and Maron, Haggai and Rieck, Bastian and Kriege, Nils M. and Grohe, Martin and Fey, Matthias and Borgwardt, Karsten},
  date = {2023-03-15},
  number = {arXiv:2112.09992},
  eprint = {arXiv:2112.09992},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.2112.09992},
  url = {http://arxiv.org/abs/2112.09992},
  urldate = {2023-03-20},
  abstract = {In recent years, algorithms and neural architectures based on the Weisfeiler-Leman algorithm, a well-known heuristic for the graph isomorphism problem, have emerged as a powerful tool for machine learning with graphs and relational data. Here, we give a comprehensive overview of the algorithm's use in a machine-learning setting, focusing on the supervised regime. We discuss the theoretical background, show how to use it for supervised graph and node representation learning, discuss recent extensions, and outline the algorithm's connection to (permutation-)equivariant neural architectures. Moreover, we give an overview of current applications and future directions to stimulate further research.},
  pubstate = {preprint},
  keywords = {Computer Science - Data Structures and Algorithms,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/home/olaf/Zotero/storage/CCUASJ8T/Morris et al. - 2023 - Weisfeiler and Leman go Machine Learning The Stor.pdf;/home/olaf/Zotero/storage/DFSICFJJ/2112.html}
}

@inproceedings{pappTheoreticalComparisonGraph2022,
  title = {A {{Theoretical Comparison}} of {{Graph Neural Network Extensions}}},
  booktitle = {Proceedings of the 39th {{International Conference}} on {{Machine Learning}}},
  author = {Papp, Pál András and Wattenhofer, Roger},
  date = {2022-06-28},
  pages = {17323--17345},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v162/papp22a.html},
  urldate = {2023-03-20},
  abstract = {We study and compare different Graph Neural Network extensions that increase the expressive power of GNNs beyond the Weisfeiler-Leman test. We focus on (i) GNNs based on higher order WL methods, (ii) GNNs that preprocess small substructures in the graph, (iii) GNNs that preprocess the graph up to a small radius, and (iv) GNNs that slightly perturb the graph to compute an embedding. We begin by presenting a simple improvement for this last extension that strictly increases the expressive power of this GNN variant. Then, as our main result, we compare the expressiveness of these extensions to each other through a series of example constructions that can be distinguished by one of the extensions, but not by another one. We also show negative examples that are particularly challenging for each of the extensions, and we prove several claims about the ability of these extensions to count cliques and cycles in the graph.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/home/olaf/Zotero/storage/VNFW4VY9/Papp et Wattenhofer - 2022 - A Theoretical Comparison of Graph Neural Network E.pdf}
}

@online{peiGeomGCNGeometricGraph2020,
  title = {Geom-{{GCN}}: {{Geometric Graph Convolutional Networks}}},
  shorttitle = {Geom-{{GCN}}},
  author = {Pei, Hongbin and Wei, Bingzhe and Chang, Kevin Chen-Chuan and Lei, Yu and Yang, Bo},
  date = {2020-02-13},
  number = {arXiv:2002.05287},
  eprint = {arXiv:2002.05287},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.2002.05287},
  url = {http://arxiv.org/abs/2002.05287},
  urldate = {2022-12-09},
  abstract = {Message-passing neural networks (MPNNs) have been successfully applied to representation learning on graphs in a variety of real-world applications. However, two fundamental weaknesses of MPNNs' aggregators limit their ability to represent graph-structured data: losing the structural information of nodes in neighborhoods and lacking the ability to capture long-range dependencies in disassortative graphs. Few studies have noticed the weaknesses from different perspectives. From the observations on classical neural network and network geometry, we propose a novel geometric aggregation scheme for graph neural networks to overcome the two weaknesses. The behind basic idea is the aggregation on a graph can benefit from a continuous space underlying the graph. The proposed aggregation scheme is permutation-invariant and consists of three modules, node embedding, structural neighborhood, and bi-level aggregation. We also present an implementation of the scheme in graph convolutional networks, termed Geom-GCN (Geometric Graph Convolutional Networks), to perform transductive learning on graphs. Experimental results show the proposed Geom-GCN achieved state-of-the-art performance on a wide range of open datasets of graphs. Code is available at https://github.com/graphdml-uiuc-jlu/geom-gcn.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/olaf/Zotero/storage/2DRBS9BM/Pei et al. - 2020 - Geom-GCN Geometric Graph Convolutional Networks.pdf;/home/olaf/Zotero/storage/GXHUT4MA/2002.html}
}

@online{phamColumnNetworksCollective2016,
  title = {Column {{Networks}} for {{Collective Classification}}},
  author = {Pham, Trang and Tran, Truyen and Phung, Dinh and Venkatesh, Svetha},
  date = {2016-11-28},
  number = {arXiv:1609.04508},
  eprint = {arXiv:1609.04508},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.1609.04508},
  url = {http://arxiv.org/abs/1609.04508},
  urldate = {2022-12-19},
  abstract = {Relational learning deals with data that are characterized by relational structures. An important task is collective classification, which is to jointly classify networked objects. While it holds a great promise to produce a better accuracy than non-collective classifiers, collective classification is computational challenging and has not leveraged on the recent breakthroughs of deep learning. We present Column Network (CLN), a novel deep learning model for collective classification in multi-relational domains. CLN has many desirable theoretical properties: (i) it encodes multi-relations between any two instances; (ii) it is deep and compact, allowing complex functions to be approximated at the network level with a small set of free parameters; (iii) local and relational features are learned simultaneously; (iv) long-range, higher-order dependencies between instances are supported naturally; and (v) crucially, learning and inference are efficient, linear in the size of the network and the number of relations. We evaluate CLN on multiple real-world applications: (a) delay prediction in software projects, (b) PubMed Diabetes publication classification and (c) film genre classification. In all applications, CLN demonstrates a higher accuracy than state-of-the-art rivals.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/olaf/Zotero/storage/9AFFY2HK/Pham et al. - 2016 - Column Networks for Collective Classification.pdf;/home/olaf/Zotero/storage/9AYJ87LK/1609.html}
}

@online{phanResolvingClassImbalance2020,
  title = {Resolving {{Class Imbalance}} in {{Object Detection}} with {{Weighted Cross Entropy Losses}}},
  author = {Phan, Trong Huy and Yamamoto, Kazuma},
  date = {2020-06-02},
  number = {arXiv:2006.01413},
  eprint = {arXiv:2006.01413},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.2006.01413},
  url = {http://arxiv.org/abs/2006.01413},
  urldate = {2023-03-16},
  abstract = {Object detection is an important task in computer vision which serves a lot of real-world applications such as autonomous driving, surveillance and robotics. Along with the rapid thrive of large-scale data, numerous state-of-the-art generalized object detectors (e.g. Faster R-CNN, YOLO, SSD) were developed in the past decade. Despite continual efforts in model modification and improvement in training strategies to boost detection accuracy, there are still limitations in performance of detectors when it comes to specialized datasets with uneven object class distributions. This originates from the common usage of Cross Entropy loss function for object classification sub-task that simply ignores the frequency of appearance of object class during training, and thus results in lower accuracies for object classes with fewer number of samples. Class-imbalance in general machine learning has been widely studied, however, little attention has been paid on the subject of object detection. In this paper, we propose to explore and overcome such problem by application of several weighted variants of Cross Entropy loss, for examples Balanced Cross Entropy, Focal Loss and Class-Balanced Loss Based on Effective Number of Samples to our object detector. Experiments with BDD100K (a highly class-imbalanced driving database acquired from on-vehicle cameras capturing mostly Car-class objects and other minority object classes such as Bus, Person and Motor) have proven better class-wise performances of detector trained with the afore-mentioned loss functions.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/olaf/Zotero/storage/LF848ZPY/Phan et Yamamoto - 2020 - Resolving Class Imbalance in Object Detection with.pdf;/home/olaf/Zotero/storage/FXNG32NQ/2006.html}
}

@online{qianOrderedSubgraphAggregation2022,
  title = {Ordered {{Subgraph Aggregation Networks}}},
  author = {Qian, Chendi and Rattan, Gaurav and Geerts, Floris and Morris, Christopher and Niepert, Mathias},
  date = {2022-10-15},
  number = {arXiv:2206.11168},
  eprint = {arXiv:2206.11168},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.2206.11168},
  url = {http://arxiv.org/abs/2206.11168},
  urldate = {2023-03-20},
  abstract = {Numerous subgraph-enhanced graph neural networks (GNNs) have emerged recently, provably boosting the expressive power of standard (message-passing) GNNs. However, there is a limited understanding of how these approaches relate to each other and to the Weisfeiler-Leman hierarchy. Moreover, current approaches either use all subgraphs of a given size, sample them uniformly at random, or use hand-crafted heuristics instead of learning to select subgraphs in a data-driven manner. Here, we offer a unified way to study such architectures by introducing a theoretical framework and extending the known expressivity results of subgraph-enhanced GNNs. Concretely, we show that increasing subgraph size always increases the expressive power and develop a better understanding of their limitations by relating them to the established \$k\textbackslash text\{-\}\textbackslash mathsf\{WL\}\$ hierarchy. In addition, we explore different approaches for learning to sample subgraphs using recent methods for backpropagating through complex discrete probability distributions. Empirically, we study the predictive performance of different subgraph-enhanced GNNs, showing that our data-driven architectures increase prediction accuracy on standard benchmark datasets compared to non-data-driven subgraph-enhanced graph neural networks while reducing computation time.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Data Structures and Algorithms,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/home/olaf/Zotero/storage/GBD87LU4/Qian et al. - 2022 - Ordered Subgraph Aggregation Networks.pdf;/home/olaf/Zotero/storage/EH68SU2X/2206.html}
}

@article{radfordLanguageModelsAre,
  title = {Language {{Models}} Are {{Unsupervised Multitask Learners}}},
  author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  pages = {24},
  abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
  langid = {english},
  file = {/home/olaf/Zotero/storage/YKX775HL/Radford et al. - Language Models are Unsupervised Multitask Learner.pdf}
}

@online{rongDropEdgeDeepGraph2020,
  title = {{{DropEdge}}: {{Towards Deep Graph Convolutional Networks}} on {{Node Classification}}},
  shorttitle = {{{DropEdge}}},
  author = {Rong, Yu and Huang, Wenbing and Xu, Tingyang and Huang, Junzhou},
  date = {2020-03-12},
  number = {arXiv:1907.10903},
  eprint = {arXiv:1907.10903},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.1907.10903},
  url = {http://arxiv.org/abs/1907.10903},
  urldate = {2022-12-07},
  abstract = {\textbackslash emph\{Over-fitting\} and \textbackslash emph\{over-smoothing\} are two main obstacles of developing deep Graph Convolutional Networks (GCNs) for node classification. In particular, over-fitting weakens the generalization ability on small dataset, while over-smoothing impedes model training by isolating output representations from the input features with the increase in network depth. This paper proposes DropEdge, a novel and flexible technique to alleviate both issues. At its core, DropEdge randomly removes a certain number of edges from the input graph at each training epoch, acting like a data augmenter and also a message passing reducer. Furthermore, we theoretically demonstrate that DropEdge either reduces the convergence speed of over-smoothing or relieves the information loss caused by it. More importantly, our DropEdge is a general skill that can be equipped with many other backbone models (e.g. GCN, ResGCN, GraphSAGE, and JKNet) for enhanced performance. Extensive experiments on several benchmarks verify that DropEdge consistently improves the performance on a variety of both shallow and deep GCNs. The effect of DropEdge on preventing over-smoothing is empirically visualized and validated as well. Codes are released on\textasciitilde\textbackslash url\{https://github.com/DropEdge/DropEdge\}.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Computer Science - Networking and Internet Architecture,Statistics - Machine Learning},
  file = {/home/olaf/Zotero/storage/HLGMCQE8/Rong et al. - 2020 - DropEdge Towards Deep Graph Convolutional Network.pdf;/home/olaf/Zotero/storage/IY2Q8AWP/1907.html}
}

@online{ronnebergerUNetConvolutionalNetworks2015,
  title = {U-{{Net}}: {{Convolutional Networks}} for {{Biomedical Image Segmentation}}},
  shorttitle = {U-{{Net}}},
  author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  date = {2015-05-18},
  number = {arXiv:1505.04597},
  eprint = {arXiv:1505.04597},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.1505.04597},
  url = {http://arxiv.org/abs/1505.04597},
  urldate = {2023-02-09},
  abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/olaf/Zotero/storage/IGEAJD8G/Ronneberger et al. - 2015 - U-Net Convolutional Networks for Biomedical Image.pdf;/home/olaf/Zotero/storage/KE5RMESK/1505.html}
}

@online{rozemberczkiMultiscaleAttributedNode2021,
  title = {Multi-Scale {{Attributed Node Embedding}}},
  author = {Rozemberczki, Benedek and Allen, Carl and Sarkar, Rik},
  date = {2021-03-21},
  number = {arXiv:1909.13021},
  eprint = {arXiv:1909.13021},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.1909.13021},
  url = {http://arxiv.org/abs/1909.13021},
  urldate = {2022-12-13},
  abstract = {We present network embedding algorithms that capture information about a node from the local distribution over node attributes around it, as observed over random walks following an approach similar to Skip-gram. Observations from neighborhoods of different sizes are either pooled (AE) or encoded distinctly in a multi-scale approach (MUSAE). Capturing attribute-neighborhood relationships over multiple scales is useful for a diverse range of applications, including latent feature identification across disconnected networks with similar attributes. We prove theoretically that matrices of node-feature pointwise mutual information are implicitly factorized by the embeddings. Experiments show that our algorithms are robust, computationally efficient and outperform comparable models on social networks and web graphs.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Computer Science - Networking and Internet Architecture,Computer Science - Social and Information Networks,Statistics - Machine Learning},
  file = {/home/olaf/Zotero/storage/MSLCUUQD/Rozemberczki et al. - 2021 - Multi-scale Attributed Node Embedding.pdf;/home/olaf/Zotero/storage/2GZW72VY/1909.html}
}

@online{rybinInvariantLayersGraphs2023,
  title = {Invariant {{Layers}} for {{Graphs}} with {{Nodes}} of {{Different Types}}},
  author = {Rybin, Dmitry and Sun, Ruoyu and Luo, Zhi-Quan},
  date = {2023-02-27},
  number = {arXiv:2302.13551},
  eprint = {arXiv:2302.13551},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.2302.13551},
  url = {http://arxiv.org/abs/2302.13551},
  urldate = {2023-03-20},
  abstract = {Neural networks that satisfy invariance with respect to input permutations have been widely studied in machine learning literature. However, in many applications, only a subset of all input permutations is of interest. For heterogeneous graph data, one can focus on permutations that preserve node types. We fully characterize linear layers invariant to such permutations. We verify experimentally that implementing these layers in graph neural network architectures allows learning important node interactions more effectively than existing techniques. We show that the dimension of space of these layers is given by a generalization of Bell numbers, extending the work (Maron et al., 2019). We further narrow the invariant network design space by addressing a question about the sizes of tensor layers necessary for function approximation on graph data. Our findings suggest that function approximation on a graph with \$n\$ nodes can be done with tensors of sizes \$\textbackslash leq n\$, which is tighter than the best-known bound \$\textbackslash leq n(n-1)/2\$. For \$d \textbackslash times d\$ image data with translation symmetry, our methods give a tight upper bound \$2d - 1\$ (instead of \$d\^\{4\}\$) on sizes of invariant tensor generators via a surprising connection to Davenport constants.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/home/olaf/Zotero/storage/IIX94D89/Rybin et al. - 2023 - Invariant Layers for Graphs with Nodes of Differen.pdf;/home/olaf/Zotero/storage/7B7U976E/2302.html}
}

@article{shelhamerFullyConvolutionalNetworks2017,
  title = {Fully {{Convolutional Networks}} for {{Semantic Segmentation}}},
  author = {Shelhamer, Evan and Long, Jonathan and Darrell, Trevor},
  date = {2017-04},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {39},
  number = {4},
  pages = {640--651},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2016.2572683},
  abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, improve on the previous best result in semantic segmentation. Our key insight is to build “fully convolutional” networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional networks achieve improved segmentation of PASCAL VOC (30\% relative improvement to 67.2\% mean IU on 2012), NYUDv2, SIFT Flow, and PASCAL-Context, while inference takes one tenth of a second for a typical image.},
  eventtitle = {{{IEEE Transactions}} on {{Pattern Analysis}} and {{Machine Intelligence}}},
  keywords = {Computer architecture,Convolution,Convolutional Networks,Deep Learning,Fuses,Image segmentation,Proposals,Semantic Segmentation,Semantics,Training,Transfer Learning},
  file = {/home/olaf/Zotero/storage/PK4VMNB9/Shelhamer et al. - 2017 - Fully Convolutional Networks for Semantic Segmenta.pdf;/home/olaf/Zotero/storage/JMCLBEQ7/7478072.html}
}

@article{siddiqueUNetItsVariants2021,
  title = {U-{{Net}} and {{Its Variants}} for {{Medical Image Segmentation}}: {{A Review}} of {{Theory}} and {{Applications}}},
  shorttitle = {U-{{Net}} and {{Its Variants}} for {{Medical Image Segmentation}}},
  author = {Siddique, Nahian and Paheding, Sidike and Elkin, Colin P. and Devabhaktuni, Vijay},
  date = {2021},
  journaltitle = {IEEE Access},
  volume = {9},
  pages = {82031--82057},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2021.3086020},
  abstract = {U-net is an image segmentation technique developed primarily for image segmentation tasks. These traits provide U-net with a high utility within the medical imaging community and have resulted in extensive adoption of U-net as the primary tool for segmentation tasks in medical imaging. The success of U-net is evident in its widespread use in nearly all major image modalities, from CT scans and MRI to X-rays and microscopy. Furthermore, while U-net is largely a segmentation tool, there have been instances of the use of U-net in other applications. Given that U-net's potential is still increasing, this narrative literature review examines the numerous developments and breakthroughs in the U-net architecture and provides observations on recent trends. We also discuss the many innovations that have advanced in deep learning and discuss how these tools facilitate U-net. In addition, we review the different image modalities and application areas that have been enhanced by U-net.},
  eventtitle = {{{IEEE Access}}},
  keywords = {Biomedical imaging,Computer architecture,Convolution,deep learning,Deep learning,Image segmentation,Logic gates,neural network architecture,segmentation,Three-dimensional displays,U-net},
  file = {/home/olaf/Zotero/storage/64N4FEJD/Siddique et al. - 2021 - U-Net and Its Variants for Medical Image Segmentat.pdf;/home/olaf/Zotero/storage/EG98YPYJ/9446143.html}
}

@online{simonyanDeepConvolutionalNetworks2014,
  title = {Deep {{Inside Convolutional Networks}}: {{Visualising Image Classification Models}} and {{Saliency Maps}}},
  shorttitle = {Deep {{Inside Convolutional Networks}}},
  author = {Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
  date = {2014-04-19},
  number = {arXiv:1312.6034},
  eprint = {arXiv:1312.6034},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.1312.6034},
  url = {http://arxiv.org/abs/1312.6034},
  urldate = {2022-12-31},
  abstract = {This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score [Erhan et al., 2009], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [Zeiler et al., 2013].},
  pubstate = {preprint},
  version = {2},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/olaf/Zotero/storage/CIMA5E3P/Simonyan et al. - 2014 - Deep Inside Convolutional Networks Visualising Im.pdf;/home/olaf/Zotero/storage/HJ3YBN5C/1312.html}
}

@online{sinhaCLUTRRDiagnosticBenchmark2019,
  title = {{{CLUTRR}}: {{A Diagnostic Benchmark}} for {{Inductive Reasoning}} from {{Text}}},
  shorttitle = {{{CLUTRR}}},
  author = {Sinha, Koustuv and Sodhani, Shagun and Dong, Jin and Pineau, Joelle and Hamilton, William L.},
  date = {2019-09-03},
  number = {arXiv:1908.06177},
  eprint = {arXiv:1908.06177},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.1908.06177},
  url = {http://arxiv.org/abs/1908.06177},
  urldate = {2022-12-04},
  abstract = {The recent success of natural language understanding (NLU) systems has been troubled by results highlighting the failure of these models to generalize in a systematic and robust way. In this work, we introduce a diagnostic benchmark suite, named CLUTRR, to clarify some key issues related to the robustness and systematicity of NLU systems. Motivated by classic work on inductive logic programming, CLUTRR requires that an NLU system infer kinship relations between characters in short stories. Successful performance on this task requires both extracting relationships between entities, as well as inferring the logical rules governing these relationships. CLUTRR allows us to precisely measure a model's ability for systematic generalization by evaluating on held-out combinations of logical rules, and it allows us to evaluate a model's robustness by adding curated noise facts. Our empirical results highlight a substantial performance gap between state-of-the-art NLU models (e.g., BERT and MAC) and a graph neural network model that works directly with symbolic inputs---with the graph-based model exhibiting both stronger generalization and greater robustness.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Logic in Computer Science,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/olaf/Zotero/storage/696E8MWR/Sinha et al. - 2019 - CLUTRR A Diagnostic Benchmark for Inductive Reaso.pdf;/home/olaf/Zotero/storage/SNYYX475/1908.html}
}

@article{soomroImageSegmentationMR2023a,
  title = {Image {{Segmentation}} for {{MR Brain Tumor Detection Using Machine Learning}}: {{A Review}}},
  shorttitle = {Image {{Segmentation}} for {{MR Brain Tumor Detection Using Machine Learning}}},
  author = {Soomro, Toufique A. and Zheng, Lihong and Afifi, Ahmed J. and Ali, Ahmed and Soomro, Shafiullah and Yin, Ming and Gao, Junbin},
  date = {2023},
  journaltitle = {IEEE Reviews in Biomedical Engineering},
  volume = {16},
  pages = {70--90},
  issn = {1941-1189},
  doi = {10.1109/RBME.2022.3185292},
  abstract = {Magnetic Resonance Imaging (MRI) has commonly been used to detect and diagnose brain disease and monitor treatment as non-invasive imaging technology. MRI produces three-dimensional images that help neurologists to identify anomalies from brain images precisely. However, this is a time-consuming and labor-intensive process. The improvement in machine learning and efficient computation provides a computer-aid solution to analyze MRI images and identify the abnormality quickly and accurately. Image segmentation has become a hot and research-oriented area in the medical image analysis community. The computer-aid system for brain abnormalities identification provides the possibility for quickly classifying the disease for early treatment. This article presents a review of the research papers (from 1998 to 2020) on brain tumors segmentation from MRI images. We examined the core segmentation algorithms of each research paper in detail. This article provides readers with a complete overview of the topic and new dimensions of how numerous machine learning and image segmentation approaches are applied to identify brain tumors. By comparing the state-of-the-art and new cutting-edge methods, the deep learning methods are more effective for the segmentation of the tumor from MRI images of the brain.},
  eventtitle = {{{IEEE Reviews}} in {{Biomedical Engineering}}},
  keywords = {Brain,Brain images,brain tumors,Cancer,CNN models,Computed tomography,deep learning,Diseases,Image segmentation,Magnetic resonance imaging,MRI,segmentation,Tumors},
  file = {/home/olaf/Zotero/storage/ILV8FDLK/Soomro et al. - 2023 - Image Segmentation for MR Brain Tumor Detection Us.pdf;/home/olaf/Zotero/storage/NU895Z8U/9804867.html}
}

@online{stahlbergLearningGeneralizedPolicies2022,
  title = {Learning {{Generalized Policies Without Supervision Using GNNs}}},
  author = {Ståhlberg, Simon and Bonet, Blai and Geffner, Hector},
  date = {2022-05-12},
  number = {arXiv:2205.06002},
  eprint = {arXiv:2205.06002},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.2205.06002},
  url = {http://arxiv.org/abs/2205.06002},
  urldate = {2023-03-20},
  abstract = {We consider the problem of learning generalized policies for classical planning domains using graph neural networks from small instances represented in lifted STRIPS. The problem has been considered before but the proposed neural architectures are complex and the results are often mixed. In this work, we use a simple and general GNN architecture and aim at obtaining crisp experimental results and a deeper understanding: either the policy greedy in the learned value function achieves close to 100\% generalization over instances larger than those used in training, or the failure must be understood, and possibly fixed, logically. For this, we exploit the relation established between the expressive power of GNNs and the \$C\_\{2\}\$ fragment of first-order logic (namely, FOL with 2 variables and counting quantifiers). We find for example that domains with general policies that require more expressive features can be solved with GNNs once the states are extended with suitable "derived atoms" encoding role compositions and transitive closures that do not fit into \$C\_\{2\}\$. The work follows the GNN approach for learning optimal general policies in a supervised fashion (Stahlberg, Bonet, Geffner, 2022); but the learned policies are no longer required to be optimal (which expands the scope, as many planning domains do not have general optimal policies) and are learned without supervision. Interestingly, value-based reinforcement learning methods that aim to produce optimal policies, do not always yield policies that generalize, as the goals of optimality and generality are in conflict in domains where optimal planning is NP-hard.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/olaf/Zotero/storage/ELBTCYI3/Ståhlberg et al. - 2022 - Learning Generalized Policies Without Supervision .pdf;/home/olaf/Zotero/storage/WUDRKS8H/2205.html}
}

@inproceedings{strudelSegmenterTransformerSemantic2021,
  title = {Segmenter: {{Transformer}} for {{Semantic Segmentation}}},
  shorttitle = {Segmenter},
  booktitle = {2021 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Strudel, Robin and Garcia, Ricardo and Laptev, Ivan and Schmid, Cordelia},
  date = {2021-10},
  pages = {7242--7252},
  publisher = {{IEEE}},
  location = {{Montreal, QC, Canada}},
  doi = {10.1109/ICCV48922.2021.00717},
  url = {https://ieeexplore.ieee.org/document/9710959/},
  urldate = {2023-03-14},
  abstract = {Image segmentation is often ambiguous at the level of individual image patches and requires contextual information to reach label consensus. In this paper we introduce Segmenter, a transformer model for semantic segmentation. In contrast to convolution-based methods, our approach allows to model global context already at the first layer and throughout the network. We build on the recent Vision Transformer (ViT) and extend it to semantic segmentation. To do so, we rely on the output embeddings corresponding to image patches and obtain class labels from these embeddings with a point-wise linear decoder or a mask transformer decoder. We leverage models pre-trained for image classification and show that we can fine-tune them on moderate sized datasets available for semantic segmentation. The linear decoder allows to obtain excellent results already, but the performance can be further improved by a mask transformer generating class masks. We conduct an extensive ablation study to show the impact of the different parameters, in particular the performance is better for large models and small patch sizes. Segmenter attains excellent results for semantic segmentation. It outperforms the state of the art on both ADE20K and Pascal Context datasets and is competitive on Cityscapes.},
  eventtitle = {2021 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  isbn = {978-1-66542-812-5},
  langid = {english},
  file = {/home/olaf/Zotero/storage/TGGMQLFA/Strudel et al. - 2021 - Segmenter Transformer for Semantic Segmentation.pdf}
}

@inproceedings{sudreGeneralisedDiceOverlap2017,
  title = {Generalised {{Dice Overlap}} as a {{Deep Learning Loss Function}} for {{Highly Unbalanced Segmentations}}},
  booktitle = {Deep {{Learning}} in {{Medical Image Analysis}} and {{Multimodal Learning}} for {{Clinical Decision Support}}},
  author = {Sudre, Carole H. and Li, Wenqi and Vercauteren, Tom and Ourselin, Sebastien and Jorge Cardoso, M.},
  editor = {Cardoso, M. Jorge and Arbel, Tal and Carneiro, Gustavo and Syeda-Mahmood, Tanveer and Tavares, João Manuel R.S. and Moradi, Mehdi and Bradley, Andrew and Greenspan, Hayit and Papa, João Paulo and Madabhushi, Anant and Nascimento, Jacinto C. and Cardoso, Jaime S. and Belagiannis, Vasileios and Lu, Zhi},
  date = {2017},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {240--248},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-67558-9_28},
  abstract = {Deep-learning has proved in recent years to be a powerful tool for image analysis and is now widely used to segment both 2D and 3D medical images. Deep-learning segmentation frameworks rely not only on the choice of network architecture but also on the choice of loss function. When the segmentation process targets rare observations, a severe class imbalance is likely to occur between candidate labels, thus resulting in sub-optimal performance. In order to mitigate this issue, strategies such as the weighted cross-entropy function, the sensitivity function or the Dice loss function, have been proposed. In this work, we investigate the behavior of these loss functions and their sensitivity to learning rate tuning in the presence of different rates of label imbalance across 2D and 3D segmentation tasks. We also propose to use the class re-balancing properties of the Generalized Dice overlap, a known metric for segmentation assessment, as a robust and accurate deep-learning loss function for unbalanced tasks.},
  isbn = {978-3-319-67558-9},
  langid = {english},
  file = {/home/olaf/Zotero/storage/BKRI8NTH/Sudre et al. - 2017 - Generalised Dice Overlap as a Deep Learning Loss F.pdf}
}

@inproceedings{sunPositionawareStructureLearning2022,
  title = {Position-Aware {{Structure Learning}} for {{Graph Topology-imbalance}} by {{Relieving Under-reaching}} and {{Over-squashing}}},
  booktitle = {Proceedings of the 31st {{ACM International Conference}} on {{Information}} \& {{Knowledge Management}}},
  author = {Sun, Qingyun and Li, Jianxin and Yuan, Haonan and Fu, Xingcheng and Peng, Hao and Ji, Cheng and Li, Qian and Yu, Philip S.},
  date = {2022-10-17},
  eprint = {2208.08302},
  eprinttype = {arxiv},
  eprintclass = {cs},
  pages = {1848--1857},
  doi = {10.1145/3511808.3557419},
  url = {http://arxiv.org/abs/2208.08302},
  urldate = {2022-12-08},
  abstract = {Topology-imbalance is a graph-specific imbalance problem caused by the uneven topology positions of labeled nodes, which significantly damages the performance of GNNs. What topology-imbalance means and how to measure its impact on graph learning remain under-explored. In this paper, we provide a new understanding of topology-imbalance from a global view of the supervision information distribution in terms of under-reaching and over-squashing, which motivates two quantitative metrics as measurements. In light of our analysis, we propose a novel position-aware graph structure learning framework named PASTEL, which directly optimizes the information propagation path and solves the topology-imbalance issue in essence. Our key insight is to enhance the connectivity of nodes within the same class for more supervision information, thereby relieving the under-reaching and over-squashing phenomena. Specifically, we design an anchor-based position encoding mechanism, which better incorporates relative topology position and enhances the intra-class inductive bias by maximizing the label influence. We further propose a class-wise conflict measure as the edge weights, which benefits the separation of different node classes. Extensive experiments demonstrate the superior potential and adaptability of PASTEL in enhancing GNNs' power in different data annotation scenarios.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/olaf/Zotero/storage/XF9WFVSI/Sun et al. - 2022 - Position-aware Structure Learning for Graph Topolo.pdf;/home/olaf/Zotero/storage/E4XJM338/2208.html}
}

@article{tingyangSubgraphSamplingStrategy,
  title = {Subgraph {{Sampling Strategy}} for {{Equivariant Subgraph Aggregation Network Based}} on {{Weisfeiler-Lehman Similarity}}},
  author = {Tingyang, Yu},
  abstract = {Subgraph augmentation is a widely used method for graph classification when the original graph can not be efficiently distinguished for the network. With the rapid development of message-passing graph neural networks (MPNN), many MPNN architectures are designed to process graphs augmented by their subgraphs. In this work, we present a new subgraph sampling strategy EGO+WL based on Weisfeiler-Lehman similarity. It achieves superior classification accuracy on TU datasets compared to the existing state-of-the-art strategies. It further reduces the space complexity up to 55\% on dataset IMDB-MULTI compared to the second best strategy and reduces 10-20\% training time on dataset NCI1. The code is available at https://github.com/YistYU/ESAN WLS.},
  langid = {english},
  file = {/home/olaf/Zotero/storage/GHT4WIIM/Tingyang - Subgraph Sampling Strategy for Equivariant Subgrap.pdf}
}

@online{toppingUnderstandingOversquashingBottlenecks2022,
  title = {Understanding Over-Squashing and Bottlenecks on Graphs via Curvature},
  author = {Topping, Jake and Di Giovanni, Francesco and Chamberlain, Benjamin Paul and Dong, Xiaowen and Bronstein, Michael M.},
  date = {2022-11-12},
  number = {arXiv:2111.14522},
  eprint = {arXiv:2111.14522},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.2111.14522},
  url = {http://arxiv.org/abs/2111.14522},
  urldate = {2022-12-08},
  abstract = {Most graph neural networks (GNNs) use the message passing paradigm, in which node features are propagated on the input graph. Recent works pointed to the distortion of information flowing from distant nodes as a factor limiting the efficiency of message passing for tasks relying on long-distance interactions. This phenomenon, referred to as 'over-squashing', has been heuristically attributed to graph bottlenecks where the number of \$k\$-hop neighbors grows rapidly with \$k\$. We provide a precise description of the over-squashing phenomenon in GNNs and analyze how it arises from bottlenecks in the graph. For this purpose, we introduce a new edge-based combinatorial curvature and prove that negatively curved edges are responsible for the over-squashing issue. We also propose and experimentally test a curvature-based graph rewiring method to alleviate the over-squashing.},
  pubstate = {preprint},
  version = {3},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/olaf/Zotero/storage/42JLWD99/Topping et al. - 2022 - Understanding over-squashing and bottlenecks on gr.pdf;/home/olaf/Zotero/storage/RA25TFV9/2111.html}
}

@online{vaswaniAttentionAllYou2017,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  date = {2017-12-05},
  number = {arXiv:1706.03762},
  eprint = {arXiv:1706.03762},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.1706.03762},
  url = {http://arxiv.org/abs/1706.03762},
  urldate = {2023-02-28},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/olaf/Zotero/storage/I8NSH9XQ/Vaswani et al. - 2017 - Attention Is All You Need.pdf;/home/olaf/Zotero/storage/MIY5B2GT/1706.html}
}

@online{velickovicGraphAttentionNetworks2018,
  title = {Graph {{Attention Networks}}},
  author = {Veličković, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Liò, Pietro and Bengio, Yoshua},
  date = {2018-02-04},
  number = {arXiv:1710.10903},
  eprint = {arXiv:1710.10903},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.1710.10903},
  url = {http://arxiv.org/abs/1710.10903},
  urldate = {2022-12-04},
  abstract = {We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training).},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Social and Information Networks,Statistics - Machine Learning},
  file = {/home/olaf/Zotero/storage/KSLFGFE2/Veličković et al. - 2018 - Graph Attention Networks.pdf;/home/olaf/Zotero/storage/F9UC5ISL/1710.html}
}

@online{velickovicMessagePassingAll2022,
  title = {Message Passing All the Way Up},
  author = {Veličković, Petar},
  date = {2022-02-22},
  number = {arXiv:2202.11097},
  eprint = {arXiv:2202.11097},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.2202.11097},
  url = {http://arxiv.org/abs/2202.11097},
  urldate = {2023-03-20},
  abstract = {The message passing framework is the foundation of the immense success enjoyed by graph neural networks (GNNs) in recent years. In spite of its elegance, there exist many problems it provably cannot solve over given input graphs. This has led to a surge of research on going "beyond message passing", building GNNs which do not suffer from those limitations -- a term which has become ubiquitous in regular discourse. However, have those methods truly moved beyond message passing? In this position paper, I argue about the dangers of using this term -- especially when teaching graph representation learning to newcomers. I show that any function of interest we want to compute over graphs can, in all likelihood, be expressed using pairwise message passing -- just over a potentially modified graph, and argue how most practical implementations subtly do this kind of trick anyway. Hoping to initiate a productive discussion, I propose replacing "beyond message passing" with a more tame term, "augmented message passing".},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Computer Science - Social and Information Networks,Statistics - Machine Learning},
  file = {/home/olaf/Zotero/storage/4DSYL95B/Veličković - 2022 - Message passing all the way up.pdf;/home/olaf/Zotero/storage/K22UZZ5J/2202.html}
}

@inproceedings{wangAutomaticBrainTumor2018,
  title = {Automatic {{Brain Tumor Segmentation Using Cascaded Anisotropic Convolutional Neural Networks}}},
  booktitle = {Brainlesion: {{Glioma}}, {{Multiple Sclerosis}}, {{Stroke}} and {{Traumatic Brain Injuries}}},
  author = {Wang, Guotai and Li, Wenqi and Ourselin, Sébastien and Vercauteren, Tom},
  editor = {Crimi, Alessandro and Bakas, Spyridon and Kuijf, Hugo and Menze, Bjoern and Reyes, Mauricio},
  date = {2018},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {178--190},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-75238-9_16},
  abstract = {A cascade of fully convolutional neural networks is proposed to segment multi-modal Magnetic Resonance (MR) images with brain tumor into background and three hierarchical regions: whole tumor, tumor core and enhancing tumor core. The cascade is designed to decompose the multi-class segmentation problem into a sequence of three binary segmentation problems according to the subregion hierarchy. The whole tumor is segmented in the first step and the bounding box of the result is used for the tumor core segmentation in the second step. The enhancing tumor core is then segmented based on the bounding box of the tumor core segmentation result. Our networks consist of multiple layers of anisotropic and dilated convolution filters, and they are combined with multi-view fusion to reduce false positives. Residual connections and multi-scale predictions are employed in these networks to boost the segmentation performance. Experiments with BraTS 2017 validation set show that the proposed method achieved average Dice scores of 0.7859, 0.9050, 0.8378 for enhancing tumor core, whole tumor and tumor core, respectively. The corresponding values for BraTS 2017 testing set were 0.7831, 0.8739, and 0.7748, respectively.},
  isbn = {978-3-319-75238-9},
  langid = {english},
  keywords = {Brain tumor,Convolutional neural network,Segmentation},
  file = {/home/olaf/Zotero/storage/MJ4TDMBI/Wang et al. - 2018 - Automatic Brain Tumor Segmentation Using Cascaded .pdf}
}

@inproceedings{wangEfficientExpressiveGNNs2022,
  title = {Towards {{Efficient}} and {{Expressive GNNs}} for {{Graph Classification}} via {{Subgraph-aware Weisfeiler-Lehman}}},
  author = {Wang, Zhaohui and Cao, Qi and Shen, Huawei and Bingbing, Xu and Zhang, Muhan and Cheng, Xueqi},
  date = {2022-12-09},
  url = {https://openreview.net/forum?id=ha9hPpthvQ},
  urldate = {2023-03-20},
  abstract = {The expressive power of GNNs is upper-bounded by the Weisfeiler-Lehman (WL) test. To achieve GNNs with high expressiveness, researchers resort to subgraph-based GNNs (WL/GNN on subgraphs), deploying GNNs on subgraphs centered around each node to encode subgraphs instead of rooted subtrees like WL. However, deploying multiple GNNs on subgraphs suffers from much higher computational cost than deploying a single GNN on the whole graph, limiting its application to large-size graphs. In this paper, we propose a novel paradigm, namely Subgraph-aware WL (SaWL), to obtain graph representation that reaches subgraph-level expressiveness with a single GNN. We prove that SaWL has beyond-WL capability for graph isomorphism testing, while sharing similar runtime to WL. To generalize SaWL to graphs with continuous node features, we propose a neural version named Subgraph-aware GNN (SaGNN) to learn graph representation. Both SaWL and SaGNN are more expressive than 1-WL while having similar computational cost to 1-WL/GNN, without causing much higher complexity like other more expressive GNNs. Experimental results on several benchmark datasets demonstrate that fast SaWL and SaGNN significantly outperform competitive baseline methods on the task of graph classification, while achieving high efficiency.},
  eventtitle = {The {{First Learning}} on {{Graphs Conference}}},
  langid = {english},
  file = {/home/olaf/Zotero/storage/YTMEIHS8/Wang et al. - 2022 - Towards Efficient and Expressive GNNs for Graph Cl.pdf}
}

@online{wangGenerativeCoarseGrainingMolecular2022,
  title = {Generative {{Coarse-Graining}} of {{Molecular Conformations}}},
  author = {Wang, Wujie and Xu, Minkai and Cai, Chen and Miller, Benjamin Kurt and Smidt, Tess and Wang, Yusu and Tang, Jian and Gómez-Bombarelli, Rafael},
  date = {2022-06-16},
  number = {arXiv:2201.12176},
  eprint = {arXiv:2201.12176},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.2201.12176},
  url = {http://arxiv.org/abs/2201.12176},
  urldate = {2023-03-20},
  abstract = {Coarse-graining (CG) of molecular simulations simplifies the particle representation by grouping selected atoms into pseudo-beads and drastically accelerates simulation. However, such CG procedure induces information losses, which makes accurate backmapping, i.e., restoring fine-grained (FG) coordinates from CG coordinates, a long-standing challenge. Inspired by the recent progress in generative models and equivariant networks, we propose a novel model that rigorously embeds the vital probabilistic nature and geometric consistency requirements of the backmapping transformation. Our model encodes the FG uncertainties into an invariant latent space and decodes them back to FG geometries via equivariant convolutions. To standardize the evaluation of this domain, we provide three comprehensive benchmarks based on molecular dynamics trajectories. Experiments show that our approach always recovers more realistic structures and outperforms existing data-driven methods with a significant margin.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Physics - Chemical Physics,Physics - Computational Physics},
  file = {/home/olaf/Zotero/storage/4NULKWAV/Wang et al. - 2022 - Generative Coarse-Graining of Molecular Conformati.pdf;/home/olaf/Zotero/storage/JS8ZUA2B/2201.html}
}

@inproceedings{wangHeterogeneousGraphAttention2019,
  title = {Heterogeneous {{Graph Attention Network}}},
  booktitle = {The {{World Wide Web Conference}}},
  author = {Wang, Xiao and Ji, Houye and Shi, Chuan and Wang, Bai and Ye, Yanfang and Cui, Peng and Yu, Philip S},
  date = {2019-05-13},
  series = {{{WWW}} '19},
  pages = {2022--2032},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3308558.3313562},
  url = {https://doi.org/10.1145/3308558.3313562},
  urldate = {2022-12-19},
  abstract = {Graph neural network, as a powerful graph representation technique based on deep learning, has shown superior performance and attracted considerable research interest. However, it has not been fully considered in graph neural network for heterogeneous graph which contains different types of nodes and links. The heterogeneity and rich semantic information bring great challenges for designing a graph neural network for heterogeneous graph. Recently, one of the most exciting advancements in deep learning is the attention mechanism, whose great potential has been well demonstrated in various areas. In this paper, we first propose a novel heterogeneous graph neural network based on the hierarchical attention, including node-level and semantic-level attentions. Specifically, the node-level attention aims to learn the importance between a node and its meta-path based neighbors, while the semantic-level attention is able to learn the importance of different meta-paths. With the learned importance from both node-level and semantic-level attention, the importance of node and meta-path can be fully considered. Then the proposed model can generate node embedding by aggregating features from meta-path based neighbors in a hierarchical manner. Extensive experimental results on three real-world heterogeneous graphs not only show the superior performance of our proposed model over the state-of-the-arts, but also demonstrate its potentially good interpretability for graph analysis.},
  isbn = {978-1-4503-6674-8},
  keywords = {Graph Analysis,Neural Network,Social Network},
  file = {/home/olaf/Zotero/storage/K445M82Q/Wang et al. - 2019 - Heterogeneous Graph Attention Network.pdf}
}

@online{wangTwostage3DUnet2018,
  title = {A Two-Stage {{3D Unet}} Framework for Multi-Class Segmentation on Full Resolution Image},
  author = {Wang, Chengjia and MacGillivray, Tom and Macnaught, Gillian and Yang, Guang and Newby, David},
  date = {2018-04-12},
  number = {arXiv:1804.04341},
  eprint = {arXiv:1804.04341},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.1804.04341},
  url = {http://arxiv.org/abs/1804.04341},
  urldate = {2023-03-15},
  abstract = {Deep convolutional neural networks (CNNs) have been intensively used for multi-class segmentation of data from different modalities and achieved state-of-the-art performances. However, a common problem when dealing with large, high resolution 3D data is that the volumes input into the deep CNNs has to be either cropped or downsampled due to limited memory capacity of computing devices. These operations lead to loss of resolution and increment of class imbalance in the input data batches, which can downgrade the performances of segmentation algorithms. Inspired by the architecture of image super-resolution CNN (SRCNN) and self-normalization network (SNN), we developed a two-stage modified Unet framework that simultaneously learns to detect a ROI within the full volume and to classify voxels without losing the original resolution. Experiments on a variety of multi-modal volumes demonstrated that, when trained with a simply weighted dice coefficients and our customized learning procedure, this framework shows better segmentation performances than state-of-the-art Deep CNNs with advanced similarity metrics.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/olaf/Zotero/storage/GJ2HCXZV/Wang et al. - 2018 - A two-stage 3D Unet framework for multi-class segm.pdf;/home/olaf/Zotero/storage/UA99LGAH/1804.html}
}

@article{weisfeilerReductionGraphCanonical1968,
  title = {A Reduction of a Graph to a Canonical Form and an Algebra Arising during This Reduction},
  author = {Weisfeiler, B Yu and Leman, A A},
  date = {1968},
  journaltitle = {Nauchno-Technicheskaya Informatsia},
  volume = {2},
  number = {9},
  pages = {11},
  abstract = {We consider an algorithm for the reduction of a given finite multigraph Γ to canonical form. Therein the new invariant of a graph appears — the algebra A(Γ). The study of properties of the algebra A(Γ) turns out to be helpful in solving a number of graph-theoretic problems. We pose and discuss some conjectures on the relation between properties of the algebra A(Γ) and the automorphism group Aut(Γ) of a graph Γ. We give an example of undirected graph Γ whose algebra A(Γ) coincides with the group algebra of some noncommutative group.},
  langid = {english},
  file = {/home/olaf/Zotero/storage/2FFHKVDV/Weisfeiler et Leman - THE REDUCTION OF A GRAPH TO CANONICAL FORM AND THE.pdf}
}

@inproceedings{wuRepresentingLongRangeContext2021,
  title = {Representing {{Long-Range Context}} for {{Graph Neural Networks}} with {{Global Attention}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Wu, Zhanghao and Jain, Paras and Wright, Matthew and Mirhoseini, Azalia and Gonzalez, Joseph E and Stoica, Ion},
  date = {2021},
  volume = {34},
  pages = {13266--13279},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2021/hash/6e67691b60ed3e4a55935261314dd534-Abstract.html},
  urldate = {2022-12-07},
  abstract = {Graph neural networks are powerful architectures for structured datasets. However, current methods struggle to represent long-range dependencies. Scaling the depth or width of GNNs is insufficient to broaden receptive fields as larger GNNs encounter optimization instabilities such as vanishing gradients and representation oversmoothing, while pooling-based approaches have yet to become as universally useful as in computer vision. In this work, we propose the use of Transformer-based self-attention to learn long-range pairwise relationships, with a novel “readout” mechanism to obtain a global graph embedding. Inspired by recent computer vision results that find position-invariant attention performant in learning long-range relationships, our method, which we call GraphTrans, applies a permutation-invariant Transformer module after a standard GNN module. This simple architecture leads to state-of-the-art results on several graph classification tasks, outperforming methods that explicitly encode graph structure. Our results suggest that purely-learning-based approaches without graph structure may be suitable for learning high-level, long-range relationships on graphs. Code for GraphTrans is available at https://github.com/ucbrise/graphtrans.},
  file = {/home/olaf/Zotero/storage/A32B7WDM/Wu et al. - 2021 - Representing Long-Range Context for Graph Neural N.pdf}
}

@article{xieHierarchicalNeighborPropagation2021,
  title = {Hierarchical {{Neighbor Propagation With Bidirectional Graph Attention Network}} for {{Relation Prediction}}},
  author = {Xie, Zhiwen and Zhu, Runjie and Liu, Jin and Zhou, Guangyou and Huang, Jimmy Xiangji},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {29},
  pages = {1762--1773},
  issn = {2329-9304},
  doi = {10.1109/TASLP.2021.3079812},
  abstract = {The graph attention network (GAT) [1] has started to become a mainstream neural network architecture since 2018, yielding remarkable performance gains in various natural language processing (NLP) tasks. Although GAT has reached the state-of-the-art (SOTA) performance as a recent success in relation prediction in knowledge graph, the current model is still limited by the following two aspects: (1) the existing model only considers the neighbors from the inbound-direction of the given entity, but ignores the rich neighborhood information from outbound-directions; (2) the existing model only uses the k-th hop output to learn the multi-hop embeddings, which leads to the loss of a large amount of early-stage embedding information (e.g., one-hop) at the graph attention step. In this study, we propose a novel bidirectional graph attention network (BiGAT) to learn the hierarchical neighbor propagation. In our proposed BiGAT, an inbound-directional GAT and an outbound-directional GAT are introduced to capture sufficient neighborhood information before propagating the bidirectional neighborhood information to learn the multi-hop feature embeddings in a hierarchical manner. Experiments conducted on the four publicly available datasets show that BiGAT achieves the competitive results in comparison to other SOTA methods.},
  eventtitle = {{{IEEE}}/{{ACM Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  keywords = {Context modeling,knowledge graph,Magnetic heads,Natural language processing,Predictive models,representation learning,Semantics,Speech processing,Task analysis,Tensors,text mining},
  file = {/home/olaf/Zotero/storage/AA8UUMFC/Xie et al. - 2021 - Hierarchical Neighbor Propagation With Bidirection.pdf;/home/olaf/Zotero/storage/VDSHNGLA/9429930.html}
}

@online{xuHowPowerfulAre2019,
  title = {How {{Powerful}} Are {{Graph Neural Networks}}?},
  author = {Xu, Keyulu and Hu, Weihua and Leskovec, Jure and Jegelka, Stefanie},
  date = {2019-02-22},
  number = {arXiv:1810.00826},
  eprint = {arXiv:1810.00826},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.1810.00826},
  url = {http://arxiv.org/abs/1810.00826},
  urldate = {2022-12-06},
  abstract = {Graph Neural Networks (GNNs) are an effective framework for representation learning of graphs. GNNs follow a neighborhood aggregation scheme, where the representation vector of a node is computed by recursively aggregating and transforming representation vectors of its neighboring nodes. Many GNN variants have been proposed and have achieved state-of-the-art results on both node and graph classification tasks. However, despite GNNs revolutionizing graph representation learning, there is limited understanding of their representational properties and limitations. Here, we present a theoretical framework for analyzing the expressive power of GNNs to capture different graph structures. Our results characterize the discriminative power of popular GNN variants, such as Graph Convolutional Networks and GraphSAGE, and show that they cannot learn to distinguish certain simple graph structures. We then develop a simple architecture that is provably the most expressive among the class of GNNs and is as powerful as the Weisfeiler-Lehman graph isomorphism test. We empirically validate our theoretical findings on a number of graph classification benchmarks, and demonstrate that our model achieves state-of-the-art performance.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/olaf/Zotero/storage/X8ULHQ3S/Xu et al. - 2019 - How Powerful are Graph Neural Networks.pdf;/home/olaf/Zotero/storage/V3FJ8GTH/1810.html}
}

@online{xuOptimizationGraphNeural2021,
  title = {Optimization of {{Graph Neural Networks}}: {{Implicit Acceleration}} by {{Skip Connections}} and {{More Depth}}},
  shorttitle = {Optimization of {{Graph Neural Networks}}},
  author = {Xu, Keyulu and Zhang, Mozhi and Jegelka, Stefanie and Kawaguchi, Kenji},
  date = {2021-05-26},
  number = {arXiv:2105.04550},
  eprint = {arXiv:2105.04550},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.2105.04550},
  url = {http://arxiv.org/abs/2105.04550},
  urldate = {2022-12-07},
  abstract = {Graph Neural Networks (GNNs) have been studied through the lens of expressive power and generalization. However, their optimization properties are less well understood. We take the first step towards analyzing GNN training by studying the gradient dynamics of GNNs. First, we analyze linearized GNNs and prove that despite the non-convexity of training, convergence to a global minimum at a linear rate is guaranteed under mild assumptions that we validate on real-world graphs. Second, we study what may affect the GNNs' training speed. Our results show that the training of GNNs is implicitly accelerated by skip connections, more depth, and/or a good label distribution. Empirical results confirm that our theoretical results for linearized GNNs align with the training behavior of nonlinear GNNs. Our results provide the first theoretical support for the success of GNNs with skip connections in terms of optimization, and suggest that deep GNNs with skip connections would be promising in practice.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/home/olaf/Zotero/storage/YDBXI5IF/Xu et al. - 2021 - Optimization of Graph Neural Networks Implicit Ac.pdf;/home/olaf/Zotero/storage/6BR7Z76G/2105.html}
}

@online{xuRepresentationLearningGraphs2018,
  title = {Representation {{Learning}} on {{Graphs}} with {{Jumping Knowledge Networks}}},
  author = {Xu, Keyulu and Li, Chengtao and Tian, Yonglong and Sonobe, Tomohiro and Kawarabayashi, Ken-ichi and Jegelka, Stefanie},
  date = {2018-06-25},
  number = {arXiv:1806.03536},
  eprint = {arXiv:1806.03536},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.1806.03536},
  url = {http://arxiv.org/abs/1806.03536},
  urldate = {2022-12-07},
  abstract = {Recent deep learning approaches for representation learning on graphs follow a neighborhood aggregation procedure. We analyze some important properties of these models, and propose a strategy to overcome those. In particular, the range of "neighboring" nodes that a node's representation draws from strongly depends on the graph structure, analogous to the spread of a random walk. To adapt to local neighborhood properties and tasks, we explore an architecture -- jumping knowledge (JK) networks -- that flexibly leverages, for each node, different neighborhood ranges to enable better structure-aware representation. In a number of experiments on social, bioinformatics and citation networks, we demonstrate that our model achieves state-of-the-art performance. Furthermore, combining the JK framework with models like Graph Convolutional Networks, GraphSAGE and Graph Attention Networks consistently improves those models' performance.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/olaf/Zotero/storage/XN3TQ97H/Xu et al. - 2018 - Representation Learning on Graphs with Jumping Kno.pdf;/home/olaf/Zotero/storage/G4FLDD2Y/1806.html}
}

@inproceedings{yanardagDeepGraphKernels2015,
  title = {Deep {{Graph Kernels}}},
  booktitle = {Proceedings of the 21th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Yanardag, Pinar and Vishwanathan, S.V.N.},
  date = {2015-08-10},
  series = {{{KDD}} '15},
  pages = {1365--1374},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/2783258.2783417},
  url = {https://dl.acm.org/doi/10.1145/2783258.2783417},
  urldate = {2023-03-21},
  abstract = {In this paper, we present Deep Graph Kernels, a unified framework to learn latent representations of sub-structures for graphs, inspired by latest advancements in language modeling and deep learning. Our framework leverages the dependency information between sub-structures by learning their latent representations. We demonstrate instances of our framework on three popular graph kernels, namely Graphlet kernels, Weisfeiler-Lehman subtree kernels, and Shortest-Path graph kernels. Our experiments on several benchmark datasets show that Deep Graph Kernels achieve significant improvements in classification accuracy over state-of-the-art graph kernels.},
  isbn = {978-1-4503-3664-2},
  keywords = {bioinformatics,collaboration networks,deep learning,graph kernels,r-convolution kernels,social networks,string kernels,structured data},
  file = {/home/olaf/Zotero/storage/VT9VQJQ3/Yanardag et Vishwanathan - 2015 - Deep Graph Kernels.pdf}
}

@online{yangSPAGANShortestPath2021,
  title = {{{SPAGAN}}: {{Shortest Path Graph Attention Network}}},
  shorttitle = {{{SPAGAN}}},
  author = {Yang, Yiding and Wang, Xinchao and Song, Mingli and Yuan, Junsong and Tao, Dacheng},
  date = {2021-01-09},
  number = {arXiv:2101.03464},
  eprint = {arXiv:2101.03464},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.2101.03464},
  url = {http://arxiv.org/abs/2101.03464},
  urldate = {2022-12-31},
  abstract = {Graph convolutional networks (GCN) have recently demonstrated their potential in analyzing non-grid structure data that can be represented as graphs. The core idea is to encode the local topology of a graph, via convolutions, into the feature of a center node. In this paper, we propose a novel GCN model, which we term as Shortest Path Graph Attention Network (SPAGAN). Unlike conventional GCN models that carry out node-based attentions within each layer, the proposed SPAGAN conducts path-based attention that explicitly accounts for the influence of a sequence of nodes yielding the minimum cost, or shortest path, between the center node and its higher-order neighbors. SPAGAN therefore allows for a more informative and intact exploration of the graph structure and further \{a\} more effective aggregation of information from distant neighbors into the center node, as compared to node-based GCN methods. We test SPAGAN on the downstream classification task on several standard datasets, and achieve performances superior to the state of the art. Code is publicly available at https://github.com/ihollywhy/SPAGAN.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/olaf/Zotero/storage/YA4RFUE7/Yang et al. - 2021 - SPAGAN Shortest Path Graph Attention Network.pdf;/home/olaf/Zotero/storage/RBV3YAZ8/2101.html}
}

@online{yanTwoSidesSame2022,
  title = {Two {{Sides}} of the {{Same Coin}}: {{Heterophily}} and {{Oversmoothing}} in {{Graph Convolutional Neural Networks}}},
  shorttitle = {Two {{Sides}} of the {{Same Coin}}},
  author = {Yan, Yujun and Hashemi, Milad and Swersky, Kevin and Yang, Yaoqing and Koutra, Danai},
  date = {2022-11-28},
  number = {arXiv:2102.06462},
  eprint = {arXiv:2102.06462},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.2102.06462},
  url = {http://arxiv.org/abs/2102.06462},
  urldate = {2022-12-07},
  abstract = {In node classification tasks, graph convolutional neural networks (GCNs) have demonstrated competitive performance over traditional methods on diverse graph data. However, it is known that the performance of GCNs degrades with increasing number of layers (oversmoothing problem) and recent studies have also shown that GCNs may perform worse in heterophilous graphs, where neighboring nodes tend to belong to different classes (heterophily problem). These two problems are usually viewed as unrelated, and thus are studied independently, often at the graph filter level from a spectral perspective. We are the first to take a unified perspective to jointly explain the oversmoothing and heterophily problems at the node level. Specifically, we profile the nodes via two quantitative metrics: the relative degree of a node (compared to its neighbors) and the node-level heterophily. Our theory shows that the interplay of these two profiling metrics defines three cases of node behaviors, which explain the oversmoothing and heterophily problems jointly and can predict the performance of GCNs. Based on insights from our theory, we show theoretically and empirically the effectiveness of two strategies: structure-based edge correction, which learns corrected edge weights from structural properties (i.e., degrees), and feature-based edge correction, which learns signed edge weights from node features. Compared to other approaches, which tend to handle well either heterophily or oversmoothing, we show that \{our model, GGCN\}, which incorporates the two strategies performs well in both problems.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning},
  file = {/home/olaf/Zotero/storage/DZFPUTY5/Yan et al. - 2022 - Two Sides of the Same Coin Heterophily and Oversm.pdf;/home/olaf/Zotero/storage/LULI3CVW/2102.html}
}

@online{yunGraphTransformerNetworks2020,
  title = {Graph {{Transformer Networks}}},
  author = {Yun, Seongjun and Jeong, Minbyul and Kim, Raehyun and Kang, Jaewoo and Kim, Hyunwoo J.},
  date = {2020-02-04},
  number = {arXiv:1911.06455},
  eprint = {arXiv:1911.06455},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.1911.06455},
  url = {http://arxiv.org/abs/1911.06455},
  urldate = {2022-12-06},
  abstract = {Graph neural networks (GNNs) have been widely used in representation learning on graphs and achieved state-of-the-art performance in tasks such as node classification and link prediction. However, most existing GNNs are designed to learn node representations on the fixed and homogeneous graphs. The limitations especially become problematic when learning representations on a misspecified graph or a heterogeneous graph that consists of various types of nodes and edges. In this paper, we propose Graph Transformer Networks (GTNs) that are capable of generating new graph structures, which involve identifying useful connections between unconnected nodes on the original graph, while learning effective node representation on the new graphs in an end-to-end fashion. Graph Transformer layer, a core layer of GTNs, learns a soft selection of edge types and composite relations for generating useful multi-hop connections so-called meta-paths. Our experiments show that GTNs learn new graph structures, based on data and tasks without domain knowledge, and yield powerful node representation via convolution on the new graphs. Without domain-specific graph preprocessing, GTNs achieved the best performance in all three benchmark node classification tasks against the state-of-the-art methods that require pre-defined meta-paths from domain knowledge.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Computer Science - Social and Information Networks,Statistics - Machine Learning},
  file = {/home/olaf/Zotero/storage/M443GFFI/Yun et al. - 2020 - Graph Transformer Networks.pdf;/home/olaf/Zotero/storage/GGISEBIG/1911.html}
}

@online{zhangEvaluatingDeepGraph2021,
  title = {Evaluating {{Deep Graph Neural Networks}}},
  author = {Zhang, Wentao and Sheng, Zeang and Jiang, Yuezihan and Xia, Yikuan and Gao, Jun and Yang, Zhi and Cui, Bin},
  date = {2021-08-02},
  number = {arXiv:2108.00955},
  eprint = {arXiv:2108.00955},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.2108.00955},
  url = {http://arxiv.org/abs/2108.00955},
  urldate = {2022-12-07},
  abstract = {Graph Neural Networks (GNNs) have already been widely applied in various graph mining tasks. However, they suffer from the shallow architecture issue, which is the key impediment that hinders the model performance improvement. Although several relevant approaches have been proposed, none of the existing studies provides an in-depth understanding of the root causes of performance degradation in deep GNNs. In this paper, we conduct the first systematic experimental evaluation to present the fundamental limitations of shallow architectures. Based on the experimental results, we answer the following two essential questions: (1) what actually leads to the compromised performance of deep GNNs; (2) when we need and how to build deep GNNs. The answers to the above questions provide empirical insights and guidelines for researchers to design deep and well-performed GNNs. To show the effectiveness of our proposed guidelines, we present Deep Graph Multi-Layer Perceptron (DGMLP), a powerful approach (a paradigm in its own right) that helps guide deep GNN designs. Experimental results demonstrate three advantages of DGMLP: 1) high accuracy -- it achieves state-of-the-art node classification performance on various datasets; 2) high flexibility -- it can flexibly choose different propagation and transformation depths according to graph size and sparsity; 3) high scalability and efficiency -- it supports fast training on large-scale graphs. Our code is available in https://github.com/zwt233/DGMLP.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/olaf/Zotero/storage/GUKSC7QY/Zhang et al. - 2021 - Evaluating Deep Graph Neural Networks.pdf;/home/olaf/Zotero/storage/5H93DK8L/2108.html}
}

@online{zhangGraphlevelNeuralNetworks2022,
  title = {Graph-Level {{Neural Networks}}: {{Current Progress}} and {{Future Directions}}},
  shorttitle = {Graph-Level {{Neural Networks}}},
  author = {Zhang, Ge and Wu, Jia and Yang, Jian and Xue, Shan and Hu, Wenbin and Zhou, Chuan and Peng, Hao and Sheng, Quan Z. and Aggarwal, Charu},
  date = {2022-05-31},
  number = {arXiv:2205.15555},
  eprint = {arXiv:2205.15555},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.2205.15555},
  url = {http://arxiv.org/abs/2205.15555},
  urldate = {2023-03-20},
  abstract = {Graph-structured data consisting of objects (i.e., nodes) and relationships among objects (i.e., edges) are ubiquitous. Graph-level learning is a matter of studying a collection of graphs instead of a single graph. Traditional graph-level learning methods used to be the mainstream. However, with the increasing scale and complexity of graphs, Graph-level Neural Networks (GLNNs, deep learning-based graph-level learning methods) have been attractive due to their superiority in modeling high-dimensional data. Thus, a survey on GLNNs is necessary. To frame this survey, we propose a systematic taxonomy covering GLNNs upon deep neural networks, graph neural networks, and graph pooling. The representative and state-of-the-art models in each category are focused on this survey. We also investigate the reproducibility, benchmarks, and new graph datasets of GLNNs. Finally, we conclude future directions to further push forward GLNNs. The repository of this survey is available at https://github.com/GeZhangMQ/Awesome-Graph-level-Neural-Networks.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning},
  file = {/home/olaf/Zotero/storage/MPNJJD4E/Zhang et al. - 2022 - Graph-level Neural Networks Current Progress and .pdf;/home/olaf/Zotero/storage/RFUCY8KJ/2205.html}
}

@article{zhangSurveyNeuralNetwork2021,
  title = {A {{Survey}} on {{Neural Network Interpretability}}},
  author = {Zhang, Yu and Tiňo, Peter and Leonardis, Aleš and Tang, Ke},
  date = {2021-10},
  journaltitle = {IEEE Transactions on Emerging Topics in Computational Intelligence},
  volume = {5},
  number = {5},
  pages = {726--742},
  issn = {2471-285X},
  doi = {10.1109/TETCI.2021.3100641},
  abstract = {Along with the great success of deep neural networks, there is also growing concern about their black-box nature. The interpretability issue affects people's trust on deep learning systems. It is also related to many ethical problems, e.g., algorithmic discrimination. Moreover, interpretability is a desired property for deep networks to become powerful tools in other research fields, e.g., drug discovery and genomics. In this survey, we conduct a comprehensive review of the neural network interpretability research. We first clarify the definition of interpretability as it has been used in many different contexts. Then we elaborate on the importance of interpretability and propose a novel taxonomy organized along three dimensions: type of engagement (passive vs. active interpretation approaches), the type of explanation, and the focus (from local to global interpretability). This taxonomy provides a meaningful 3D view of distribution of papers from the relevant literature as two of the dimensions are not simply categorical but allow ordinal subcategories. Finally, we summarize the existing interpretability evaluation methods and suggest possible research directions inspired by our new taxonomy.},
  eventtitle = {{{IEEE Transactions}} on {{Emerging Topics}} in {{Computational Intelligence}}},
  keywords = {Decision trees,Deep learning,inter-pretability,Machine learning,neural networks,Reliability,survey,Task analysis,Taxonomy,Tools,Training},
  file = {/home/olaf/Zotero/storage/L2SCQMTL/Zhang et al. - 2021 - A Survey on Neural Network Interpretability.pdf;/home/olaf/Zotero/storage/EAQX4VZQ/9521221.html}
}

@online{zhaoPairNormTacklingOversmoothing2020,
  title = {{{PairNorm}}: {{Tackling Oversmoothing}} in {{GNNs}}},
  shorttitle = {{{PairNorm}}},
  author = {Zhao, Lingxiao and Akoglu, Leman},
  date = {2020-02-12},
  number = {arXiv:1909.12223},
  eprint = {arXiv:1909.12223},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.1909.12223},
  url = {http://arxiv.org/abs/1909.12223},
  urldate = {2022-12-07},
  abstract = {The performance of graph neural nets (GNNs) is known to gradually decrease with increasing number of layers. This decay is partly attributed to oversmoothing, where repeated graph convolutions eventually make node embeddings indistinguishable. We take a closer look at two different interpretations, aiming to quantify oversmoothing. Our main contribution is PairNorm, a novel normalization layer that is based on a careful analysis of the graph convolution operator, which prevents all node embeddings from becoming too similar. What is more, PairNorm is fast, easy to implement without any change to network architecture nor any additional parameters, and is broadly applicable to any GNN. Experiments on real-world graphs demonstrate that PairNorm makes deeper GCN, GAT, and SGC models more robust against oversmoothing, and significantly boosts performance for a new problem setting that benefits from deeper GNNs. Code is available at https://github.com/LingxiaoShawn/PairNorm.},
  pubstate = {preprint},
  version = {2},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/olaf/Zotero/storage/34X4GG6D/Zhao et Akoglu - 2020 - PairNorm Tackling Oversmoothing in GNNs.pdf;/home/olaf/Zotero/storage/N8KSE4P3/1909.html}
}

@online{zhaoPracticalProgressivelyExpressiveGNN2022,
  title = {A {{Practical}}, {{Progressively-Expressive GNN}}},
  author = {Zhao, Lingxiao and Härtel, Louis and Shah, Neil and Akoglu, Leman},
  date = {2022-11-02},
  number = {arXiv:2210.09521},
  eprint = {arXiv:2210.09521},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.2210.09521},
  url = {http://arxiv.org/abs/2210.09521},
  urldate = {2023-03-20},
  abstract = {Message passing neural networks (MPNNs) have become a dominant flavor of graph neural networks (GNNs) in recent years. Yet, MPNNs come with notable limitations; namely, they are at most as powerful as the 1-dimensional Weisfeiler-Leman (1-WL) test in distinguishing graphs in a graph isomorphism testing frame-work. To this end, researchers have drawn inspiration from the k-WL hierarchy to develop more expressive GNNs. However, current k-WL-equivalent GNNs are not practical for even small values of k, as k-WL becomes combinatorially more complex as k grows. At the same time, several works have found great empirical success in graph learning tasks without highly expressive models, implying that chasing expressiveness with a coarse-grained ruler of expressivity like k-WL is often unneeded in practical tasks. To truly understand the expressiveness-complexity tradeoff, one desires a more fine-grained ruler, which can more gradually increase expressiveness. Our work puts forth such a proposal: Namely, we first propose the (k, c)({$<$}=)-SETWL hierarchy with greatly reduced complexity from k-WL, achieved by moving from k-tuples of nodes to sets with {$<$}=k nodes defined over {$<$}=c connected components in the induced original graph. We show favorable theoretical results for this model in relation to k-WL, and concretize it via (k, c)({$<$}=)-SETGNN, which is as expressive as (k, c)({$<$}=)-SETWL. Our model is practical and progressively-expressive, increasing in power with k and c. We demonstrate effectiveness on several benchmark datasets, achieving several state-of-the-art results with runtime and memory usage applicable to practical graphs. We open source our implementation at https://github.com/LingxiaoShawn/KCSetGNN.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning},
  file = {/home/olaf/Zotero/storage/HY88H3B9/Zhao et al. - 2022 - A Practical, Progressively-Expressive GNN.pdf;/home/olaf/Zotero/storage/424MGY28/2210.html}
}

@article{zhouReviewDeepLearning2019,
  title = {A Review: {{Deep}} Learning for Medical Image Segmentation Using Multi-Modality Fusion},
  shorttitle = {A Review},
  author = {Zhou, Tongxue and Ruan, Su and Canu, Stéphane},
  date = {2019-09-01},
  journaltitle = {Array},
  shortjournal = {Array},
  volume = {3--4},
  pages = {100004},
  issn = {2590-0056},
  doi = {10.1016/j.array.2019.100004},
  url = {https://www.sciencedirect.com/science/article/pii/S2590005619300049},
  urldate = {2023-03-13},
  abstract = {Multi-modality is widely used in medical imaging, because it can provide multiinformation about a target (tumor, organ or tissue). Segmentation using multimodality consists of fusing multi-information to improve the segmentation. Recently, deep learning-based approaches have presented the state-of-the-art performance in image classification, segmentation, object detection and tracking tasks. Due to their self-learning and generalization ability over large amounts of data, deep learning recently has also gained great interest in multi-modal medical image segmentation. In this paper, we give an overview of deep learning-based approaches for multi-modal medical image segmentation task. Firstly, we introduce the general principle of deep learning and multi-modal medical image segmentation. Secondly, we present different deep learning network architectures, then analyze their fusion strategies and compare their results. The earlier fusion is commonly used, since it’s simple and it focuses on the subsequent segmentation network architecture. However, the later fusion gives more attention on fusion strategy to learn the complex relationship between different modalities. In general, compared to the earlier fusion, the later fusion can give more accurate result if the fusion method is effective enough. We also discuss some common problems in medical image segmentation. Finally, we summarize and provide some perspectives on the future research.},
  langid = {english},
  keywords = {Deep learning,Medical image segmentation,Multi-modality fusion,Review},
  file = {/home/olaf/Zotero/storage/Z6KY4KRP/Zhou et al. - 2019 - A review Deep learning for medical image segmenta.pdf;/home/olaf/Zotero/storage/GRJVXFQN/S2590005619300049.html}
}

@inproceedings{zhouUnderstandingResolvingPerformance2021,
  title = {Understanding and {{Resolving Performance Degradation}} in {{Deep Graph Convolutional Networks}}},
  booktitle = {Proceedings of the 30th {{ACM International Conference}} on {{Information}} \& {{Knowledge Management}}},
  author = {Zhou, Kuangqi and Dong, Yanfei and Wang, Kaixin and Lee, Wee Sun and Hooi, Bryan and Xu, Huan and Feng, Jiashi},
  date = {2021-10-30},
  series = {{{CIKM}} '21},
  pages = {2728--2737},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3459637.3482488},
  url = {https://doi.org/10.1145/3459637.3482488},
  urldate = {2022-12-07},
  abstract = {A Graph Convolutional Network (GCN) stacks several layers and in each layer performs a PROPagation operation\textasciitilde (PROP) and a TRANsformation operation\textasciitilde (TRAN) for learning node representations over graph-structured data. Though powerful, GCNs tend to suffer performance drop when the model gets deep. Previous works focus on PROPs to study and mitigate this issue, but the role of TRANs is barely investigated. In this work, we study performance degradation of GCNs by experimentally examining how stacking only TRANs or PROPs works. We find that TRANs contribute significantly, or even more than PROPs, to declining performance, and moreover that they tend to amplify node-wise feature variance in GCNs, causing variance inflammation that we identify as a key factor for causing performance drop. Motivated by such observations, we propose a variance-controlling technique termed Node Normalization (NodeNorm), which scales each node's features using its own standard deviation. Experimental results validate the effectiveness of NodeNorm on addressing performance degradation of GCNs. Specifically, it enables deep GCNs to outperform shallow ones in cases where deep models are needed, and to achieve comparable results with shallow ones on 6 benchmark datasets. NodeNorm is a generic plug-in and can well generalize to other GNN architectures. Code is publicly available at https://github.com/miafei/NodeNorm.},
  isbn = {978-1-4503-8446-9},
  keywords = {deep graph convolutional networks,graph-structured data,normalization,performance degradation},
  file = {/home/olaf/Zotero/storage/KFPJWDAX/Zhou et al. - 2021 - Understanding and Resolving Performance Degradatio.pdf}
}

@online{zhuSurveyGraphStructure2022,
  title = {A {{Survey}} on {{Graph Structure Learning}}: {{Progress}} and {{Opportunities}}},
  shorttitle = {A {{Survey}} on {{Graph Structure Learning}}},
  author = {Zhu, Yanqiao and Xu, Weizhi and Zhang, Jinghao and Du, Yuanqi and Zhang, Jieyu and Liu, Qiang and Yang, Carl and Wu, Shu},
  date = {2022-02-14},
  number = {arXiv:2103.03036},
  eprint = {arXiv:2103.03036},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.2103.03036},
  url = {http://arxiv.org/abs/2103.03036},
  urldate = {2022-12-07},
  abstract = {Graphs are widely used to describe real-world objects and their interactions. Graph Neural Networks (GNNs) as a de facto model for analyzing graphstructured data, are highly sensitive to the quality of the given graph structures. Therefore, noisy or incomplete graphs often lead to unsatisfactory representations and prevent us from fully understanding the mechanism underlying the system. In pursuit of an optimal graph structure for downstream tasks, recent studies have sparked an effort around the central theme of Graph Structure Learning (GSL), which aims to jointly learn an optimized graph structure and corresponding graph representations. In the presented survey, we broadly review recent progress in GSL methods. Specifically, we first formulate a general pipeline of GSL and review state-of-the-art methods classified by the way of modeling graph structures, followed by applications of GSL across domains. Finally, we point out some issues in current studies and discuss future directions.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Computer Science - Social and Information Networks},
  file = {/home/olaf/Zotero/storage/VSE82ZBR/Zhu et al. - 2022 - A Survey on Graph Structure Learning Progress and.pdf;/home/olaf/Zotero/storage/56HJFPYK/2103.html}
}
